<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[老大难的GC原理及调优]]></title>
    <url>%2FJava%2FJVM%2F%E8%80%81%E5%A4%A7%E9%9A%BE%E7%9A%84GC%E5%8E%9F%E7%90%86%E5%8F%8A%E8%B0%83%E4%BC%98%2F</url>
    <content type="text"><![CDATA[本文介绍GC基础原理和理论，GC调优方法和思路，基于Hotspot jdk1.8，学习之后将讲解如何对生产系统出现的GC问题进行排查解决。 本文主要内容如下： GC基础原理，设计调优目标，GC事件分类、JVM内存分配策略，GC日志分析等 CMS原理及调优 G1原理及调优 GC问题排查和解决思路 GC 基础原理GC调优目标大多数情况下对Java程序进行GC调优，主要关注两个目标：响应速度 和 吞吐量 响应速度（Responsiveness）：响应速度是指程序或系统对一个请求的响应有多迅速。比如，用户订单查询响应时间，对响应速度要求很高的系统，较大的停顿时间是不可接受的。调优的重点是在短时间内快速响应。 吞吐量（Throughput）：吞吐量关注在一个特定时间段内应用系统的最大工作量。例如每小时批处理系统能完成的任务数量，在吞吐量方面优化的系统，较长的GC停顿时间也是可以接受的，因为高吞吐量应用更关心的是如何尽可能快地完成整个任务，不考虑快速响应用户请求。 GC调优中，GC导致的应用暂停时间影响系统响应速度，GC处理线线程的CPU使用率影响系统吞吐量。 GC分代搜集算法现代的垃圾收集器基本都是采用分代收集算法，其主要思想是：将Java的堆内存逻辑上分为两位：新生代和老年代，针对不同存活周期、不同大小的对象采用不同的垃圾回收策略。 新生代（Young Generation） 新生代又称为年轻代，大多数对象在新生代中被创建，很多对象的生命周期很短。每次新生代的垃圾回收（又称Young GC、Minor GC、YGC）后只有少量对象存活，所以使用复制算法，只需要少量的复制操作成本就可以完成回收。 新生代内又分为三个区：一个Eden区，两个Survivor区（S0、S1，又称为 From Survivor、To Survivor），大部分对象在Eden区中生成。当Eden区满时，还存活的的对象将被复制到两个Survivor区（中的一个）。当这个Survivor区满时，此区的存活且不满足晋升到老年代条件的对象将被复制到另外一个Survivor区。对象每经历一次复制，年龄加1，达到晋升年龄阈值后，转移到老年代。 老年代（Old Generation） 在新生代中经理了N次垃圾回收后仍然存活的对象，就会被放到老年代中，该区域中对象存活率高。老年代的垃圾回收通常使用「标记-整理」算法。 GC事件分类根据垃圾回收的区域不同，垃圾收集通常分为 Young GC、Old GC、Full GC、Mixed GC Young GC新生代内存的垃圾收集事件被称为Young GC（又称为Minor GC）。当JVM无法为新对象分配在新生代内存空间时总会触发一次Young GC，比如Eden区满时，新对象分配频率越高，Young GC频率就越高。 Young GC每次都会引起全线停顿（Stop The World），暂停所有的应用线程，停顿时间相对老年代GC造成的停顿，几乎可以忽略不计。 Old GC、Full GC、Mixed GCOld GC，只清理老年代空间的GC事件，只有CMS的并发收集是这个模式Full GC，清理整个堆的GC事件，包括新生代、老年代、元空间等。 Mixed GC，清理整个新生代以及部分老年代的GC，只有G1有这个模式 GC日志分析GC日志是一个很重要的工具，它准确的记录了每一次的GC执行时间和执行结果，通过分析GC日志可以调优堆设置和GC设置，或者改进应用程序的对象分配模式，开启的JVM启动参数如下： 1-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps 常见的Young GC、Full GC日志含义如下： 免费的GC日志图形分析工具推荐下面两个： GCViewer，下载jar包直接运行 gceasy，web工具，上传GC日志在线使用 内存分配策略Java提供的自动内存管理，可以归结为解决了对象的内存分配和回收的问题，前面已经介绍了内存回收，下面介绍几条最普遍的内存分配策略。 对象优先在Eden区分配：大多数情况下，对象在新生代Eden区分配。当Eden区没有足够空间进行分配时，虚拟机将发起一次Young GC 大对象直接进入老年代：JVM提供了一个对象大小阈值参数（-XX:PretenureSizeThreshold，默认值为0，代表不管多大都是先在Eden中分配内存），大于参数设置的阈值的对象直接在老年代分配，这样可以避免对象在Eden及两个Survivor区直接发生大内存复制。 长期存活的对象将进入老年代：对象每经历一次垃圾回收，且没有被回收掉，它的年龄就增加1，大于年龄阈值参数（-XX:MaxTenuringThreshold，默认15）的对象，将晋升到老年代中 空间分配担保：在进行Young GC之前，JVM需要评估：老年代是否能够容纳Young GC后新生代晋升到老年代的存活对象，以确定是否需要提前触发GC回收老年代空间。基于空间分配担保策略来计算。 continueSize：老年代最大可用连续空间 Young GC之后如果成功（Young GC后晋升对象能放入老年代），代表担保成功，不用再进行Full GC，提高性能。如果失败，贼会出现“promotion failed”的错误，代表担保失败，需要进行Full GC。 动态年龄判断：新生代对象的年龄可能没有达到阈值（MaxTenuringThreshold）就晋升到老年代。如果Young GC之后，新生代存活对象达到相同年龄的所有对象大小的总和大于任一Survivor区空间（S0或S1总空间）的一半，此时S0或者S1即将容纳不了存活的新生代对象，年龄大于或等于该年龄的对象就可以直接进入老年代，无需等到MaxTenuringThreshold中要求的年龄。 CMS原理及调优名词解释可达性分析算法：用于判断兑现是否存活，基本思想是通过一系列被称为“GC Root”的对象作为起点（常见的GC Root对象有系统类加载器、栈中的对象、处于激活状态的线程等），基于对象引用关系，从GC Root开始向下搜索，所有走过的路径被称为引用链，当一个对象到GC Root没有任何引用链相连，证明对象不再存活 STOP THE WORLD：GC过程中分析对象引用关系，为了保证分析结果的准确性，需要停顿所有Java执行线程，保证引用关系不再动态变化，该停顿事件被称为Stop The World（STW） SafePoint：代码执行过程中的一些特殊位置，当线程执行到这些位置时，说明虚拟机当前的状态是安全的，如果有需要GC，线程可以在这个位置暂停。HotSpot采用主动中断的方式，让执行线程在运行期轮询是否有需要暂停的标志，若需要则中断挂起。 CMS简介CMS（Concurrent Mark And Sweep 并发-标记-清除）是一款基于并发、使用标记清除算法的垃圾回收算法，只针对老年代进行垃圾回收。CMS收集器工作时，尽可能让GC线程和用户线程并发执行，以达到降级STW时间的目的。 通过使用以下命令行参数，启用CMS垃圾收集器： 1-XX:+UseConcMarkSweepGC 值得补充的是，下面介绍到的CMS GC是指老年代的GC，而Full GC指的是整个堆的GC事件，包括新生代、老年代、元空间等，两者有所区分。 新生代垃圾回收能与CMS搭配使用的新生代垃圾收集器有Serial收集器和ParNew收集器。这两个收集器都采用标记复制算法，都会触发STW事件，停止所有的应用线程。不同之处在于，Serial是单线程执行，ParNew是多线程执行。 老年代垃圾回收CMS GC以获取最小停顿时间为目的，尽可能减少STW时间，可以分为7个阶段 阶段1：初始标记（Initial Mark） 此阶段的目标是标记老年代中所有存活对象，包括GC Root的直接饮用，以及由新生代中存活对象所引用的对象，触发第一次STW事件 这个过程是支持多线程的（JDK7之前单线程，JDK8之后并行，可以通过参数CMSParallelInitialMarkEnabled调整） 阶段2：并发标记（Concurrent Mark） 此阶段GC线程和应用线程并发执行，遍历阶段1初始标记出来的存活对象，然后继续递归标记这些对象的可达对象 阶段3：并发预清理（Concurrent Preclean） 此阶段GC线程和应用线程也是并发执行的，因为阶段2是与应用线程并发执行，可能有些引用关系已经发生改变。通过卡片标记（Card Marking），提前把老年代空间逻辑划分为相等大小的区域（Card），如果引用关系发生变化，JVM会将发生改变的区域标记为「脏区」（Dirty Card），然后在本阶段，这些脏区会被找出来，刷新引用关系，清除「脏区」标记 阶段4：并发可取消的预清理（Concurrent Abortable Preclean） 此阶段也不停止应用线程。本阶段尝试在STW的最终标记阶段（Final Remark）之前尽可能地多做一些工作，以减少应用暂停时间。 在该阶段不断循环处理： 标记老年代的可达对象 扫描处理Dirty Card区域中的对象 扫描的终止条件： 达到循环次数 达到循环执行时间阈值 新生代内存使用率达到阈值 阶段5：最终标记（Final Remark） 这是GC事件中的第二次（也是最后一次）STW阶段，目标是完成老年代中所有存活对象的标记。 在此阶段执行： 遍历新生代对象，重新标记 根据GC Roots，重新标记 遍历老年代的Dirty Card，重新标记 阶段6：并发清除（Concurrent Sweep） 此阶段与应用程序并发执行，不需要STW停顿，根据标记结果清除垃圾对象 阶段7：并发重置（Concurrent Reset） 此阶段与应用程序并发执行，重置CMS算法相关的内部数据，为下一次GC循环做准备。 CMS常见问题最终标记阶段停顿时间过长问题CMS的GC停顿时间约80%都在最终标记阶段（Final Remark），若该阶段停顿时间过程，常见原因是新生代对老年代的无效引用。在上一阶段的并发可取消预清理阶段中，执行阈值时间内未完成循环，来不及触发Young GC，清理这些无效引用。 通过添加参数-XX:+CMSScavengeBeforeRemark在执行最终操作前先触发Young GC，从而减少新生代对老年代的无效引用，降低最终标记阶段的停顿。但是如果在上个阶段（并发可取消的预清理）已经触发Young GC ，也会重复触发Young GC。 并发模式失败（Concurrent mode failure） &amp; 晋升失败（promotion failed）问题 并发模式失败：当CMS在执行回收时，新生代发生垃圾回收，同时老年代又没有足够的空间容纳晋升的对象时，CMS 垃圾回收就会退化成单线程的Full GC。所有的应用线程都会被暂停，老年代中所有的无效对象都被回收 晋升失败：当新生代发生垃圾回收，老年代有足够的空间可以容纳晋升的对象，但是由于空闲空间的碎片化，导致晋升失败，此时会触发单线程且带压缩动作的Full GC。 并发模式失败和晋升失败都会导致长时间的停顿，常见解决思路如下： 降低触发CMS GC的阈值，即参数-XX:CMSInitiatingOccupancyFraction的值，让CMS GC尽早执行，以保证有足够的空间 增加CMS线程数，即参数-XX:ConcGCThreads， 增大老年代空间 让对象尽量在新生代回收，避免进入老年代 内存碎片问题通常CMS的GC过程基于标记清除算法，不带压缩动作，导致越来越多的内存碎片需要压缩，常见以下场景会触发内存碎片压缩： 新生代Young GC出现新生代晋升担保失败(promotion failed) 程序主动执行System.gc() 可通过参数CMSFullGCsBeforeCompaction的值，设置多少次Full GC触发一次压缩，默认值为0，代表每次进入Full GC都会触发压缩，带压缩动作的算法为上面提到的单线程Serial Old算法，暂停时间(STW)时间非常长，需要尽可能减少压缩时间 G1原理及调优G1简介G1(Garbage-First）是一款面向服务器的垃圾收集器，支持新生代和老年代空间的垃圾收集，主要针对配备多核处理器及大容量内存的机器，G1最主要的设计目标是: 实现可预期及可配置的STW停顿时间 G1堆空间划分 Region 为实现大内存空间的低停顿时间的回收，将划分为多个大小相等的Region。每个小堆区都可能是 Eden区，Survivor区或者Old区，但是在同一时刻只能属于某个代 在逻辑上, 所有的Eden区和Survivor区合起来就是新生代，所有的Old区合起来就是老年代，且新生代和老年代各自的内存Region区域由G1自动控制，不断变动 巨型对象 当对象大小超过Region的一半，则认为是巨型对象(Humongous Object)，直接被分配到老年代的巨型对象区(Humongous regions)，这些巨型区域是一个连续的区域集，每一个Region中最多有一个巨型对象，巨型对象可以占多个Region G1把堆内存划分成一个个Region的意义在于： 每次GC不必都去处理整个堆空间，而是每次只处理一部分Region，实现大容量内存的GC 通过计算每个Region的回收价值，包括回收所需时间、可回收空间，在有限时间内尽可能回收更多的内存，把垃圾回收造成的停顿时间控制在预期配置的时间范围内，这也是G1名称的由来: garbage-first G1工作模式针对新生代和老年代，G1提供2种GC模式，Young GC和Mixed GC，两种会导致Stop The World Young GC 当新生代的空间不足时，G1触发Young GC回收新生代空间 Young GC主要是对Eden区进行GC，它在Eden空间耗尽时触发，基于分代回收思想和复制算法，每次Young GC都会选定所有新生代的Region，同时计算下次Young GC所需的Eden区和Survivor区的空间，动态调整新生代所占Region个数来控制Young GC开销 Mixed GC 当老年代空间达到阈值会触发Mixed GC，选定所有新生代里的Region，根据全局并发标记阶段(下面介绍到)统计得出收集收益高的若干老年代 Region。在用户指定的开销目标范围内，尽可能选择收益高的老年代Region进行GC，通过选择哪些老年代Region和选择多少Region来控制Mixed GC开销 全局并发标记全局并发标记主要是为Mixed GC计算找出回收收益较高的Region区域，具体分为5个阶段 阶段 1: 初始标记(Initial Mark) 暂停所有应用线程（STW），并发地进行标记从 GC Root 开始直接可达的对象（原生栈对象、全局对象、JNI 对象），当达到触发条件时，G1 并不会立即发起并发标记周期，而是等待下一次新生代收集，利用新生代收集的 STW 时间段，完成初始标记，这种方式称为借道（Piggybacking） 阶段 2: 根区域扫描（Root Region Scan） 在初始标记暂停结束后，新生代收集也完成的对象复制到 Survivor 的工作，应用线程开始活跃起来； 此时为了保证标记算法的正确性，所有新复制到 Survivor 分区的对象，需要找出哪些对象存在对老年代对象的引用，把这些对象标记成根(Root)； 这个过程称为根分区扫描（Root Region Scanning），同时扫描的 Suvivor 分区也被称为根分区（Root Region）； 根分区扫描必须在下一次新生代垃圾收集启动前完成（接下来并发标记的过程中，可能会被若干次新生代垃圾收集打断），因为每次 GC 会产生新的存活对象集合 阶段 3: 并发标记（Concurrent Marking） 标记线程与应用程序线程并行执行，标记各个堆中Region的存活对象信息，这个步骤可能被新的 Young GC 打断 所有的标记任务必须在堆满前就完成扫描，如果并发标记耗时很长，那么有可能在并发标记过程中，又经历了几次新生代收集 阶段 4: 再次标记(Remark) 和CMS类似暂停所有应用线程（STW），以完成标记过程短暂地停止应用线程, 标记在并发标记阶段发生变化的对象，和所有未被标记的存活对象，同时完成存活数据计算 阶段 5: 清理(Cleanup) 为即将到来的转移阶段做准备, 此阶段也为下一次标记执行所有必需的整理计算工作： 整理更新每个Region各自的RSet(remember set，HashMap结构，记录有哪些老年代对象指向本Region，key为指向本Region的对象的引用，value为指向本Region的具体Card区域，通过RSet可以确定Region中对象存活信息，避免全堆扫描) 回收不包含存活对象的Region 统计计算回收收益高（基于释放空间和暂停目标）的老年代分区集合 G1调优注意点Full GC问题G1的正常处理流程中没有Full GC，只有在垃圾回收处理不过来(或者主动触发)时才会出现， G1的Full GC就是单线程执行的Serial old gc，会导致非常长的STW，是调优的重点，需要尽量避免Full GC，常见原因如下： 程序主动执行System.gc() 全局并发标记期间老年代空间被填满（并发模式失败） Mixed GC期间老年代空间被填满（晋升失败） Young GC时Survivor空间和老年代没有足够空间容纳存活对象 巨型对象分配巨型对象区中的每个Region中包含一个巨型对象，剩余空间不再利用，导致空间碎片化，当G1没有合适空间分配巨型对象时，G1会启动串行Full GC来释放空间。可以通过增加 -XX:G1HeapRegionSize来增大Region大小，这样一来，相当一部分的巨型对象就不再是巨型对象了，而是采用普通的分配方式 不要设置Young区的大小原因是为了尽量满足目标停顿时间，逻辑上的Young区会进行动态调整。如果设置了大小，则会覆盖掉并且会禁用掉对停顿时间的控制 平均响应时间设置使用应用的平均响应时间作为参考来设置MaxGCPauseMillis，JVM会尽量去满足该条件，可能是90%的请求或者更多的响应时间在这之内， 但是并不代表是所有的请求都能满足，平均响应时间设置过小会导致频繁GC 调优方法与思路如何分析系统JVM GC运行状况及合理优化？ GC优化的核心思路在于：尽可能让对象在新生代中分配和回收，尽量避免过多对象进入老年代，导致对老年代频繁进行垃圾回收，同时给系统足够的内存减少新生代垃圾回收次数，进行系统分析和优化也是围绕着这个思路展开。 分析系统的运行状况 系统每秒请求数、每个请求创建多少对象，占用多少内存 Young GC触发频率、对象进入老年代的速率 老年代占用内存、Full GC触发频率、Full GC触发的原因、长时间Full GC的原因 常用工具如下： jstat jvm自带命令行工具，可用于统计内存分配速率、GC次数，GC耗时，常用命令格式 1jstat -gc &lt;pid&gt; &lt;统计间隔时间&gt; &lt;统计次数&gt; 输出返回值代表含义如下： 例如： jstat -gc 32683 1000 10 ，统计pid=32683的进程，每秒统计1次，统计10次 jmap jvm自带命令行工具，可用于了解系统运行时的对象分布，常用命令格式如下 1234567// 命令行输出类名、类数量数量，类占用内存大小，// 按照类占用内存大小降序排列jmap -histo &lt;pid&gt;// 生成堆内存转储快照，在当前目录下导出dump.hrpof的二进制文件，// 可以用eclipse的MAT图形化工具分析jmap -dump:live,format=b,file=dump.hprof &lt;pid&gt; jinfo 命令格式 1jinfo &lt;pid&gt; 用来查看正在运行的 Java 应用程序的扩展参数，包括Java System属性和JVM命令行参数 其他GC工具 监控告警系统：Zabbix、Prometheus、Open-Falcon jdk自动实时内存监控工具：VisualVM 堆外内存监控： Java VisualVM安装Buffer Pools 插件、google perf工具、Java NMT(Native Memory Tracking)工具 GC日志分析：GCViewer、gceasy GC参数检查和优化：xxfox.perfma.com/ GC优化案例 数据分析平台系统频繁Full GC 平台主要对用户在APP中行为进行定时分析统计，并支持报表导出，使用CMS GC算法。数据分析师在使用中发现系统页面打开经常卡顿，通过jstat命令发现系统每次Young GC后大约有10%的存活对象进入老年代。 原来是因为Survivor区空间设置过小，每次Young GC后存活对象在Survivor区域放不下，提前进入老年代，通过调大Survivor区，使得Survivor区可以容纳Young GC后存活对象，对象在Survivor区经历多次Young GC达到年龄阈值才进入老年代，调整之后每次Young GC后进入老年代的存活对象稳定运行时仅几百Kb，Full GC频率大大降低 业务对接网关OOM 网关主要消费Kafka数据，进行数据处理计算然后转发到另外的Kafka队列，系统运行几个小时候出现OOM，重启系统几个小时之后又OOM，通过jmap导出堆内存，在eclipse MAT工具分析才找出原因：代码中将某个业务Kafka的topic数据进行日志异步打印，该业务数据量较大，大量对象堆积在内存中等待被打印，导致OOM 账号权限管理系统频繁长时间Full GC 系统对外提供各种账号鉴权服务，使用时发现系统经常服务不可用，通过Zabbix的监控平台监控发现系统频繁发生长时间Full GC，且触发时老年代的堆内存通常并没有占满，发现原来是业务代码中调用了System.gc() 总结GC问题可以说没有捷径，排查线上的性能问题本身就并不简单，除了将本文介绍到的原理和工具融会贯通，还需要我们不断去积累经验，真正做到性能最优。 文章转自掘金，特别鸣谢]]></content>
      <categories>
        <category>Java</category>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>JVM</tag>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TCP协议]]></title>
    <url>%2F%E7%BD%91%E7%BB%9C%2FTCP%E5%8D%8F%E8%AE%AE%2F</url>
    <content type="text"><![CDATA[周末小课堂又开张了，这次我们来聊一聊 TCP 协议。 握手多少有点令人意外的是，大多数程序员对 TCP 协议的印象仅限于在创建连接时的三次握手。 严格地说，“三次握手”其实是一个不太准确的翻译，英文原文是 “3-way handshake”，意思是握手有三个步骤。 不过既然教科书都这么翻译，我就只能先忍了。 “三次握手”的步骤相信各位都非常熟悉了：123A: 喂，听得到吗 (SYN)B: 阔以，你呢 (SYN-ACK)A: 我也阔以，开始唠吧 (ACK) （咦，这不是远程面试的开场白吗） 那么问题来了：为什么不是 2 次握手或者 4 次握手呢？ 三次握手针对“为什么不是 4 次”，知乎的段子手是这么回答的：1234A: 喂，听得到吗 (SYN)B: 阔以，你呢 (SYN-ACK)A: 我也阔以，你呢 (SYN-ACK)B: ...我不想和傻\*说话 (FIN) 实际上，上面省略了真正重要的信息，在握手过程中传输的，不是“你能不能听得到”，而是：123A: 喂，我的数据从 x 开始编号 (SYN)B: 知道了，我的从 y 开始编号 (SYN-ACK)A: 行，咱俩开始唠吧 (ACK) 协商一个序号的过程需要一个来回（告知 + 确认），理论上需要 2 个来回（ 4 次），互相确认了双方的初始序号（ ISN，Initial Sequence Number ），才能真正开始通信。 由于第二个来回的“告知”可以和前一次的“确认”合并在同一个报文里（具体怎么结合后面讲），因此最终只需要 3 次握手，就可以建立起一个 tcp 链接。 这也解释了为什么不能只有 2 次握手：因为只能协商一个序号。 不过话说回来，知乎段子手的回复也不是全在抖机灵：毕竟，发起方怎么才能确认接收方已经知道发起方知道接收方知道了呢？即使发起方再问一遍，接收方又怎么知道发起方知道了接收方知道了呢？ 很遗憾，结论是：无论多少个来回都不能保证双方达成一致。 由于实践中丢包率通常不高，因此最合理的做法就是 3 次握手（ 2 个来回），少了不够，多了白搭；同时配上相应的容错机制。 例如 SYN+ACK 包丢失，那么发起方在等待超时后重传 SYN 包即可。 想想看，如果最后一个 ACK 丢了会怎样？ 然后问题又来了：为什么需要协商初始序号，才能开始通信呢？ 可靠我们都知道，tcp 是一个“可靠”（ Reliable ）的协议。 这里“可靠”指的不是保证送达，毕竟网络链路中存在太多不可靠因素。 在 IETF 的 RFC 793 （ TCP 协议）中，Reliability 的具体定义是：TCP 协议必须能够应对网络通信系统中损坏、丢失、重复或者乱序发送的数据。 12345Reliability:The TCP must recover from data that is damaged, lost, duplicated, or delivered out of order by the internet communication system.https://tools.ietf.org/html/rfc793 为了保证这一点，tcp 需要给每一个 [字节] 编号：双方通过三次握手，互相确定了对方的初始序号，后续 [每个包的序号 - 初始序号] 就能标识该包在字节流中所处的位置，这样就可以通过重传来保证数据的连续性。 举个例子： 发送方（ ISN=4000 ） 发出 4001 、4002 、4003 、4004 假设每个包只有 1 字节的数据 接收方 收到 4001 、4002 、4004 4003 因为某种原因没有抵达 这时上层应用只能读到 4001 、4002 中的信息 由于接收方没有收到 4003，因此给发送方的 ACK 中，序号最大值是 4003 （表示收到了 4003 之前的数据）。 过了一段时间（ Linux 下默认是 1s ），发送方发现 4003 一直没被 ACK，就会重传这个包。 当接收方最终收到 4003 以后，上层应用才可以读到 4003 和 4004，从而保证其收到的消息都是可靠的。（以及，接收方需要给发送方 ACK，序号是 4005 ） 注意：虽然 ISN=4000，但是发送方发送的第一个包，SEQ 是 4001 开始的，TCP 协议规定 SYN 需要占一个序号（虽然 SYN 并不是实际传输的数据），所以前面示意图中 ACK 的 seq 是 x+1 。同样，FIN 也会占用一个序号，这样可以保证 FIN 报文的重传和确认不会有歧义。 但是，为什么序号不能从 0 开始呢？ 真实世界的复杂性总是让人头秃。 我们知道，操作系统使用五元组（协议=tcp，源 IP，源端口，目的 IP，目的端口）来标识一个连接，当一个包抵达时，会根据这个包的信息，将它分发到对应的连接去处理。 一般情况下，服务器的端口号通常是固定的（如 http 80 ），而操作系统会为客户端随机分配一个最近没有被使用的端口号，因此包总能被分发到正确的连接里。 但在某些特殊的场景下（例如快速、连续地开启和关闭连接），客户端使用的端口号也可能和上一次一样（或者用了其他刚断开的连接的端口号）。 而 TCP 协议并不对此作出限制：1The protocol places no restriction on a particular connection being used over and over again. ... New instances of a connection will be referred to as incarnations of the connection. 那么： 如果前一个连接的包，因为某种原因滞留在网络中，这会儿才送达，客户端可能无法区分（其 sequence number 在本连接中可能是有效的）。 恶意第三方伪造报文的难度很小。注意，在这个场景里，第三方并 [不需要] 处于通信双方的链路之间，只要他发出的报文可以抵达通信的一方即可。 因此我们需要精心挑选一个 ISN，使得上述 case 发生的可能性尽可能低。 注意：不是在 tcp 协议的层面上 100%避免，因为这会导致协议变得更复杂，实现上增加额外的开销，而在绝大多数情况下是不必要的。如果需要“100%可靠”，需要在应用层协议上增加额外的校验机制；或者使用类似 IPSec 这样的网络层协议来保证对包的有效识别。 那么，ISN 应该如何挑选呢？ ISN 生成器说起来其实很简单： TCP 协议的要求是，实现一个大约每 4 微秒加 1 的 32bit 计数器（时钟），在每次创建一个新连接时，使用这个计数器的值作为 ISN 。 假设传输速度是 2 Mb/s，连接使用的 sequence number 大约需要 4.55 小时才会溢出并绕回（ wrap-around ）到 ISN 。即使提高到 100 Mb/s，也需要大约 5.4 分钟。 而一个包在网络中滞留的时间通常是有限的，这个时间我们称之为 MSL （ Maximum Segment Lifetime ），工程实践中一般认为不会超过 2 分钟。 所以我们一般不用担心本次连接的早期 segment （ tcp 协议称之为 old duplicates ）导致的混淆。 注：在家用千兆以太网已经逐渐普及、服务器间开始使用万兆以太网卡的今天，wrap-around 的时间已经降低到 32.8s （千兆）、3.28s （万兆），这个假定已经不太站得住脚了，因此 rfc1185 针对这种高带宽环境提出了一种扩展方案，通过在报文中加上时间戳，从而可以识别出这些 old duplicates 。 主要风险在于前面提到的场景：前一个连接可能传输了较多数据，因此其序列号可能大于当前连接的 ISN ；如果该连接的报文因为某种原因滞留、现在又突然冒出来，当前连接将无法分辨。 因此，TCP 协议要求在断开连接时，TIME-WAIT 状态需要保留 2 MSL 的时间才能转成 CLOSED （如下图底部所示）。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647 +---------+ ---------\ active OPEN | CLOSED | \ ----------- +---------+&lt;---------\ \ create TCB | ^ \ \ snd SYN passive OPEN | | CLOSE \ \ ------------ | | ---------- \ \ create TCB | | delete TCB \ \ V | \ \ +---------+ CLOSE | \ | LISTEN | ---------- | | +---------+ delete TCB | | rcv SYN | | SEND | | ----------- | | ------- | V+---------+ snd SYN,ACK / \ snd SYN +---------+| |&lt;----------------- ------------------&gt;| || SYN | rcv SYN | SYN || RCVD |&lt;-----------------------------------------------| SENT || | snd ACK | || |------------------ -------------------| |+---------+ rcv ACK of SYN \ / rcv SYN,ACK +---------+ | -------------- | | ----------- | x | | snd ACK | V V | CLOSE +---------+ | ------- | ESTAB | | snd FIN +---------+ | CLOSE | | rcv FIN V ------- | | -------+---------+ snd FIN / \ snd ACK +---------+| FIN |&lt;----------------- ------------------&gt;| CLOSE || WAIT-1 |------------------ | WAIT |+---------+ rcv FIN \ +---------+ | rcv ACK of FIN ------- | CLOSE | | -------------- snd ACK | ------- | V x V snd FIN V+---------+ +---------+ +---------+|FINWAIT-2| | CLOSING | | LAST-ACK|+---------+ +---------+ +---------+ | rcv ACK of FIN | rcv ACK of FIN | | rcv FIN -------------- | Timeout=2MSL -------------- | | ------- x V ------------ x V \ snd ACK +---------+delete TCB +---------+ ------------------------&gt;|TIME WAIT|------------------&gt;| CLOSED | +---------+ +---------+ TCP Connection State Diagram Figure 6. tcp 连接状态图，截取自 rfc 793 那么问题又来了：为什么只有 TIME-WAIT 需要等待 2MSL，而 LAST-ACK 不需要呢？ 报文针对 TCP 协议可以提的问题太多了，写得有点累，所以这里不打算继续自问自答了。 但写了这么多，还没有看一下 TCP 报文是什么结构的，实在不应该，这里还是祭出 rfc 793 里的 ascii art （并顺便佩服 rfc 大佬的画图功力） 123456789101112131415161718192021 0 1 2 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+| Source Port | Destination Port |+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+| Sequence Number |+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+| Acknowledgment Number |+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+| Data | |U|A|P|R|S|F| || Offset| Reserved |R|C|S|S|Y|I| Window || | |G|K|H|T|N|N| |+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+| Checksum | Urgent Pointer |+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+| Options | Padding |+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+| data |+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ TCP Header Format 简单介绍下： 一行是 4 个字节（ 32 bits ），header 一般共 5 行（ options 和 padding 是可选的） 第一行包含了源端口和目的端口 每个端口 16bits，所以端口最大是 65535 源 IP 和目的 IP 在 IP 报文头里 第二行是本次报文的 Sequence Number 第三行是 ACK 序列号 第四行包含了较多信息： 数据偏移量：4 字节的倍数，最小是 0101 （ 5 ），表示数据从第 20 个字节开始（大部分情况） 控制位（ CTL ）：一共 6 个，其中的 ACK 、SYN 、FIN 就不介绍了 RST 是 Reset，遇到异常情况时通知对方重置连接（我们敬爱的防火墙很爱用它） URG 表示这个报文很重要，应该优先传送、接收方应该及时给上层应用。URG 的数据不影响 seq，实际很少被用到，感兴趣的话可以参考下 RFC 854 （ Telnet 协议） PSH 表示这个报文不应该被缓存、应当立即被发送出去。在交互式应用中比较常用，如 ssh，用户每按下一个键都应该及时发出去。注意和 Nagle 算法可能会有一些冲突。 窗口大小：表示这个包的发送方当前可以接受的数据量（字节数），从这个包里的 ack 序号开始算起。用于控制滑动窗口大小的关键字段就是它了。 举个例子，三次握手的第二步，SYN 和 ACK 合并的报文就是这么生成的： Sequence Number 填入从 ISN 生成器中获取的值 Acknowledgement Number 填入 [发送方的序号 + 1] 将控制位中的 ACK 位、SYN 位都置 1 总结 TCP“三次握手”翻译不准确 握手的目的是双方协商初始序列号 ISN 序列号是用于保证通信的可靠性 不使用 0 作为 ISN 可以避免一些坑 TCP 报文里包含了端口号、2 个序列号、一些控制位、滑动窗口大小]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL条件查询不存在行，使用for update加锁的分析]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E5%BA%93%2FMySQL%E6%9D%A1%E4%BB%B6%E6%9F%A5%E8%AF%A2%E4%B8%8D%E5%AD%98%E5%9C%A8%E8%A1%8C%EF%BC%8C%E4%BD%BF%E7%94%A8for-update%E5%8A%A0%E9%94%81%E7%9A%84%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[先说结论： MySQL for update加的是独占锁，而且如果对应的索引是唯一索引加的是行锁，一个事务加锁了，另一个事务应该被阻塞了。 但是如果该查询条件对应的记录不存在，则加了gap锁。 同时如果并发执行insert语句，需要insert意向锁，和gap锁是冲突的，容易产生死锁 有如下的表：12345CREATE TABLE `test` ( `id` int(11) NOT NULL, `name` varchar(100) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 表中数据如下：123456789101112131415mysql&gt; SELECT * FROM test;+----+------+| id | name |+----+------+| 1 | 1 || 6 | 6 || 11 | 11 || 16 | 16 || 21 | 21 || 26 | 26 || 31 | 31 || 36 | 36 |+----+------+8 rows in set (0.00 sec) 有两个并发的事务执行相同的SQL：1234START TRANSACTION;SELECT * FROM test WHERE id = 13 FOR UPDATE;INSERT INTO test VALUES (13, &apos;13&apos;);COMMIT; 第二个事务会出现死锁，退出后随即第一个事务insert执行成功。123456789mysql&gt; START TRANSACTION;Query OK, 0 rows affected (0.00 sec) mysql&gt; SELECT * FROM test_2 WHERE id = 13 FOR UPDATE;Empty set (0.00 sec) mysql&gt; INSERT INTO test VALUES (13, &apos;13&apos;);ERROR 1213 (40001): Deadlock found when trying to get lock; try restarting transactionmysql&gt; 该例子和幻读挺类似的。但是for update应该加的是独占锁，而且如果对应的索引是唯一索引加的是行锁，一个事务加锁了，另一个事务应该被阻塞了。但是如果该查询条件对应的记录不存在，加的则是gap锁，该例子中锁的范围是(11,16)，不包括11，16，gap是互相兼容的，另一个事务不会阻塞。执行insert语句时，需要insert意向锁，和gap锁是冲突的，所以产生了死锁。]]></content>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux索引节点(Inode)用满导致空间不足]]></title>
    <url>%2FLinux%2FLinux%E7%B4%A2%E5%BC%95%E8%8A%82%E7%82%B9(Inode)%E7%94%A8%E6%BB%A1%E5%AF%BC%E8%87%B4%E7%A9%BA%E9%97%B4%E4%B8%8D%E8%B6%B3%2F</url>
    <content type="text"><![CDATA[好不容易有个周末，今天想在博客上整理下之前的一些笔记，结果发现Hexo-Admin管理页面打开白屏了，简单排查后了解原来是系统Inode用完了，经过一番排查后终于得以顺利写下这篇记录文章。 本文着重介绍了如何解决Linux系统由于INode用完而导致的No space left on device的问题。 问题描述打开Hexo-Admin管理后台时，发现竟然白屏了。一脸懵逼 考虑到自己服务器性能一般的现状，加上“重启解决99%的问题”的指导系统，先给服务器来了一波reboot。然鹅重启完成后打开页面仍然是白屏。 没办法，只能观察日志了，Nginx日志中有如下报错：12020/04/06 18:07:14 [crit] 21499#0: *26403 open() &quot;/usr/local/nginx/proxy_temp/1/00/0000000001&quot; failed (28: No space left on device) while reading upstream, client: *.*.*.*, server: *.winsky.wang, request: &quot;GET /admin/bundle.js HTTP/2.0&quot;, upstream: &quot;http://127.0.0.1:4000/admin/bundle.js&quot;, host: &quot;*.winsky.wang&quot;, referrer: &quot;https://*.winsky.wang/*/&quot; 报错倒是很明确，No space left on device，难道是服务器磁盘满了？df -h看一把1234Filesystem Size Used Avail Use% Mounted on/dev/sda2 11G 7.4G 2.6G 75% /tmpfs 254M 0 254M 0% /dev/shm/dev/sda1 283M 146M 122M 55% /boot 虽然磁盘使用率不低，但是仍然有一定空间，排除是磁盘的锅了。 那，难道是索引节点Inode停工了？df -i看一波1234Filesystem Inodes IUsed IFree IUse% Mounted on/dev/sda2 696256 696256 0 100% /tmpfs 64903 1 64902 1% /dev/shm/dev/sda1 76912 58 76854 1% /boot 太惨了，果然是Inode被剥削光了，导致系统无法创建新目录和文件。 解决方案定位到问题后，解决就相对方便多了，本质就一句话，服务器上小文件太多了，Linux系统都撑不住了，删掉一些就好了。 首先考虑到的是/tmp目录，这个文件夹下经常会有一些临时文件，直接删除之。rm -rf /tmp/* 再次查看Inode的使用情况，释放了大概一千多个可用索引节点，还不够啊。 借助for i in /*; do echo $i; find $i |wc -l|sort -nr; done命令看了下机器上每个文件夹下的文件数量，发现/var/spool/clientmqueue文件下超级多小文件。阿西，原来是机器上跑着的定时任务，输出内容没有重定向，导致产出了超级无敌多的小文件，蚕食了系统的Inode资源。依次执行下面的命令，然后问题就都解决了。 123cd /var/spool/clientmqueue# 这儿不要直接用rm -rf *，你会得到参数太多的报错提示的。find /var/spool/clientmqueue -type f -exec rm &#123;&#125; \; 安静的等待删除脚本执行完成，应急也就算OK了。删除后的Inode使用情况：1234Filesystem Inodes IUsed IFree IUse% Mounted on/dev/sda2 696256 224577 471679 33% /tmpfs 64903 1 64902 1% /dev/shm/dev/sda1 76912 58 76854 1% /boot 世界瞬间干净了许多，完美。 拓展思考问题根因Inode译成中文就是索引节点，每个存储设备（例如硬盘）或存储设备的分区被格式化为文件系统后，应该有两部分，一部分是Inode，另一部分是Block，Block是用来存储数据用的。而Inode呢，就是用来存储这些数据的信息，这些信息包括文件大小、属主、归属的用户组、读写权限等。Inode为每个文件进行信息索引，所以就有了Inode的数值。操作系统根据指令，能通过Inode值最快的找到相对应的文件。 而这台服务器的Block虽然还有剩余，但Inode已经用满，因此在创建新目录或文件时，系统提示磁盘空间不足。 Inode的数量是有限制的，每个文件对应一个Inode，通过df -i可以查看Inode的最大数量和当前使用情况。 Linux系统之Inode索引节点Inode：保存的其实是实际的数据的一些信息，这些信息称为“元数据”(也就是对文件属性的描述)。例如：文件大小，设备标识符，用户标识符，用户组标识符，文件模式，扩展属性，文件读取或修改的时间戳，链接数量，指向存储该内容的磁盘区块的指针，文件分类等等。(注意数据分成：元数据+数据本身) 同时注意：Inode有两种，一种是VFS的Inode，一种是具体文件系统的Inode。前者在内存中，后者在磁盘中。所以每次其实是将磁盘中的Inode调进填充内存中的Inode，这样才是算使用了磁盘文件Inode。 Inode怎样生成的：每个Inode节点的大小，一般是128字节或256字节。Inode节点的总数，在格式化时就给定(现代OS可以动态变化)，一般每2KB就设置一个Inode。一般文件系统中很少有文件小于2KB的，所以预定按照2KB分，一般Inode是用不完的。所以Inode在文件系统安装的时候会有一个默认数量，后期会根据实际的需要发生变化。 Inode号：Inode号是唯一的，表示不同的文件。其实在Linux内部的时候，访问文件都是通过Inode号来进行的，所谓文件名仅仅是给用户容易使用的。当我们打开一个文件的时候，首先，系统找到这个文件名对应的Inode号；然后，通过Inode号，得到Inode信息，最后，由Inode找到文件数据所在的block，现在可以处理文件数据了。 Inode和文件的关系：当创建一个文件的时候，就给文件分配了一个Inode。一个Inode只对应一个实际文件，一个文件也会只有一个Inode。Inode最大数量就是文件的最大数量。 阮一峰的网络日志：理解inode 定时任务接锅本案例中，/var/spool/clientmqueue文件夹中文件最多，原因竟然是，crontab中配置的定时任务，执行的程序有输出内容，输出内容会以邮件形式发给cron的用户，而sendmail没有启动所以就产生了这些文件。 需要本案例中，/var/spool/clientmqueue文件夹中文件最多，原因竟然是，crontab中配置的定时任务，执行的程序有输出内容，输出内容会以邮件形式发给cron的用户，而sendmail没有启动所以就产生了这些文件。 将crontab里面的命令后面加上&gt; /dev/null 2&gt;&amp;1或者&gt; /dev/null，这样就不会在定时任务每次执行时产生一堆小文件了。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Inode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记一次Idea加载项目特别慢的解决经历]]></title>
    <url>%2F%E5%A5%87%E6%8A%80%E6%B7%AB%E5%B7%A7%2F%E8%AE%B0%E4%B8%80%E6%AC%A1Idea%E5%8A%A0%E8%BD%BD%E9%A1%B9%E7%9B%AE%E7%89%B9%E5%88%AB%E6%85%A2%E7%9A%84%E8%A7%A3%E5%86%B3%E7%BB%8F%E5%8E%86%2F</url>
    <content type="text"><![CDATA[公司的小二后台项目，刚开始引入项目的时候，idea打开项目挺快。今天想运行一下一个web前端页面（使用了React框架）。我使用npm安装运行后，idea这里就开始变得特别慢，一直卡在构建index菊花圈中。 并且一直扫描的是node_modules目录，大概就是node_modules目录引起的。 解决方案：重新打开一个小项目，在Perferencs-&gt;Editor-&gt;File Types-&gt;ignore files and folders添加node_modules, 重启idea。 问题完美解决，希望能帮助遇到类似问题的童鞋们。]]></content>
      <categories>
        <category>奇技淫巧</category>
      </categories>
      <tags>
        <tag>IDEA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Switch如何科学上网]]></title>
    <url>%2F%E5%A5%87%E6%8A%80%E6%B7%AB%E5%B7%A7%2FSwitch%E5%A6%82%E4%BD%95%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91%2F</url>
    <content type="text"><![CDATA[双十一前妹子从日本买了心心念的 Switch ，最近周末空了终于装好了想玩一玩。然鹅，由于某些不可描述的原因，上面的游戏下载始终在龟速，忍无可忍，于是便想着给 Switch 搞个梯子，一键起飞。 本文利用 ShadowsocksX-NG 提供的 HTTP 代理服务器来实现 Switch 翻墙。这是现有条件下的被迫解决方案，需要一直开着笔记本。 个人觉得最好的还是在家里的路由器上直接架上梯子，这样局域网内所有的设备都可以自由起飞，美滋滋。 为什么 Switch 要翻墙一是使用 Switch 所支持的社交账号 Twitter （另一个是 Facebook），二是某些游戏的下载需要借助梯子来实现更快的访问速度。 Switch 翻墙所需条件 Switch 一台（硬指标） WiFi 梯子服务 国外VPS，强推 搬瓦工，价格和线路质量都没得说，如果你不会使用国外的VPS搭建梯子的话，也可以直接使用现成的服务 一键起飞 Switch 如何翻墙准备代理服务器如果你已经有一台能够自由飞翔的 HTTP 代理服务器，可以直接跳过本小节。 若你和我一样在 macOS 上使用 ShadowsocksX-NG 翻墙，那么你完全可以按照我的做法达到 Switch 翻墙的目的。 若不是，方法类似。 配置 HTTP 代理服务器打开 ShadowsocksX-NG Preferences 窗口，选择 HTTP 标签页，填上你的局域网 IP 和端口(保持默认1087即可)，并开启 HTTP Proxy Enable 选项。 然后关闭 Preferences 窗口就准备好代理服务器了。 Switch 连接 WiFi 并设置代理首先，到 Switch System Settings 里选择 Internet &gt; Internet Settings 并确定(A)，连接你的 WiFi。 然后，在 Internet Settings 里选择刚连接的 WiFi，并确定(A)。在出现的界面中选择 Change Settings 并确定(A)。 接着，在出现的界面中选择 Proxy Settings 并确定(A)，并在出现的界面中选择 On 并确定(A)。 最后，在 Proxy Settings 下面填上上一节所设置的代理服务器的 IP 地址(Server)、端口(Port)以及授权信息并保存(根据实际情况填写，本教程中的代理服务器无授权信息就不用填了。)。 测试代理是否设置成功到 System Settings &gt; Internet &gt; Internet Settings 里选择 Test Connection 并确定开始测试。 若测试结果显示成功则表示你的 Switch 已经可以穿越到异世界了，恭喜！ 注：测试成功时，Global IP Address 显示的是你的真正的代理服务器的 IP 地址。(我这里显示的是 ShadowsocksX-NG 所使用的翻墙服务器的IP地址)]]></content>
      <categories>
        <category>奇技淫巧</category>
      </categories>
      <tags>
        <tag>扶墙</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo中插入HTML表格并解决过多空白的问题]]></title>
    <url>%2FHexo%E5%8D%9A%E5%AE%A2%2FHexo%E4%B8%AD%E6%8F%92%E5%85%A5HTML%E8%A1%A8%E6%A0%BC%E5%B9%B6%E8%A7%A3%E5%86%B3%E8%BF%87%E5%A4%9A%E7%A9%BA%E7%99%BD%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[今天在写博文时，发现markdown无法支持复杂的表格，不支持单元格合并，只能借助HTML语言来绘制表格。 本文介绍如何快速地生成HTML表格，并解决HEXO样式下HTML表格有大量空白的问题。 我用下面的HTML代码做一个表格。1234567891011121314151617181920&lt;table style=&quot;undefined;table-layout: fixed; width: 400px&quot;&gt; &lt;tr&gt; &lt;th rowspan=&quot;3&quot;&gt;资产&lt;/th&gt; &lt;th&gt;负债&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;应付票据 $5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;所有者权益&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;现金 $10&lt;/td&gt; &lt;td&gt;初始投资 $5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;总计 $10&lt;/td&gt; &lt;td&gt;总计 $10&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt; 在有道云笔记中预览时，一切展示正常，但是部署到博客上的时候，却出现了大量的空白。 我们可以在浏览器中右击“查看源代码”，找到这个表格会看到，多出很多标签来。html中标签用于换行。 我后来在hexo的Issues中也发现了其他人出现了这个问题。然后开始各种找解决办法,下面给出两种解决办法。 方法一将代码改为紧凑模式，修改代码如下1&lt;table style=&quot;undefined;table-layout: fixed; width: 402px&quot;&gt;&lt;colgroup&gt;&lt;col style=&quot;width: 201px&quot;&gt;&lt;col style=&quot;width: 201px&quot;&gt;&lt;/colgroup&gt;&lt;tr&gt;&lt;th rowspan=&quot;3&quot;&gt;资产&lt;/th&gt;&lt;th&gt;负债&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;应付票据 $5&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;应付账款 $10&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;现金 $12&lt;/td&gt;&lt;td&gt;所有者权益&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;应收账款 $3&lt;/td&gt;&lt;td&gt;初始投资 $5&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;存货 $0&lt;/td&gt;&lt;td&gt;利润&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;固定资产 $10&lt;/td&gt;&lt;td&gt;盈利 $5&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;总计 $25&lt;/td&gt;&lt;td&gt;总计 $25&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt; 也就是说代码标签之间不要留白，全部改为紧贴着的。 我们还可以利用这个Table Generator在线工具来编辑表格，提供了html表格和markdown表格来生成用于hexo的表格。 方法二(推荐)123&#123;% raw %&#125;html tags &amp; content&#123;% endraw %&#125; 我们可以利用上面的格式来编写表格，我个人认为这种最为简单便捷。我们只需要把代码修改为以下这样即可。12345678910111213141516171819202122&#123;% raw %&#125;&lt;table style=&quot;undefined;table-layout: fixed; width: 400px&quot;&gt; &lt;tr&gt; &lt;th rowspan=&quot;3&quot;&gt;资产&lt;/th&gt; &lt;th&gt;负债&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;应付票据 $5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;所有者权益&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;现金 $10&lt;/td&gt; &lt;td&gt;初始投资 $5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;总计 $10&lt;/td&gt; &lt;td&gt;总计 $10&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&#123;% endraw %&#125; 生成的表格同样不会出现大量空白，具体效果可以参考 小白学借贷记账法]]></content>
      <categories>
        <category>Hexo博客</category>
      </categories>
      <tags>
        <tag>HEXO</tag>
        <tag>博客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小白学借贷记账法]]></title>
    <url>%2F%E8%B4%A2%E5%8A%A1%E7%9F%A5%E8%AF%86%2F%E5%B0%8F%E7%99%BD%E5%AD%A6%E5%80%9F%E8%B4%B7%E8%AE%B0%E8%B4%A6%E6%B3%95%2F</url>
    <content type="text"><![CDATA[看到“借贷”两字时，我先入为主地将其与借钱、贷款等联系起来了，这从字面意思上看确实很误导人。甚至在读完《世界上最简单的会计书》后，脑海中有了资产、负债、权责发生制、收付实现制等名词概念，也没有意识到“原来通篇讲解的就是借贷记账法啊！”。这本书通过生动的“夏日柠柠檬汁摊子”这么一个例子来讲解会计知识，简明易懂，能够形成一个初步的认识，但也让我感受到一个弊端：读完之后觉得理解了，但是在现实中见到专业术语却让人一脸懵逼。 实际上，“借贷记账法”这里的借贷，与借钱、贷款并没有什么关系，他们仅仅是用力啊记账的符号而已。“借”这一符号记为DR（Debit record）或D，“贷”这一符号记为CR（Credit record）或C，所以我们可以简单地先将其视为与￥、$、L（left）、R（right）一样的符号。这是前提。 本文主要介绍小白初学借贷记账法的一些认识和理解。 我们从《世界上最简单的会计书》这本书的例子入手，梳理一遍关于借贷记账法的基础知识，如资产负债表。 现在假设我们有5美元现金，但是我们准备开一个柠檬汁摊子来创业，并且将这5美元都投入进去进行创业。但是5美元满足不了创业的预算，于是我们便从好心的银行贷款了5美元，并准备应用到创业大计。我们使用一张表格来记录一下。 资产 负债 应付票据 $5 所有者权益 现金 $10 初始投资 $5 总计 $10 总计 $10 妈耶，表格里突然出现好多不认识的名词啊。不要慌！之所以将表格分为左右两栏，是从“资金的流向”来考虑的 右边表示资金从哪里来，即资金的“来龙” 左边表示资金现在被用到哪里或者以何种状态存在着，即资金的“去脉”。 现在我们来详细的看表格的左右两边。 首先左边这一栏总称为“资产”。所谓资产，就是我们这个企业“柠檬汁小摊”所持有的，预期会给我们带来经济效益的所有资源。 右边这一栏又细分了两项，负债和所有者权益。 负债容易理解，就是一定时期后企业必须偿还的账务。这里我们向银行贷款了$5，于是我们就有一张还款单需要划款。那么应付票据就可以看做是这张还款单。 相对于负债这一“我们借的钱”，所有者权益可以理解为“别人（包括自己）投资给我们的钱”。如果我们认为负债是“坏东西”，所有者权益是好东西，那么不管是好的还是坏的，都是我们所拥有的东西，这就是资产。 现在我们企业还没有开张，我们暂且发现：123左边 = 右边即：资产 = 负债 + 所有者权益 现在我们从这$10中拿出$5租摊位，$5买榨汁机，并且赊账$10从小卖铺购进了柠檬，并且自己开心的将所有的柠檬榨成了柠檬汁，然后顺利地卖光了所有的柠檬汁，收到现金$12和谷歌赊的账$3。 将这个结果也用表格记录如下： 资产 负债 应付票据 $5 应付账款 $10 现金 $12 所有者权益 应收账款 $3 初始投资 $5 存货 $0 利润 固定资产 $10 盈利 $5 总计 $25 总计 $25 依旧来解释一下表格。 我们租的摊位和榨汁机是我们的固定资产，总价值$10； 我们将所有的柠檬做成的柠檬汁都卖掉了，所以存货（仓库里的柠檬和柠檬汁）就为0； 卖掉后我们收到$12现金和$3的赊账。这$3便被称为应收账款。 相应的，我们还向小卖铺赊账了$10购买了原材料，这$10就被称为应付账款。 整个流程下来，我们理论上收入了$15，原材料作为成本有$10，所以这趟生意我们盈利了$5，作为企业利润。 通过这个表，我们发现左边竟然还等于右边。也就是12资产 = 负债 + 所有者权益 + 利润 = 浮渣 + 所有者权益 + (收入 - 费用) 如果我们观察下表格，就会发现其实左边一栏是我们将流入的资金“物化”了，而右边一栏还是在“钱”的层面上，指明了钱从哪里来。 好啦，我们现在已经了解了这个会计恒等式了。接下来围绕恒等式左右两边始终保持平衡，那么左右两边就要同增同减，我们要进入“有借必有贷，借贷必相等”的天地了。 核心资产类账户和负债类账户进入使用借贷来对账户记账的天地，我们首先需要熟悉两个身份的认证：资产类账户和负债类账户。 通俗地来讲，资产类账户就是记录企业所拥有资产的账户，负债类账户就是记录企业所背负负债的账户，也就是债主的账户啦。 但是我们也会发现，确认某个账户是资产类账户还是负债类账户，首先需要指定看待问题的主体，也就是从谁的角度来确认某账户的身份。举个栗子，支付宝中的用户余额账户，从用户自身角度而言，这个账户记录了我的资产，所以对用户而言是资产类账户。但是从支付宝的角度讲，这个账户记录了支付宝欠用户多少钱，用户是支付宝的最大债主，所以对支付宝而言是负债类账户。 借贷记账法资金像流水一样，是运动的。对于资金的“来向”和“去向”，我们需要记录一笔“流水账”。通过上述的铺垫，我们现在学习使用一种“借D”和“贷C”来记录资金流向的方法。 首先还是先回顾一下前面讲的恒等式1资产 = 负债 + 所有则权益 官方地来讲，“借”是相对于资产而言的，“贷”是针对负债+所有者权益而言的。 在我看来，借D和贷C就是表示资金流向的符号而已。就像E表示东，W表示西一样。尽管他们是中文，似乎赋予了意义，但是为了便于记录，我们可以这样认识： 借即 我借出去的 贷即 我贷来的，欠别人的 从主观上来讲，借是好东西，借出去的还是我的；贷是坏东西，贷来的迟早要还的。 在这种方法中，我们要做的第一步，是要确定要为谁记账。 第二步，确定资金的来向、去向分表是什么角色。即两方账户各自属于资产类账户还是负债类账户。我们将资产类账户记为“账户类型 D”，负债类账户记为“账户类型 C”。 这一步至关重要，因为它决定了资金流动过程中，两个账户分别处于哪个方向，也就是哪个在“借方向”，哪个在“贷方向”。在“借方向”即记下D，在“贷方向”即记下C。 然后，我们要看关键的推演规则了。 资产类账户的资金金额增加，或者负债类账户的资金金额减少记为借D 资产类账户的资金及金额减少，或者负债类账户的资金金额增加记为贷C 这一规则回扣到恒等式上， “借”表示，资产的增加（等式左边增加），或者负债+所有者权益减少（等式右边减少）。 “贷”表示，资产的减少（等式左边减少），或者负债+所有者权益增加（等式右边增加）。 “有借必有贷，借贷必相等”也体现在这里，左边增加（借）右边也必然得增加（贷），而且增量相同才能保持等式成立。 很容易理解，前面我们讲过，可以将“借”视作好东西，当我们的资产类账户钱多了，也就是我们的资产多了，我们的负债类账户钱少了，也就是我们欠的钱少了，难道不是好东西吗？ 这个规则可以通过下图清晰的表示出来。 通过简写，我们也能够发现规则：同向相加，异向相减。 示例下面我们通过几个示例，来检验一下这个方法。 以下的分析都是站在支付宝的角度进行记账 1. 充值张三通过银行卡向支付宝余额充值1000元 这个过程隐含的流程是：张三的银行卡账户-1000元，支付宝的头寸账户（支付宝在该银行开的卡）+1000元，张三支付宝余额+1000元 张三支付宝余额账户 支付宝头寸账户 负债类账户 C 资产类账户 D 资金额增加 + 资金额增加 + 记为贷 C 记为借 D 2. 提现张三从支付宝余额体现到银行卡1000元 这个过程隐含的流程是：张三的支付宝余额账户-1000元，支付宝的头寸账户-1000元，张三的银行卡账户+1000元 张三支付宝余额账户 支付宝头寸账户 负债类账户 C 资产类账户 D 资金额减少 - 资金额减少 - 记为借 D 记为贷 C 3. 转账张三从支付宝余额转账1000元到李四的支付宝余额 这个过程的隐含流程是：张三的支付宝余额账户-1000元，李四的支付宝余额账户+1000元 张三支付宝余额账户 李四支付宝余额账户 负债类账户 C 负债类账户 C 资金额减少 - 资金额增加 + 记为借 D 记为贷 C 4. 消费张三使用支付宝余额支付了淘宝订单1000元 这个过程的隐含流程是：张三的支付宝余额账户-1000元，支付宝的头寸账户-1000元，淘宝卖家账户+1000元 张三支付宝余额账户 支付宝头寸账户 负债类账户 C 资产类账户 D 资金额减少 - 资金额减少 + 记为借 D 记为贷 C 5. 花呗消费张三使用花呗支付了淘宝订单1000元 这个过程的隐含流程是：张三的花呗账户+1000元，支付宝的头寸账户-1000元，淘宝卖家账户+1000元 张三花呗账户 支付宝头寸账户 资产类账户 D 资产类账户 D 资金额增加 + 资金额减少 - 记为借 D 记为贷 C 附：对支付宝来讲，常见的资产类账户有银行头寸、在途户和应收待清算，常见的负债类账户有客户余额、应付过渡户、损益账户和待清算。 总结总的来讲，有以下几点： 资产负债表常见的成分 会计恒等式：资产 = 负债 + 所有者权益 借贷记账法格言：有借必有贷，借贷必相等 借贷记账法的要点：确定记账主体后，归类两账户类型，按照 同加异减 的法则，确定记C或者记D 感谢九老板倾情指导]]></content>
      <categories>
        <category>财务知识</category>
      </categories>
      <tags>
        <tag>财务</tag>
        <tag>借贷记账法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring中事务的隔离级别]]></title>
    <url>%2FSpring%2FSpring%E4%B8%AD%E4%BA%8B%E5%8A%A1%E7%9A%84%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%2F</url>
    <content type="text"><![CDATA[最近学习了公司的分布式事务框架，这个是基于Spring的事务来实现的，其中涉及到Spring事务的隔离级别，联想到平时项目的实现中经常会用到分布式事务，偶尔还会用到嵌套式事务，我对此一知半解，所以借此机会，正好系统地学习一下Spring的事务隔离级别。 详细展开前，先汇总一下Spring的几种事务隔离级别： PROPAGATION_REQUIRED【Spring默认的事务隔离级别】 PROPAGATION_SUPPORTS PROPAGATION_MANDATORY PROPAGATION_REQUIRES_NEW] PROPAGATION_NOT_SUPPORTED PROPAGATION_NEVER PROPAGATION_NESTED PROPAGATION_REQUIRED默认的事务传播特性，通常情况下我们用这个事务传播特性就可以了。 如果当前事务上下文中没有事务，那么就会新创建一个事务。如果当前的事务上下文中已经有一个事务了，那么新开启的事务会开启一个新的逻辑事务范围，但是会和原有的事务共用一个物理事务，我们暂且不关心逻辑事务和物理事务的区别，先看看这样会导致怎样的代码行为。12345678910111213141516171819public void txRollbackInnerTxRollbackPropagationRequires() &#123; transactionTemplate.execute(new TransactionCallbackWithoutResult() &#123; @Override protected void doInTransactionWithoutResult(TransactionStatus transactionStatus) &#123; jdbcTemplate.update("insert into user (name, password) values (?, ?)", "Huang", "1111112"); transactionTemplate.execute(new TransactionCallbackWithoutResult() &#123; @Override protected void doInTransactionWithoutResult(TransactionStatus status) &#123; jdbcTemplate.update("insert into user (name, password) values (?, ?)", "Huang", "1111112"); // 内部事务设置了 setRollbackOnly， status.setRollbackOnly(); &#125; &#125;); &#125; &#125;);&#125;` 这段代码在一个嵌套的事务内部（内外层的事务都是PROPAGATION_REQUIRED的）设置了回滚，那么对于外部的事务来说，它会收到一个UnexpectedRollbackException，因为内部的事务和外部的事务是共用一个物理事务的，所以显然内部的事务必然会导致外部事务的回滚，但是因为这个回滚并不是外部事务自己设置的，所以外层事务回滚的时候会需要抛出一个UnexpectedRollbackException，让事务的调用方知道这个回滚并不是外部事务自己想要回滚，是始料未及的。 但是，如果内层的事务不是通过设置setRollbackOnly()来回滚，而是抛出了RuntimeException来回滚，那么外层的事务接收到了内层抛出的RuntimeException也会跟着回滚，这个是可以预料到的行为，所以不会有UnexpectedRollbackException。 PROPAGATION_SUPPORTSPROPAGATION_SUPPORTS的特性是如果事务上下文中已经存在一个事务，那么新的事务（传播特性为 PROPAGATION_SUPPORTS）就会和原来的事务共用一个物理事务，其行为和PROPAGATION_REQUIRED一样。但是，如果当前事务上下文中没有事务，那么PROPAGATION_SUPPORTS就按无事务的方式执行代码。1234567891011@Overridepublic void txRollbackInnerTxRollbackPropagationSupports() &#123; supportsTransactionTemplate.execute(new TransactionCallbackWithoutResult() &#123; @Override protected void doInTransactionWithoutResult(TransactionStatus status) &#123; jdbcTemplate.update("insert into user (name, password) values (?, ?)", "Huang", "1111112"); throw new CustomRuntimeException(); &#125; &#125;);&#125; 看上面这段代码，虽然我们在事务（PROPAGATION_SUPPORTS的）中抛出了一个RuntimeException，但是因为其事务上下文中没有事务存在，所以这段代码实际上是以无事务的方式执行的，因此代码中的jdbcTemplate.update()操作也不会被回滚。 PROPAGATION_MANDATORYPROPAGATION_MANDATORY要求事务上下文中必须存在事务，如果事务上下文中存在事务，那么其行为和PROPAGATION_REQUIRED一样。如果当前事务上下文中没有事务，那么就会抛出IllegalTransactionStateException，比如下面这段代码就会这样。 123456789public void txRollbackInnerTxRollbackPropagationMandatory() &#123; mandatoryTransactionTemplate.execute(new TransactionCallbackWithoutResult() &#123; @Override protected void doInTransactionWithoutResult(TransactionStatus transactionStatus) &#123; jdbcTemplate.update("insert into user (name, password) values (?, ?)", "Huang", "1111112"); &#125; &#125;);&#125; PROPAGATION_REQUIRES_NEWPROPAGATION_REQUIRES_NEW无论当前事务上下文中有没有事务，都会开启一个新的事务，并且和原来的事务完全是隔离的，外层事务的回滚不会影响到内层的事务，内层事务的回滚也不会影响到外层的事务（这个说法得稍微打点折扣：因为如果内层抛出RuntimeException的话，那么外层还是会收到这个异常并且触发回滚），我们分析下几段代码。123456789101112131415161718public void txRollbackInnerTxRollbackPropagationRequiresNew() &#123; transactionTemplate.execute(new TransactionCallbackWithoutResult() &#123; @Override protected void doInTransactionWithoutResult(TransactionStatus transactionStatus) &#123; requiresNewTransactionTemplate.execute(new TransactionCallbackWithoutResult() &#123; @Override protected void doInTransactionWithoutResult(TransactionStatus status) &#123; jdbcTemplate.update("insert into user (name, password) values (?, ?)", "Huang", "1111112"); &#125; &#125;); // 外部事务发生回滚，内部事务应该不受影响还是能够提交 throw new RuntimeException(); &#125; &#125;);&#125; 这段代码外层的事务回滚了，但是不会影响到内层的事务的提交，内层事务不受外层的事务的影响。 再看：12345678910111213141516171819public void txRollbackInnerTxRollbackPropagationRequiresNew2() &#123; transactionTemplate.execute(new TransactionCallbackWithoutResult() &#123; @Override protected void doInTransactionWithoutResult(TransactionStatus transactionStatus) &#123; jdbcTemplate.update("insert into user (name, password) values (?, ?)", "Huang", "1111112"); // Nested transaction committed. requiresNewTransactionTemplate.execute(new TransactionCallbackWithoutResult() &#123; @Override protected void doInTransactionWithoutResult(TransactionStatus status) &#123; jdbcTemplate.update("insert into user (name, password) values (?, ?)", "Huang", "1111112"); // 内部事务发生回滚，但是外部事务不应该发生回滚 status.setRollbackOnly(); &#125; &#125;); &#125; &#125;);&#125; 这段代码在内层事务上设置了setRollbackOnly，内层事务肯定会回滚，但是由于内层事务和外层事务是隔离的，所以外层事务不会被回滚。 再看：12345678910111213141516171819public void txRollbackInnerTxRollbackPropagationRequiresNew3() &#123; transactionTemplate.execute(new TransactionCallbackWithoutResult() &#123; @Override protected void doInTransactionWithoutResult(TransactionStatus transactionStatus) &#123; jdbcTemplate.update("insert into user (name, password) values (?, ?)", "Huang", "1111112"); requiresNewTransactionTemplate.execute(new TransactionCallbackWithoutResult() &#123; @Override protected void doInTransactionWithoutResult(TransactionStatus status) &#123; jdbcTemplate.update("insert into user (name, password) values (?, ?)", "Huang", "1111112"); // 内部事务抛出 RuntimeException，外部事务接收到异常，依旧会发生回滚 throw new RuntimeException(); &#125; &#125;); &#125; &#125;);&#125; 这段代码在内层事务抛出了一个RuntimeException，虽然内层事务和外层事务在事务上是隔离，但是RuntimeException显然还会抛到外层去，所以外层事务也会发生回滚。 PROPAGATION_NOT_SUPPORTEDPROPAGATION_NOT_SUPPORTED不管当前事务上下文中有没有事务，代码都会在按照无事务的方式执行. 看下面这段代码：123456789101112131415161718public void txRollbackInnerTxRollbackPropagationNotSupport() &#123; transactionTemplate.execute(new TransactionCallbackWithoutResult() &#123; @Override protected void doInTransactionWithoutResult(TransactionStatus transactionStatus) &#123; jdbcTemplate.update("insert into user (name, password) values (?, ?)", "Huang", "1111112"); notSupportedTransactionTemplate.execute(new TransactionCallbackWithoutResult() &#123; @Override protected void doInTransactionWithoutResult(TransactionStatus status) &#123; jdbcTemplate.update("insert into user (name, password) values (?, ?)", "Huang", "1111112"); &#125; &#125;); // 外部事务回滚，不会把内部的也连着回滚 transactionStatus.setRollbackOnly(); &#125; &#125;);&#125; 上面这段代码中虽然外部事务发生了回滚，但是由于内部的事务是PROPAGATION_NOT_SUPPORTED，根本不在外层的事务范围内，所以内层事务不会发生回滚。 PROPAGATION_NEVERPROPAGATION_NEVER如果当前事务上下文中存在事务，就会抛出IllegalTransactionStateException异常，自己也会按照非事务的方式执行。12345678910111213141516public void txRollbackInnerTxRollbackPropagationNever2() &#123; transactionTemplate.execute(new TransactionCallbackWithoutResult() &#123; @Override protected void doInTransactionWithoutResult(TransactionStatus transactionStatus) &#123; jdbcTemplate.update("insert into user (name, password) values (?, ?)", "Huang", "1111112"); neverTransactionTemplate.execute(new TransactionCallbackWithoutResult() &#123; @Override protected void doInTransactionWithoutResult(TransactionStatus status) &#123; jdbcTemplate.update("insert into user (name, password) values (?, ?)", "Huang", "1111112"); &#125; &#125;); &#125; &#125;);&#125; 比如上面这段代码，在一个PROPAGATION_REQUIRES里面嵌入了一个PROPAGATION_NEVER，内层就会抛出一个IllegalTransactionStateException，导致外层事务被回滚。 PROPAGATION_NESTEDPROPAGATION_NESTED只能应用于像DataSource这样的事务，可以通过在一个事务内部开启一个PROPAGATION_NESTED而达到一个事务内部有多个保存点的效果。一个内嵌的事务发生回滚，只会回滚到它自己的保存点，外层事务还会继续，比如下面这段代码： 12345678910111213141516171819public void txRollbackInnerTxRollbackPropagationNested() &#123; nestedTransactionTemplate.execute(new TransactionCallbackWithoutResult() &#123; @Override protected void doInTransactionWithoutResult(TransactionStatus transactionStatus) &#123; jdbcTemplate.update("insert into user (name, password) values (?, ?)", "Huang", "1111112"); nestedTransactionTemplate.execute(new TransactionCallbackWithoutResult() &#123; @Override protected void doInTransactionWithoutResult(TransactionStatus status) &#123; jdbcTemplate.update("insert into user (name, password) values (?, ?)", "Huang", "1111112"); // 内部事务设置了 rollbackOnly，外部事务应该不受影响，可以继续提交 status.setRollbackOnly(); &#125; &#125;); &#125; &#125;);&#125; 内层的事务发生了回滚，只会回滚其内部的操作，不会影响到外层的事务。]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring</tag>
        <tag>事务隔离级别</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Gitlab日常备份及迁移]]></title>
    <url>%2FGit%2Fgitlab%2FGitlab%E6%97%A5%E5%B8%B8%E5%A4%87%E4%BB%BD%E5%8F%8A%E8%BF%81%E7%A7%BB%2F</url>
    <content type="text"><![CDATA[Gitlab安装体验一文介绍了如何在阿里云上安装Gitlab，安装是很简单方便，但是出于数据安全的考虑，我们需要做一些备份，以防万一。 本文重点介绍Gitlab的日常备份及迁移恢复 Gitlab创建备份使用Gitlab一键安装包安装Gitlab非常简单, 同样的备份恢复与迁移也非常简单. 使用一条命令即可创建完整的Gitlab备份: 1gitlab-rake gitlab:backup:create 使用以上命令会在/var/opt/gitlab/backups目录下创建一个名称类似为1393513186_gitlab_backup.tar的压缩包, 这个压缩包就是Gitlab整个的完整部分, 其中开头的1393513186是备份创建的日期。 Gitlab修改备份文件默认目录你也可以通过修改/etc/gitlab/gitlab.rb来修改默认存放备份文件的目录 1gitlab_rails['backup_path'] = '/mnt/backups' /mnt/backups修改为你想存放备份的目录即可，修改完成之后使用gitlab-ctl reconfigure命令重载配置文件即可。 Gitlab恢复备份同样, Gitlab的从备份恢复也非常简单 123456789# 停止相关数据连接服务gitlab-ctl stop unicorngitlab-ctl stop sidekiq# 从1393513186编号备份中恢复gitlab-rake gitlab:backup:restore BACKUP=1393513186_gitlab_backup# 启动Gitlabsudo gitlab-ctl start Gitlab迁移迁移如同备份与恢复的步骤一样，只需要将老服务器/var/opt/gitlab/backups目录下的备份文件拷贝到新服务器上的/var/opt/gitlab/backups即可(如果你没修改过默认备份目录的话)。 但是需要注意的是新服务器上的Gitlab的版本必须与创建备份时的Gitlab版本号相同。 比如新服务器安装的是最新的7.60版本的Gitlab，那么迁移之前，最好将老服务器的Gitlab升级为7.60在进行备份。 Gitlab定时自动异地备份由于Gitlab部署在了阿里云上，把Gitlab再备份在本地意义不是特别大，所以这里利用了家里的NAS做了一个简单的异地备份。 服务器备份脚本在阿里云服务器上创建如下脚本，存储在/home/scripts/gitlab_backup.sh文件中 1234567891011121314151617181920212223242526272829303132333435#!/bin/bash# Gitlab自动备份脚本#Gitlab 备份地址LocalBackDir=/var/opt/gitlab/backups#备份日志文件LogFile=$LocalBackDir/remote_backup.log#新建备份日志文件touch $LogFileecho "-------------------------------------------------------------------------" &gt;&gt; $LogFile#记录本地生成gitlab备份日志echo "Gitlab auto backup at local server, start at $(date +"%Y-%m-%d %H:%M:%S")" &gt;&gt; $LogFile#执行gitlab本地备份gitlab-rake gitlab:backup:create &gt;&gt; $LogFile 2&gt;&amp;1# $?符号显示上一条命令的返回值，如果为0则代表执行成功，其他表示失败if [ $? -eq 0 ];then #追加日志到日志文件 echo "Gitlab auto backup at local server successed at $(date +"%Y-%m-%d %H:%M:%S")" &gt;&gt; $LogFileelse #追加日志到日志文件 echo "Gitlab auto backup at local server failed at $(date +"%Y-%m-%d %H:%M:%S")" &gt;&gt; $LogFilefi#查找本地备份目录修改时间为10分钟以内且后缀为.tar的Gitlab备份文件Backfile_Send_To_Remote=`find $LocalBackDir -type f -mmin -10 -name '*.tar' | tail -1` &gt;&gt; $LogFile 2&gt;&amp;1echo $Backfile_Send_To_Remote NAS免密登录阿里云异地备份思路需要在NAS上登录到阿里云服务器上执行对应的备份命令并下载文件，所以需要让NAS可以免密登录阿里云服务器，以定时自动执行这个任务。 免密登录的配置，可以参考SSH免密登录配置 NAS服务器脚本SSH进入NAS，然后创建脚本，自动在服务器上执行备份，并下载到NAS本地来。脚本存储在/volume1/homes/gitlab_back/scripts/gitlab_backup_download.sh文件中。 123456789101112131415161718192021222324252627282930313233343536373839#!/bin/bash# 远程登录gitlab服务器，执行自动备份脚本，然后传输至本地#Gitlab服务器RemoteServer=git.ufeng.topRemoteServerUser=root#Gitlab服务器备份地址RemoteBackDir=/var/opt/gitlab/backups#NAS本地备份地址LocalBackDir=/volume1/homes/gitlab_back#备份日志文件LogFile=$LocalBackDir/remote_backup.log#新建备份日志文件touch $LogFileecho "-------------------------------------------------------------------------" &gt;&gt; $LogFile#记录NAS下载gitlab备份日志echo "Gitlab backup auto download at NAS, start at $(date +"%Y-%m-%d %H:%M:%S")" &gt;&gt; $LogFile#远程登录gitlab服务器并执行备份脚本，获取备份文件的名字result=`ssh $RemoteServerUser@$RemoteServer "sh /home/scripts/gitlab_backup.sh"`#远程下载备份文件到本地scp $RemoteServerUser@$RemoteServer:$result $LocalBackDirecho "Gitlab remote backup file is $&#123;result&#125;" &gt;&gt; $LogFile# 备份结果追加到备份日志if [ $? -eq 0 ];then echo "" echo "$(date +"%Y-%m-%d %H:%M:%S") Gitlab Remote download Succeed!" &gt;&gt; $LogFileelse echo "$(date +"%Y-%m-%d %H:%M:%S") Gitlab Remote download Failed!" &gt;&gt; $LogFilefi 然后配置群晖的定时任务，每天自动执行。 自动清理历史备份文件由于阿里云的存储空间有限，所以对Gitlab的历史备份文件需要定期删除，释放磁盘空间。 同样在/home/scripts/gitlab_auto_del_backup.sh文件中存储自动清理历史备份的脚本内容。123456789101112131415161718192021222324252627#!/bin/bash#清理多余的历史备份#Gitlab 备份地址LocalBackDir=/var/opt/gitlab/backups#备份日志文件LogFile=$LocalBackDir/remote_clean.log#新建备份日志文件touch $LogFileecho "--------------------------------------------------------------------- " &gt;&gt; $LogFileecho "Gitlab auto clean local backup, start at $(date +"%Y-%m-%d %H:%M:%S")" &gt;&gt; $LogFile# 找到3*24*60分钟前，以tar结尾的文件并删除find $LocalBackDir -type f -mmin +4320 -name "*.tar" -exec rm -rf &#123;&#125; \;# $?符号显示上一条命令的返回值，如果为0则代表执行成功，其他表示失败if [ $? -eq 0 ];then #追加日志到日志文件 echo "Gitlab auto clean local backup success at $(date +"%Y-%m-%d %H:%M:%S")" &gt;&gt; $LogFileelse #追加日志到日志文件 echo "Gitlab auto clean local backup failed at $(date +"%Y-%m-%d %H:%M:%S")" &gt;&gt; $LogFilefi 然后赋予执行权限1chmod u+x /home/scripts/gitlab_auto_del_backup.sh 配置定时任务123crontab -e0 3 * * * /home/scripts/gitlab_auto_del_backup.sh]]></content>
      <categories>
        <category>Git</category>
        <category>gitlab</category>
      </categories>
      <tags>
        <tag>Gitlab</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Gitlab安装体验]]></title>
    <url>%2FGit%2Fgitlab%2FGitlab%E5%AE%89%E8%A3%85%E4%BD%93%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[由于项目开发代码托管需求，需要在阿里云上部署一套独立的Gitlab仓库，来进行项目代码管控。 GitLab是由GitLab Inc.开发，使用MIT许可证的基于网络的Git仓库管理工具，且具有wiki和issue跟踪功能。 本文记录了如何在阿里云上安装部署自己的Gitlab仓库。 Gitlab安装官方的安装教程：https://about.gitlab.com/installation/#centos-7 进入官方安装教程，我们发现 Gitlab 提供了很多不同的版本，如下 我的阿里云系统是 CentOS7 , 所以我直接选择 CentOS7 。然后下面就会出现安装的命令。 第一步 防火墙放行在系统防火墙中打开HTTP和SSH访问，有部署自己的Gitlab的同学一定会这种操作，这里不展开了。 第二步 安装邮件通知服务安装Postfix 邮件通知服务，其实这一步是可以省略的，在 Gitlab 安装完成后还可以配置。安装命令如下，依次运行下面的命令就OK了。 123sudo yum install postfixsudo systemctl enable postfixsudo systemctl start postfix 第三步 安装软件包安装 Gitlab 软件包，这个才是真正的主角。 1curl https://packages.gitlab.com/install/repositories/gitlab/gitlab-ee/script.rpm.sh | sudo bash 接下来就是漫长的安装过程了，这个没有办法，只能耐心等待。 第四步 安装Gitlab配置 Gitlab 访问的域名并安装，这里我使用了自己注册的一个域名，并配置了一个git的二级域名专门用于访问。命令中的gitlab.example.com需要替换成自己的域名。如果服务器没有绑定域名，这里也可以先使用公网ip代替。这个 域名/ip 以后可以在浏览器中访问 Gitlab 服务。安装配置命令如下： 1sudo EXTERNAL_URL=&quot;http://gitlab.example.com&quot; yum install -y gitlab-ee 下载完成后会自动安装，直至安装完成。 到这里就表示 GitLab 已经安装完成了。图中的http地址就可以直接访问gitlab了，版本号是：gitlab-ee , 10.4.0 第五步 设置初始密码下面我们在浏览器中访问刚刚配置的地址，就可以看到Gitlab的页面了 需要设置初始密码，连续输入两遍，然后点击下面的按钮。密码设置完成后，就会跳转到登录界面，登录用户名默认是 root，密码就是刚才设置的 登录完成后，就可以看到如下的界面。 至此，Gitlab 已经安装完成了。 修改端口Gitlab本身采用80端口，如安装前服务器有占用80，安装完访问会报错。需更改Gitlab的默认端口，比如我们将Gitlab的默认端口改为9090 。 第一步 更改配置文件修改 Gitlab 默认端口配置打开/etc/gitlab/gitlab.rb文件，找到external_url字段，如下图所示，将原先配置的IP或者域名后面加上对应的端口号 第二步 重启然后重新加载配置并重启应用程序12gitlab-ctl reconfiguregitlab-ctl restart 然后在浏览器中访问对应的域名+端口就可以看到Gitlab的页面了。 可选的第三步如果就是想用80端口来访问，这里提供一种思路，使用Nginx转发对应的端口，这里贴下对应的Nginx配置文件，不再具体描述其中的步骤。 1234567891011server &#123; listen 80; server_name git.example.com; location / &#123; proxy_set_header HOST $host; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://localhost:9090/; &#125;&#125;]]></content>
      <categories>
        <category>Git</category>
        <category>gitlab</category>
      </categories>
      <tags>
        <tag>Gitlab</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SSH免密登录]]></title>
    <url>%2FLinux%2FSSH%E5%85%8D%E5%AF%86%E7%99%BB%E5%BD%95%2F</url>
    <content type="text"><![CDATA[Linux系统有一个钥匙环(keyring)的管理程序．钥匙环受到用户登录密码的保护．当你登录Linux系统时，会自动解开钥匙环的密码，从而可访问钥匙环．SSH的密钥和公钥也存储在钥匙环．所以初次使用SSH密钥登录远程Linux服务器时需要输入一次SSH密钥的密码．而将来使用SSH密钥登录时不再输入密码．Ubuntu的钥匙环程序是seahorse． SSH密钥就好比是你的身份证明．远程Linux服务器用你生成的SSH公钥来加密一条消息，而只有你的SSH密钥可以解开这条消息．所以其他人如果没有你的SSH密钥，是无法解开加密消息的，从而也就无法登录你的Linux服务器． 本文主要介绍SSH免密登录的原理，以及如何配置Linux免密登录。 心急的可以直接跳到操作指南公钥免密登录配置 什么是SSHSSH（Secure Shell）是一种通信加密协议，加密算法包括：RSA、DSA等。 简单说，SSH是一种网络协议，用于计算机之间的加密登录。 如果一个用户从本地计算机，使用SSH协议登录另一台远程计算机，我们就可以认为，这种登录是安全的，即使被中途截获，密码也不会泄露。 需要指出的是，SSH只是一种协议，存在多种实现，既有商业实现，也有开源实现。本文针对的实现是OpenSSH，它是自由软件，应用非常广泛。 需要指出的是，SSH只是一种协议，存在多种实现，既有商业实现，也有开源实现。本文针对的实现是OpenSSH，它是自由软件，应用非常广泛。 中间人攻击SSH之所以能够保证安全，原因在于它采用了公钥加密。 整个过程是这样的：（1）远程主机收到用户的登录请求，把自己的公钥发给用户。（2）用户使用这个公钥，将登录密码加密后，发送回来。（3）远程主机用自己的私钥，解密登录密码，如果密码正确，就同意用户登录。 这个过程本身是安全的，但是实施的时候存在一个风险：如果有人截获了登录请求，然后冒充远程主机，将伪造的公钥发给用户，那么用户很难辨别真伪。因为不像https协议，SSH协议的公钥是没有证书中心（CA）公证的，也就是说，都是自己签发的。 可以设想，如果攻击者插在用户与远程主机之间（比如在公共的wifi区域），用伪造的公钥，获取用户的登录密码。再用这个密码登录远程主机，那么SSH的安全机制就荡然无存了。这种风险就是著名的”中间人攻击”（Man-in-the-middle attack）。 SSH协议是如何应对的呢？ 口令登录如果你是第一次登录对方主机，系统会出现下面的提示：1234$ ssh user@hostThe authenticity of host &apos;host (12.18.429.21)&apos; can&apos;t be established.RSA key fingerprint is 98:2e:d7:e0:de:9f:ac:67:28:c2:42:2d:37:16:58:4d.Are you sure you want to continue connecting (yes/no)? 这段话的意思是，无法确认host主机的真实性，只知道它的公钥指纹，问你还想继续连接吗？ 所谓”公钥指纹”，是指公钥长度较长（这里采用RSA算法，长达1024位），很难比对，所以对其进行MD5计算，将它变成一个128位的指纹。上例中是98:2e:d7:e0:de:9f:ac:67:28:c2:42:2d:37:16:58:4d，再进行比较，就容易多了。 很自然的一个问题就是，用户怎么知道远程主机的公钥指纹应该是多少？回答是没有好办法，远程主机必须在自己的网站上贴出公钥指纹，以便用户自行核对。 假定经过风险衡量以后，用户决定接受这个远程主机的公钥。 1Are you sure you want to continue connecting (yes/no)? yes 系统会出现一句提示，表示host主机已经得到认可。1Warning: Permanently added &apos;host,12.18.429.21&apos; (RSA) to the list of known hosts. 然后，会要求输入密码。1Password: (enter password) 如果密码正确，就可以登录了。 当远程主机的公钥被接受以后，它就会被保存在文件$HOME/.ssh/known_hosts之中。下次再连接这台主机，系统就会认出它的公钥已经保存在本地了，从而跳过警告部分，直接提示输入密码。 每个SSH用户都有自己的known_hosts文件，此外系统也有一个这样的文件，通常是/etc/ssh/ssh_known_hosts，保存一些对所有用户都可信赖的远程主机的公钥。 公钥免密登录配置使用密码登录，每次都必须输入密码，非常麻烦。好在SSH还提供了公钥登录，可以省去输入密码的步骤。 所谓”公钥登录”，原理很简单，就是用户将自己的公钥储存在远程主机上。登录的时候，远程主机会向用户发送一段随机字符串，用户用自己的私钥加密后，再发回来。远程主机用事先储存的公钥进行解密，如果成功，就证明用户是可信的，直接允许登录shell，不再要求密码。 为了描述方便，假设服务器A需要配置免密登录服务器B 生成用户自己的公钥在服务器A上，在用户主目录下，生成用户自己的公钥1ssh-keygen -t rsa 将会在家目录下的隐藏目录/.ssh下生成文件：123456id_rsa.pub //公钥 id_rsa //密钥``## 追加公钥可以使用ssh-copy-id命令来完成 ssh-copy-id username@remote-server123456输入远程用户的密码后，SSH公钥就会自动上传了．SSH公钥保存在远程Linux服务器的.ssh/authorized_keys文件中．上传完成后，SSH登录就不需要再次输入密码了．但是首次使用SSH Key登录时需要输入一次SSH密钥的加密密码．（只需要输入一次，将来会自动登录，不再需要输入密钥的密码．）如果还是不行，就打开远程主机的/etc/ssh/sshd_config这个文件，检查下面几行前面&quot;#&quot;注释是否取掉。 RSAAuthentication yesPubkeyAuthentication yesAuthorizedKeysFile .ssh/authorized_keys`]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>SSH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS服务器挂载Swap分区]]></title>
    <url>%2FLinux%2FCentOS%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%8C%82%E8%BD%BDSwap%E5%88%86%E5%8C%BA%2F</url>
    <content type="text"><![CDATA[经常购买使用一些Linux的服务器，有些厂商提供的CentOS模板中并没有配置swap分区，这就使得服务器的内存有点捉襟见肘（尤其是小内存的情况下）。本文也就记录如何在CentOS系统下给服务器添加swap分区， 增大内存。 什么是swap分区在开始前，我们还是先了解一下基础知识，什么是swap分区。已经了解的也可以直接跳到操作步骤 Linux中Swap（即：交换分区），类似于Windows的虚拟内存，就是当内存不足的时候，把一部分硬盘空间虚拟成内存使用，从而解决内存容量不足的情况。 交换分区，英文的说法是swap，意思是“交换”、“实物交易”。它的功能就是在内存不够的情况下，操作系统先把内存中暂时不用的数据，存到硬盘的交换空间，腾出内存来让别的程序运行，和Windows的虚拟内存（pagefile.sys）的作用是一样的。 需要注意的是，虽然这个SWAP分区能够作为”虚拟”的内存,但它的速度比物理内存可是慢多了，因此如果需要更快的速度的话,并不能寄厚望于SWAP，最好的办法仍然是加大物理内存。 SWAP分区只是临时的解决办法。 如何配置swap分区 添加Swap分区通常有两种方法： 1.使用未划分的磁盘空间，创建一个分区，然后格式化为swap格式，之后挂载使用。 2.使用dd命令创建一个整块文件，然后格式化为swap格式，作为swap分区使用，之后挂载使用。当然这种方法创建的swap分区性能比第一种方法差一些。 我这里因为整个磁盘在系统初始化的的时候都已经被全部使用，所以只能采取第二种方法。 1.使用dd命令创建一个4G的整块文件1dd if=/var/zero of=/var/swap bs=1M count=4096 2.把/var/swap格式化为swap分区1mkswap /var/swap 3.使用swapon命令开启交换分区1swapon /var/swap 现在使用free -m命令查看，就可以看到Swap已经可以用了。12345free -m total used free shared buffers cachedMem: 1875 1808 67 0 6 1389-/+ buffers/cache: 412 1463Swap: 4095 0 4095 4.设置开机自动挂载Swap分区前面使用swapon开启交换分区的操作，在系统重启之后就会失效。所以，我们这里需要通过修改/etc/fstab文件，设置成开机自动挂载Swap分区。 1echo &apos;/var/swap swap ext3 defaults 0 0&apos; &gt;&gt; /etc/fstab 5.设置Swap分区使用规则Swap分区虽然已经启用，但是used一直为0。这是因为Swap分区的启用是有一定规则的。我们可以查看/proc/sys/vm/swappiness文件。12cat /proc/sys/vm/swappiness0 这个值的意思就是：当内存使用100%-0%=100%的时候，采用Swap分区。 当然，这个0的意思并不是绝对的当内存用完了到时候，才使用Swap，只是说尽可能不使用Swap。 阿里云服务器默认这个值为0，是因为，采用Swap会频繁读取硬盘，加大IO负担,所以让程序运行尽可能的使用内存而不是Swap。当然，当内存吃紧的时候，还是要用的。 这个值通常设置为40%-60%。 1echo &quot;40&quot;&gt;/proc/sys/vm/swappiness 这种修改方式只会临时有效，当系统重启之后，就会失效。想要彻底有效需要修改/etc/sysctl.conf配置文件，里面有一参数vm.swappiness = 0，把它修改为需要的值。 1vim /etc/sysctl.conf 这是因为系统启动的时候，会先读取/etc/sysctl.conf里面的参数vm.swappiness。通过这个参数来设置/proc/sys/vm/swappiness的值。 现在查看，Swap就可以被使用了。 删除swap分区1.首先将Swap文件取消激活：1swapoff /var/swap 2.然后删除设置的Swap文件1rm -rf /var/swap 3.最后取消自动挂载编辑/etc/fstab中新增的一行自动挂载配置]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[组合与聚合的区别]]></title>
    <url>%2FUML%E8%AE%BE%E8%AE%A1%2F%E7%BB%84%E5%90%88%E4%B8%8E%E8%81%9A%E5%90%88%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[组合和聚合是有很大区别的，这个区别不是在形式上，而是在本质上： 比如A类中包含B类的一个引用b，当A类的一个对象消亡时，b这个引用所指向的对象也同时消亡（没有任何一个引用指向它，成了垃圾对象），这种情况叫做组合。 反之b所指向的对象还会有另外的引用指向它，这种情况叫聚合。 一言以蔽之举个栗子： 现实生活中，人和手，脚是组合关系，因为当人死亡后人的手也就不复存在了。人和他的电脑是聚合关系。 代码描述12class Hand&#123;&#125; 12class Computer&#123;&#125; 组合：1234567class Person&#123; private Hand hand; public Person()&#123; hand = new Hand(); &#125;&#125; 聚合：1234567class Person&#123; private Computer computer; public setComputer()&#123; computer = new Computer(); &#125;&#125; 可以说聚合是一种强组合的关系 组合与聚合的区别和联系组合与聚合都是整体与部分的关系.组合的关系更强一点，对组合关系来说，如果失去部分，整体也将不存在了。 代码实现上来看： 组合：在整体的构造器中实例化部分，这个部分不能被其他实例共享。整体与部分的生命周期是同步的。 聚合关系的部分,可以在构造器中通过参数传递的形式进行初始化。 从数据库的层面上看： 组合关系：需要级联删除 聚合关系不需要。]]></content>
      <categories>
        <category>UML设计</category>
      </categories>
      <tags>
        <tag>UML</tag>
        <tag>组合</tag>
        <tag>聚合</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[虚拟机线程状态 vs 操作系统线程状态]]></title>
    <url>%2FJava%2FJava%E5%9F%BA%E7%A1%80%2F%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%BA%BF%E7%A8%8B%E7%8A%B6%E6%80%81%E4%B8%8E%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E7%BA%BF%E7%A8%8B%E7%8A%B6%E6%80%81%2F</url>
    <content type="text"><![CDATA[线程的状态一文中我们曾经介绍了线程的状态，注意这里描述的是Java中定义的6种状态，是虚拟机层面上暴露给我们的状态，这些状态是由枚举类Thread.State中明确定义的。 也许有同学就要问了，为什么我之前还听说过线程只有只有5种状态呢？是哪里有问题吗？其实，不论是6种状态还是5种状态，这两种说法都是对的，只不过他们描述的对象不同，一个是Java虚拟机的线程状态，一个是操作系统中的线程状态。 本文主要介绍Java虚拟机线程状态和操作系统线程状态的关系。 你可能听说过这样的说法，比如在Windows系统下，很多的虚拟机实现实际都把Java线程一一映射到操作系统的内核线程kernel thread上。当然，除了1:1映射，还可能有N:1或者N:M映射。总之，世界很乱。。。 自然，操作系统的线程也有它自己的状态。你Java有6种，Windows可能有N种，到了Linux系统，它可能有M种，加上各种操作系统版本满天乱飞，我也不知道具体有多少种了….. 但是！！虚拟机层的存在，统一了这些差别，不管它是N还是M种，到了Java层面它们都被映射到了6种状态上来。因此，两个层面上有很多状态其实是大同小异的。 我们前面提到的5种状态，其实是很多操作系统教科书上的“官方说法”，而且指的是进程的状态。 不幸的是，有很多的书上常常把这些进程状态，线程状态与Java线程状态混在一起谈。 这里所谓“进程状态”，指早期的那种“单线程进程”的状态。对于现在普遍的“多线程进程”，显然，谈论“进程状态”已经没有意义，应该谈论“进程下某个线程的状态”或者直接说“线程状态”。 不过有时还是会把“进程状态”和“线程状态”混着去说。有些系统把线程叫成“轻量级进程”（light-weight process），所以还是在谈“进程状态”。有时则甚至既不叫“进程”，也不叫“线程”，它们叫“task”或者“job”。 总之还是有些乱的，我们不妨就拿Windows系统为例，用的就是“进程”和“线程”这两种较为标准的叫法，这时一个进程下至少有一个线程，线程是CPU调度的基本单位，进程不参与CPU调度，CPU根本不知道进程的存在。你在“任务管理器”中看到的所谓“进程状态”，跟线程状态不是一回事。 至于Java线程的状态，有的说有4种状态，有的说有5种，各种各样的说法都有。 比如看到Java只有RUNNABLE（可运行的）状态，就觉得这还不够呀，应该还有Running（运行中）状态； 又或者觉得RUNNABLE就是Running，所以应该还有个Ready（就绪）状态才对。 然而这些说法都是不准确的！如果我们读下Thread.State源码中的注释中，它说得很清楚： These states are virtual machine states which do not reflect any operating system thread states。这些状态是虚拟机状态，它不反映任何操作系统的线程状态。 一个 Java 线程它所对应的操作系统内核线程中的状态可能有Running又有Ready，但在虚拟机层面则统一映射成了RUNNABLE。如果Java中觉得没必要去区分这些，我们又何必去纠结这些呢？ 以RUNNABLE为例，源码中的注释是这样说的：executing in the Java virtual machine（正在Java虚拟机中执行）至于它是否真正在执行，不是我们要操心的事。 还有的情况则比如把Java状态中的BLOCKED，WAITING，TIMED_WAITING三种状态都笼统地称为blocked或者waiting； 操作系统也许只有一种状态，但这一次，Java作了细分，给出了三种状态。 很多声称Java线程只有4种或5种状态常常都是自作主张地合并了这些状态，把这些东西混为一谈是非常容易引发混乱的。我们将会在后面具体地谈到。 又或者把TIMED_WAITING当作不存在，从来不提有这个状态。 显然，这种做法又是受到传统进程状态划分的影响。尽管它与WAITING很像，我们最好按着Thread.State中的定义来，不要自己随意发挥。 综上所述，为避免出现混乱，厘清概念所处的层次是非常重要的。如无特别说明，讨论均在JVM层面上。]]></content>
      <categories>
        <category>Java</category>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>线程</tag>
        <tag>虚拟机线程</tag>
        <tag>操作系统线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo自动部署优化——解决TTFB过长的问题]]></title>
    <url>%2FHexo%E5%8D%9A%E5%AE%A2%2FHexo%E8%87%AA%E5%8A%A8%E9%83%A8%E7%BD%B2%E4%BC%98%E5%8C%96%E2%80%94%E2%80%94%E8%A7%A3%E5%86%B3TTFB%E8%BF%87%E9%95%BF%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[差不多去年这个时候，自己萌生了玩VPS和搭建自己的博客的想法，眨眼间博客也运行快一年了。博客使用过程中，中途发现自己的站点打开速度很慢，之前也零零碎碎地看过这个问题，但是一直没能解决。今天周末好不容易闲下来，终于研究出了问题所在。 博客访问慢的直接原因在于，网站的 TTFB 等待过长，关于什么是TTFB，我们文末再介绍。问题的解决方案，就是，之前VPS上自动部署的方式不对，没有依赖静态页面，而是利用了Hexo的服务器。通过直接访问Hexo提前生成好的静态页面，博客加载速度有了明显提高。 背景及起因关于之前的自动部署方式，可以参考这篇文章 自动将更新部署到VPS.md 当时采用的在博客上运行hexo server来提供网站服务。实际上，这种方式是本地编写文章时测试用的，用在VPS上提供博客站点服务，就会显得访问速度没有理想的那么快（因为这是实时编译生成的网页，非静态的资源页面），TTFB时间过长。 解决方案找到了问题，就可以很方便的对症下药了。 Hexo提供了hexo -g来生成博客中所有文章对应的静态页面，我们要做的就是在访问博客的时候直接访问VPS上的静态页面。 多分支管理为了方便管理，我新建了一个分支dev，在这个分支上进行博客的编写，同时删除了原先在Master分支上的全部内容1234567891011# 新建分支git checkout -b devgit push origin dev# 清空Master分支内容git checkout master# @@注意备份@@rm -rf *git add .git commit -m "rm all files in master"git push origin master 自动部署Master分支我们借助hexo -d命令来实现自动部署。 首先，修改站点的_config.yml配置文件，修改其中的deploy节点123456# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repo: https://github.com/winsky94/HexoBlog.git branch: master 这样通过使用hexo -d命令可以来自动部署文章到github。如果提示错误，可能你需要安装hexo-deployer-git1npm install hexo-deployer-git --save 自动部署到VPS上一步的deploy参数正确配置后，文章写完使用hexo g -d命令就可以直接部署，并提交到GitHub上的Master分支。 然后在VPS上clone下来Master分支，我的存储路径是/home/blog/HexoBlog，然后借助Nginx提供静态站点的访问 什么，你还不知道Nginx？出门左转谷歌一下，你就知道 修改原先的/usr/local/nginx/conf/vhost/blog.conf，改成如下内容12345678910111213141516171819202122232425262728293031323334server&#123; listen 443; server_name blog.winsky.wang ; ssl on; ssl_certificate /etc/letsencrypt/live/blog.winsky.wang/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/blog.winsky.wang/privkey.pem; index index.html index.htm index.php default.html default.htm default.php; #这里要改成网站的根目录 root /home/blog/HexoBlog; include other.conf; #error_page 404 /404.html; location ~ .*\.(ico|gif|jpg|jpeg|png|bmp|swf)$ &#123; access_log off; expires 1d; &#125; location ~ .*\.(js|css|txt|xml)?$ &#123; access_log off; expires 12h; &#125;&#125;server&#123; server_name blog.winsky.wang ; listen 80; rewrite ^/(.*) https://$server_name$1 permanent;#跳转到Https&#125; 然后重启一下Nginx站点配置nginx -s reload，然后再访问https://blog.winsky.wang/，发现站点已经快了很多。爽歪歪，有木有！！ 先别急着爽，作为一个大忙（lan）人，我可不想每次更新都要登上VPS手动拉取最新的更新。之前我们使用了webhook来自动部署，现在还一样，不过脚本要有一点小小的变动了。 修改之前的deploy.sh脚本，更新内容如下123cd /home/blog/HexoBloggit reset --hardgit pull origin master 每次写完文章，执行一下hexo clean &amp;&amp; hexo generate &amp;&amp; hexo deploy就可以借助webhook自动更新VPS上的文件内容了。是不是真的很方便，要不要点个赞！ 修改推送到dev分支网站页面是保存了，部署也自动执行了，但这时候我们还没有保存我们的hexo原始文件，包括我们的文章md文件，我们千辛万苦修改的主题配置等。。。 接下来使用下面的步骤将他们都统统推送到hexo分支上去。123git add .git commit -m “change description”git push origin dev 这样就OK了，万事大吉~ 什么是 TTFBTTFB 是 Time to First Byte 的缩写，指的是浏览器开始收到服务器响应数据的时间（后台处理时间+重定向时间），是反映服务端响应速度的重要指标。 就像你问朋友了一个问题，你的朋友思考了一会儿才给你答案，你朋友思考的时间就相当于 TTFB。你朋友思考的时间越短，就说明你朋友越聪明或者对你的问题越熟悉。 对服务器来说，TTFB 时间越短，就说明服务器响应越快。]]></content>
      <categories>
        <category>Hexo博客</category>
      </categories>
      <tags>
        <tag>HEXO</tag>
        <tag>博客</tag>
        <tag>Next</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[QQ音乐歌词接口]]></title>
    <url>%2F%E7%88%AC%E8%99%AB%2FQQ%E9%9F%B3%E4%B9%90%E6%AD%8C%E8%AF%8D%E6%8E%A5%E5%8F%A3%2F</url>
    <content type="text"><![CDATA[之前在公司实习的时候，做过一个QQ音乐歌词爬虫的项目，期间网上找了不少参考资料，自己也研究过QQ音乐的js解析，这里简单做个记录，以供以后参考。 单曲搜索接口访问链接1http://c.y.qq.com/soso/fcgi-bin/search_cp?t=0&amp;aggr=1&amp;cr=1&amp;catZhida=1&amp;lossless=0&amp;flag_qc=0&amp;p=1&amp;w=#&#123;1&#125;&amp;n=#&#123;2&#125;&amp;g_tk=938407465&amp;loginUin=0&amp;hostUin=0&amp;format=json&amp;inCharset=utf8&amp;outCharset=utf-8&amp;notice=0&amp;platform=yqq&amp;needNewCode=0 w表示的是搜索关键词 n表示的是结果返回的个数 format表示返回结果的格式，QQ原本的format方式是jsonp，这里改成json，使其返回的结果变成json格式的数据 返回结果示例目前使用的几个主要属性如下： data-song-list是搜索的结果 albumid 专辑id albummid 专辑mid albumname 专辑名称 grp QQ音乐搜索结果页面上有些结果是折腾显示的，折叠的数据就在这里面，其基本结构跟list的一个元素是一样的 singer 歌手列表，每个歌手是singer下的一个元素 id 歌手id mid 歌手mid name 歌手名 songid 歌曲id songmid 歌曲mid songname 歌曲名称 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586&#123; &quot;code&quot;: 0, &quot;data&quot;: &#123; &quot;keyword&quot;: &quot;明天你好&quot;, &quot;priority&quot;: 0, &quot;qc&quot;: [], &quot;semantic&quot;: &#123; &quot;curnum&quot;: 0, &quot;curpage&quot;: 1, &quot;list&quot;: [], &quot;totalnum&quot;: 0 &#125;, &quot;song&quot;: &#123; &quot;curnum&quot;: 1, &quot;curpage&quot;: 1, &quot;list&quot;: [&#123; &quot;albumid&quot;: 75139, &quot;albummid&quot;: &quot;003K4mFV3B9UfM&quot;, &quot;albumname&quot;: &quot;Lost &amp; Found去寻找&quot;, &quot;albumname_hilight&quot;: &quot;Lost &amp; Found去寻找&quot;, &quot;alertid&quot;: 100002, &quot;chinesesinger&quot;: 0, &quot;docid&quot;: &quot;5363839958007993519&quot;, &quot;format&quot;: &quot;qqhq;common;mp3common;wmacommon&quot;, &quot;grp&quot;: [], &quot;interval&quot;: 271, &quot;isonly&quot;: 1, &quot;lyric&quot;: &quot;《加油吧实习生》电视剧插曲&quot;, &quot;lyric_hilight&quot;: &quot;《加油吧实习生》电视剧插曲&quot;, &quot;media_mid&quot;: &quot;002OrhQA0bNYFg&quot;, &quot;msgid&quot;: 14, &quot;nt&quot;: 10000, &quot;pay&quot;: &#123; &quot;payalbum&quot;: 0, &quot;payalbumprice&quot;: 0, &quot;paydownload&quot;: 1, &quot;payinfo&quot;: 1, &quot;payplay&quot;: 1, &quot;paytrackmouth&quot;: 1, &quot;paytrackprice&quot;: 200 &#125;, &quot;preview&quot;: &#123; &quot;trybegin&quot;: 60926, &quot;tryend&quot;: 103574, &quot;trysize&quot;: 683362 &#125;, &quot;pubtime&quot;: 1310313600, &quot;pure&quot;: 0, &quot;singer&quot;: [&#123; &quot;id&quot;: 4422, &quot;mid&quot;: &quot;0012bj8d36Xkw1&quot;, &quot;name&quot;: &quot;牛奶咖啡&quot;, &quot;name_hilight&quot;: &quot;牛奶咖啡&quot; &#125;], &quot;size128&quot;: 4350319, &quot;size320&quot;: 10875498, &quot;sizeape&quot;: 28565524, &quot;sizeflac&quot;: 29182620, &quot;sizeogg&quot;: 6509450, &quot;songid&quot;: 7109361, &quot;songmid&quot;: &quot;002OrhQA0bNYFg&quot;, &quot;songname&quot;: &quot;明天，你好&quot;, &quot;songname_hilight&quot;: &quot;&lt;span class=\&quot;c_tx_highlight\&quot;&gt;明天&lt;/span&gt;，&lt;span class=\&quot;c_tx_highlight\&quot;&gt;你好&lt;/span&gt;&quot;, &quot;songurl&quot;: &quot;http://y.qq.com/#type=song&amp;id=7109361&quot;, &quot;stream&quot;: 9, &quot;switch&quot;: 636675, &quot;t&quot;: 0, &quot;tag&quot;: 0, &quot;type&quot;: 0, &quot;ver&quot;: 0, &quot;vid&quot;: &quot;I0010NL1U5Y&quot; &#125;], &quot;totalnum&quot;: 46 &#125;, &quot;totaltime&quot;: 0, &quot;zhida&quot;: &#123; &quot;chinesesinger&quot;: 0, &quot;type&quot;: 0 &#125; &#125;, &quot;message&quot;: &quot;&quot;, &quot;notice&quot;: &quot;&quot;, &quot;subcode&quot;: 0, &quot;time&quot;: 1471404983, &quot;tips&quot;: &quot;&quot;&#125; 歌曲详情页访问链接1http://y.qq.com/portal/song/#&#123;1&#125;.html #{1}处填写的是歌曲的mid这是通用情况，绝大部分歌曲都是用的这个链接来访问 但是，存在一小部分歌曲，songid等一系列基本数据都是0，后经部分观察发现，使用上面那个url来获得歌曲详情的歌曲，type都是0对于songid为0的这部分歌曲，目前来看是type=2，应该还有其他类型，但是没有发现对应的歌曲。这部分歌曲，他的访问链接和正常歌曲是不一样的。12http://y.qq.com/portal/song2/#&#123;1&#125;/#&#123;2&#125;.htmlhttp://y.qq.com/portal/song2/46/16783190164570565401.html 其中，#{2}对应于搜索json的docid,#{1}没弄清楚是什么东西，song2后面接的都是46.至于song2，这是js中生成的，目前猜测是对应于type=2中的2。其他的因为没有找到对应的歌曲，所以也就没有深入去分析QQ那边的js。对于绝大部分歌曲来说，QQ那边的songid都不会是0【目前获得的51049条记录中，仅1468条记录的songid是0】所以这种情况应该可以不予考虑了。 这里记录一下QQ中对应的js解析部分，以备后面需要深入研究时再继续处理。构造规则在http://imgcache.gtimg.cn/music/portal/js/common/pkg/common_61970c5.js?max_age=31536000请求获得的music.js文件中123456789gotoSongdetail: function(a) &#123; // 默认是 根据songmid为构造歌曲详情的url var b = &quot;//y.qq.com/portal/song/&quot; + a.mid + &quot;.html&quot;; a.songtype &amp;&amp; 1 != a.songtype &amp;&amp; 11 != a.songtype &amp;&amp; 13 != a.songtype &amp;&amp; 3 != a.songtype &amp;&amp; (b = &quot;//y.qq.com/portal/song2/&quot; + a.songtype + &quot;/&quot; + a.id + &quot;.html&quot;); a.songtype &amp;&amp; (111 == a.songtype || 112 == a.songtype || 113 == a.songtype) &amp;&amp; (b = &quot;//y.qq.com/portal/song2/&quot; + a.songtype + &quot;/&quot; + a.id + &quot;.html&quot;); a.disstid &amp;&amp; a.songtype &amp;&amp; (b = &quot;//y.qq.com/portal/song3/&quot; + a.songtype + &quot;/&quot; + a.disstid + &quot;/&quot; + a.id + &quot;.html&quot;); window.open(b, f.util.getPageTarget())&#125; 歌词提取一开始是打算直接提取html页面的内容，但是查看网页源文件后发现歌曲详情页的实际数据是异步构造的。简单尝试直接请求返回歌词的url后发现，这边的url做了防范，直接请求返回的是{&quot;retcode&quot;:-1310,&quot;code&quot;:-1310,&quot;subcode&quot;:-1310}，心想还是看看有没有办法获得js加载完毕后的html吧。 Google+各种尝试后发现，可以使用phantomjs来获取js加载完成后的网页，于是折腾了一会儿把本机配起来运行了。乍一看，似乎还可以，但是等到大量歌曲运行起来后，发现还是有一定几率存在没等js加载完就解析网页的，感觉命中率不能忍受。 http://blog.csdn.net/imlsz/article/details/24325623 没办法，只能继续去啃原生的js请求了。chrome调试后，找到请求歌词的实际url1http://c.y.qq.com/lyric/fcgi-bin/fcg_query_lyric.fcg?nobase64=1&amp;musicid=7109361&amp;callback=jsonp1&amp;g_tk=938407465&amp;jsonpCallback=jsonp1&amp;loginUin=0&amp;hostUin=0&amp;format=jsonp&amp;inCharset=utf8&amp;outCharset=utf-8&amp;notice=0&amp;platform=yqq&amp;needNewCode=0 针对请求返回的是错误代码，仔细研究了一下request header，发现其中有个Referer:http://y.qq.com/portal/song/002OrhQA0bNYFg.html请求头，是QQ用来防盗链的（咋就这一个js请求要防盗链呢，其他都不防，醉了。难道是被人爬歌词爬怕了hhhh）。在HTTPClient的get请求中附带上Referer，指向实际歌曲详情的页面，终于发现返回的不是错误代码了。。。。 可是问题的关键是，返回的结果是一堆乱码。一开始以为是编码问题，尝试了各种字符编码，还是乱码。仔细看了下Response header，发现其中有个Content-Encoding:gzip，Google了一下发现其实真正“乱码”原因是被压缩了，需要java这边手动解压一下。解压之后发现，就是这段了，他返回了当前歌曲的动态歌词。123456789101112131415161718192021222324//获取消息头Content-Encoding判断数据流是否gzip压缩过Header[] contentEncodings = httpResponse.getHeaders(&quot;Content-Encoding&quot;); Header contentEncoding = null;if (contentEncodings.length &gt; 0) &#123; contentEncoding = httpResponse.getHeaders(&quot;Content-Encoding&quot;)[0];&#125;HttpEntity entity = httpResponse.getEntity();if ((contentEncoding != null) &amp;&amp;contentEncoding.getValue().equalsIgnoreCase(&quot;gzip&quot;)) &#123; ByteArrayOutputStream out = new ByteArrayOutputStream(); GZIPInputStream gzip = new GZIPInputStream(entity.getContent()); byte[] buffer = new byte[bufferSize]; int n = 0; while ((n = gzip.read(buffer)) &gt;= 0) &#123; out.write(buffer, 0, n); &#125; html = out.toString(&quot;UTF-8&quot;);&#125; else &#123; html = EntityUtils.toString(entity, &quot;UTF-8&quot;); //获得html源代码&#125; 还是梳理一下QQ那边的流程吧。 设置好相应的请求头，通过上面的URL获得动态歌词，然后前端js根据需要，对获得的内容进行清洗获得文本歌词具体的js操作在http://imgcache.gtimg.cn/music/portal/js/v4/song_detail_37c8119.js?max_age=31536000请求获得的song_detail.js中，关键的就是下面三行代码123var t = s.lyric.unescapeHTML(), //html反转义i = t.replace(/\[[^\[\]]*\]/g, &quot;&lt;p&gt;&quot;).replace(/\\n/g, &quot;&lt;/p&gt;&quot;).trim(); //html展示的文本标签lyricStr = t.replace(/\[[^\[\]]*\]/g, &quot;&quot;).replace(/\\n/g, &quot;\r\n&quot;).trim(); //文本歌词 动态歌词上面我们提到了，在想方设法获得文本歌词的过程中，发现了QQ实际上是将动态歌词转成文本歌词的，所以动态歌词也就理所当然的一起得到了。 但实际上在一开始的时候，不是这么获得动态歌词的。虽然那个接口数据量可能没有目前使用的接口数据量多，但是还是记录一下吧。 访问链接1http://music.qq.com/miniportal/static/lyric/songid%100/songid.xml songid就是QQ中音乐id 这边返回的是一个xml文件，并且注意编码是GB2312的，但如果java中使用GB2312来解析这个xml文件，会出现繁体字不能识别的情况。这是因为GB2312只收录了6k多个汉字和符号，GBK在兼容GB2312的基础上，扩展了GB13000，收录了2万多汉字及符号。所以我们这边使用GBK来解析这个xml文件。 值的注意的是，部分文件，动态歌词的接口返回的其实就是文本歌词，所以需要过滤一遍，不能跟QQ那边一样把文本歌词设置在动态歌词中。 QQ音乐其他接口参考 http://imguowei.blog.51cto.com/1111359/1733428]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>QQ音乐</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[锁等待超时与information_schema的三个表]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E5%BA%93%2F%E9%94%81%E7%AD%89%E5%BE%85%E8%B6%85%E6%97%B6%E4%B8%8Einformation_schema%E7%9A%84%E4%B8%89%E4%B8%AA%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[在高并发的环境下，我们经常会遇到并发处理的问题。在数据库的处理过程中，曾经碰到这样一个错误：12ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction 翻译过来就是锁等待超时，尝试重启事务。 那，这种是如何出现的呢？以及我们在开发中应该如何排查呢？ information_schema的三个表information_schema中的三个表记录了事务和锁的相关的记录，三张表的具体字段描述如下： innodb_trx当前运行的所有事务 innodb_locks当前出现的锁 innodb_lock_waits锁等待的对应关系 案例演示 第一步，创建测试表，并插入测试数据 123456create table tx1(id int primary key ,c1 varchar(20),c2 varchar(30),c3 datetime) engine=innodb default charset = utf8 ;insert into tx1 values(1,'aaaa','aaaaa2',NOW()),(2,'bbbb','bbbbb2',NOW()),(3,'cccc','ccccc2',NOW()); 第二步，手动开启事务，并查询三个表数据 123456789start transaction;update tx1 set c1='heyf',c2='heyf',c3=NOW() where id =3 ;select * from information_schema.innodb_trx\G;select * from information_schema.INNODB_LOCKS\G;select * from information_schema.INNODB_LOCK_WAITS\G; 此时没有锁，锁等待关系，只有innodb_trx表中有数据123456789101112131415161718192021222324252627mysql&gt; select * from information_schema.innodb_trx\G;*************************** 1. row *************************** trx_id: 805646 trx_state: RUNNING trx_started: 2018-09-02 14:29:58 trx_requested_lock_id: NULL trx_wait_started: NULL trx_weight: 3 trx_mysql_thread_id: 3 trx_query: select * from information_schema.innodb_trx trx_operation_state: NULL trx_tables_in_use: 0 trx_tables_locked: 1 trx_lock_structs: 2 trx_lock_memory_bytes: 1136 trx_rows_locked: 1 trx_rows_modified: 1 trx_concurrency_tickets: 0 trx_isolation_level: REPEATABLE READ trx_unique_checks: 1 trx_foreign_key_checks: 1trx_last_foreign_key_error: NULL trx_adaptive_hash_latched: 0 trx_adaptive_hash_timeout: 0 trx_is_read_only: 0trx_autocommit_non_locking: 01 row in set (0.00 sec) 第三步，在另一个会话中更新该记录，产生锁等待 123start transaction;update tx1 set c1='heyfffff',c2='heyffffff',c3=NOW() where id =3 ; 查看innodb_trx表数据12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152mysql&gt; select * from information_schema.innodb_trx\G;*************************** 1. row *************************** trx_id: 805649 trx_state: LOCK WAIT trx_started: 2018-09-02 15:08:55 trx_requested_lock_id: 805649:153:3:4 trx_wait_started: 2018-09-02 15:08:55 trx_weight: 2 trx_mysql_thread_id: 4 trx_query: update tx1 set c1=&apos;heyfffff&apos;,c2=&apos;heyffffff&apos;,c3=NOW() where id =3 trx_operation_state: starting index read trx_tables_in_use: 1 trx_tables_locked: 1 trx_lock_structs: 2 trx_lock_memory_bytes: 1136 trx_rows_locked: 1 trx_rows_modified: 0 trx_concurrency_tickets: 0 trx_isolation_level: REPEATABLE READ trx_unique_checks: 1 trx_foreign_key_checks: 1trx_last_foreign_key_error: NULL trx_adaptive_hash_latched: 0 trx_adaptive_hash_timeout: 0 trx_is_read_only: 0trx_autocommit_non_locking: 0*************************** 2. row *************************** trx_id: 805646 trx_state: RUNNING trx_started: 2018-09-02 14:29:58 trx_requested_lock_id: NULL trx_wait_started: NULL trx_weight: 3 trx_mysql_thread_id: 3 trx_query: select * from information_schema.innodb_trx trx_operation_state: NULL trx_tables_in_use: 0 trx_tables_locked: 1 trx_lock_structs: 2 trx_lock_memory_bytes: 1136 trx_rows_locked: 1 trx_rows_modified: 1 trx_concurrency_tickets: 0 trx_isolation_level: REPEATABLE READ trx_unique_checks: 1 trx_foreign_key_checks: 1trx_last_foreign_key_error: NULL trx_adaptive_hash_latched: 0 trx_adaptive_hash_timeout: 0 trx_is_read_only: 0trx_autocommit_non_locking: 02 rows in set (0.00 sec) 查看innodb_locks表数据123456789101112131415161718192021222324mysql&gt; select * from information_schema.INNODB_LOCKS\G;*************************** 1. row *************************** lock_id: 805649:153:3:4lock_trx_id: 805649 lock_mode: X lock_type: RECORD lock_table: `test`.`tx1` lock_index: PRIMARY lock_space: 153 lock_page: 3 lock_rec: 4 lock_data: 3*************************** 2. row *************************** lock_id: 805646:153:3:4lock_trx_id: 805646 lock_mode: X lock_type: RECORD lock_table: `test`.`tx1` lock_index: PRIMARY lock_space: 153 lock_page: 3 lock_rec: 4 lock_data: 32 rows in set, 1 warning (0.00 sec) 查案innodb_lock_waits表数据1234567mysql&gt; select * from information_schema.INNODB_LOCK_WAITS;\G+-------------------+-------------------+-----------------+------------------+| requesting_trx_id | requested_lock_id | blocking_trx_id | blocking_lock_id |+-------------------+-------------------+-----------------+------------------+| 805649 | 805649:153:3:4 | 805646 | 805646:153:3:4 |+-------------------+-------------------+-----------------+------------------+1 row in set, 1 warning (0.00 sec) 在执行第二个update的时候，由于第一个update事务还未提交，故而第二个update在等待，其事务状态为LOCK WAIT ，等待时间超过innodb_lock_wait_timeout值(默认是50)时，则会报ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction异常。 在第二个update锁等待超时之后，对第一个update手动提交事务，则第一个update语句成功更新数据库中数据表。 锁等待递进 如果是多个锁等待，比如有三个update，update同一行记录，则锁等待关系会层级递进，第二个第三个update都保留对第一个update的锁等待且第三个update保留对第二个update的锁等待，如下：123456789mysql&gt; select * from information_schema.INNODB_LOCK_WAITS;\G+-------------------+-------------------+-----------------+------------------+| requesting_trx_id | requested_lock_id | blocking_trx_id | blocking_lock_id |+-------------------+-------------------+-----------------+------------------+| 805653 | 805653:153:3:4 | 805652 | 805652:153:3:4 || 805653 | 805653:153:3:4 | 805651 | 805651:153:3:4 || 805652 | 805652:153:3:4 | 805651 | 805651:153:3:4 |+-------------------+-------------------+-----------------+------------------+3 rows in set, 1 warning (0.00 sec) 解决办法 1、查看并修改变量值 123show GLOBAL VARIABLES like '%innodb_lock_wait_timeout%';set GLOBAL innodb_lock_wait_timeout=100; -- 设置大小值看系统情况 2、找到一直未提交事务导致后来进程死锁等待的进程，并杀掉 根据锁等待表中的拥有锁的事务id(blocking_trx_id)，从innodb_trx表中找到trx_mysql_thread_id值，kill掉。 如 这里杀掉 进程235：1234567select trx_mysql_thread_id from information_schema.innodb_trx it JOIN information_schema.INNODB_LOCK_WAITS ilw on ilw.blocking_trx_id = it.trx_id;-- trx_mysql_thread_id: 235kill 235 3、优化SQL，优化数据库，优化项目 第一个update未执行完，第二个update就来了，超过等待时间就会报锁等待超时异常。在数据并发项目遇到这种情况概率比较大，这时候就要从项目、数据库、执行SQL多方面入手了。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式常见面试题]]></title>
    <url>%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[本文介绍了一些软件设计和设计模式的相关的问题，这些问题大多会出现在初学者面试情景。什么是设计模式？特定的设计模式又是什么？等等这些概念，也许你很轻易回答这些概念，但文内提供的这些问题也许能给你带来更多价值。 入门级程序员的面试题什么是设计模式？在你编码过程中使用了哪些设计模式？在软件工程中，设计模式（design pattern）是对软件设计中普遍存在（反复出现）的各种问题，所提出的解决方案。 平时用的比较多有单例模式（在内存中仅实例化一个对象时使用），适配器模式（典型的就是ListView和GridView的适配器），建造者模式（AlertDialog.Builder）,观察者模式可能比较隐蔽，在Android源码中BaseAdapater的NotifyDataSetChanged的实现。 你能说出在标准的JDK库中使用的一些设计模式吗？**Birdge 桥接模式这个模式将抽象和抽象操作的实现进行了解耦，这样使得抽象和实现可以独立地变化。GOF在提出桥梁模式的时候指出，桥梁模式的用意是”将抽象化(Abstraction)与实现化(Implementation)脱耦，使得二者可以独立地变化”。这句话有三个关键词，也就是抽象化、实现化和脱耦。在Java应用中，对于桥接模式有一个非常典型的例子，就是应用程序使用JDBC驱动程序进行开发的方式。所谓驱动程序，指的是按照预先约定好的接口来操作计算机系统或者是外围设备的程序。 Adapter 适配器模式用来把一个接口转化成另一个接口。使得原本由于接口不兼容而不能一起工作的那些类可以在一起工作。123java.util.Arrays#asList()java.io.InputStreamReader(InputStream)java.io.OutputStreamWriter(OutputStream) Composite 组合模式又叫做部分-整体模式，使得客户端看来单个对象和对象的组合是同等的。换句话说，某个类型的方法同时也接受自身类型作为参数。1234javax.swing.JComponent#add(Component)java.util.Map#putAll(Map)java.util.List#addAll(Collection)java.util.Set#addAll(Collection) 装饰者模式动态的给一个对象附加额外的功能，这也是子类的一种替代方式。可以看到，在创建一个类型的时候，同时也传入同一类型的对象。这在JDK里随处可见，你会发现它无处不在，所以下面这个列表只是一小部分。12345java.io.BufferedInputStream(InputStream)java.io.DataInputStream(InputStream)java.io.BufferedOutputStream(OutputStream)java.util.zip.ZipOutputStream(OutputStream)java.util.Collections#checkedList|Map|Set|SortedSet|SortedMap Facade 门面模式，即外观模式给一组组件，接口，抽象，或者子系统提供一个简单的接口。12java.lang.Classjavax.faces.webapp.FacesServlet Flyweight 享元模式使用缓存来加速大量小对象的访问时间。1234java.lang.Integer#valueOf(int)java.lang.Boolean#valueOf(boolean)java.lang.Byte#valueOf(byte)java.lang.Character#valueOf(char) Proxy 代理模式代理模式是用一个简单的对象来代替一个复杂的或者创建耗时的对象。12java.lang.reflect.ProxyRMI Abstract Factory 抽象工厂模式抽象工厂模式提供了一个协议来生成一系列的相关或者独立的对象，而不用指定具体对象的类型。它使得应用程序能够和使用的框架的具体实现进行解耦。这在JDK或者许多框架比如Spring中都随处可见。它们也很容易识别，一个创建新对象的方法，返回的却是接口或者抽象类的，就是抽象工厂模式了。12345678java.util.Calendar#getInstance()java.util.Arrays#asList()java.util.ResourceBundle#getBundle()java.sql.DriverManager#getConnection()java.sql.Connection#createStatement()java.sql.Statement#executeQuery()java.text.NumberFormat#getInstance()javax.xml.transform.TransformerFactory#newInstance() 抽象工厂模式抽象工厂模式提供了一个协议来生成一系列的相关或者独立的对象，而不用指定具体对象的类型。它使得应用程序能够和使用的框架的具体实现进行解耦。这在JDK或者许多框架比如Spring中都随处可见。它们也很容易识别，一个创建新对象的方法，返回的却是接口或者抽象类的，就是抽象工厂模式了。12345678java.util.Calendar#getInstance()java.util.Arrays#asList()java.util.ResourceBundle#getBundle()java.sql.DriverManager#getConnection()java.sql.Connection#createStatement()java.sql.Statement#executeQuery()java.text.NumberFormat#getInstance()javax.xml.transform.TransformerFactory#newInstance() Builder 建造者模式:定义了一个新的类来构建另一个类的实例，以简化复杂对象的创建。建造模式通常也使用方法链接来实现。1234java.lang.StringBuilder#append()java.lang.StringBuffer#append()java.sql.PreparedStatementjavax.swing.GroupLayout.Group#addComponent() 工厂方法就是一个返回具体对象的方法。1234567java.lang.Proxy#newProxyInstance()java.lang.Object#toString()java.lang.Class#newInstance()java.lang.reflect.Array#newInstance()java.lang.reflect.Constructor#newInstance()java.lang.Boolean#valueOf(String)java.lang.Class#forName() 原型模式使得类的实例能够生成自身的拷贝。如果创建一个对象的实例非常复杂且耗时时，就可以使用这种模式，而不重新创建一个新的实例，你可以拷贝一个对象并直接修改它。12java.lang.Object#clone()java.lang.Cloneable 单例模式用来确保类只有一个实例。Joshua Bloch在Effetive Java中建议到，还有一种方法就是使用枚举。1234java.lang.Runtime#getRuntime()java.awt.Toolkit#getDefaultToolkit()java.awt.GraphicsEnvironment#getLocalGraphicsEnvironment()java.awt.Desktop#getDesktop() 责任链模式通过把请求从一个对象传递到链条中下一个对象的方式，直到请求被处理完毕，以实现对象间的解耦。12java.util.logging.Logger#log()javax.servlet.Filter#doFilter() 命令模式将操作封装到对象内，以便存储，传递和返回。12java.lang.Runnablejavax.swing.Action 解释器模式这个模式通常定义了一个语言的语法，然后解析相应语法的语句。123java.util.Patternjava.text.Normalizerjava.text.Format 迭代器模式提供一个一致的方法来顺序访问集合中的对象，这个方法与底层的集合的具体实现无关。12java.util.Iteratorjava.util.Enumeration 中介者模式通过使用一个中间对象来进行消息分发以及减少类之间的直接依赖。1234java.util.Timerjava.util.concurrent.Executor#execute()java.util.concurrent.ExecutorService#submit()java.lang.reflect.Method#invoke() 备忘录模式生成对象状态的一个快照，以便对象可以恢复原始状态而不用暴露自身的内容。Date对象通过自身内部的一个long值来实现备忘录模式。12java.util.Datejava.io.Serializable 空对象模式这个模式通过一个无意义的对象来代替没有对象这个状态。它使得你不用额外对空对象进行处理。123java.util.Collections#emptyList()java.util.Collections#emptyMap()java.util.Collections#emptySet() 观察者模式它使得一个对象可以灵活的将消息发送给感兴趣的对象。1234java.util.EventListenerjavax.servlet.http.HttpSessionBindingListenerjavax.servlet.http.HttpSessionAttributeListenerjavax.faces.event.PhaseListener 状态模式通过改变对象内部的状态，使得你可以在运行时动态改变一个对象的行为。12java.util.Iteratorjavax.faces.lifecycle.LifeCycle#execute() 策略模式使用这个模式来将一组算法封装成一系列对象。通过传递这些对象可以灵活的改变程序的功能。123java.util.Comparator#compare()javax.servlet.http.HttpServletjavax.servlet.Filter#doFilter() 模板方法模式让子类可以重写方法的一部分，而不是整个重写，你可以控制子类需要重写那些操作。1234java.util.Collections#sort()java.io.InputStream#skip()java.io.InputStream#read()java.util.AbstractList#indexOf() 访问者模式提供一个方便的可维护的方式来操作一组对象。它使得你在不改变操作的对象前提下，可以修改或者扩展对象的行为。12javax.lang.model.element.Element and javax.lang.model.element.ElementVisitorjavax.lang.model.type.TypeMirror and javax.lang.model.type.TypeVisitor Java中什么是单例设计模式？用Java写出线程安全的单例保证一个类仅有一个实例，并提供一个访问它的全局访问点; 使用单例模式最核心的一点是体现了面向对象封装特性中的“单一职责”和“对象自治”原则; 并且可以节省系统开销。 单例模式的若干种写法 使用工厂模式最主要的好处是什么？你在哪里使用？使用工厂的理由：Factory模式最主要的优势在于当创建对象时可提高封装水平。如果你使用Factory模式来创建对象，你可以在后期重置最初产品的装置或者无须任何客户层就可实现更先进更高性能的类。你所关心的仅仅是工厂方法返回的接口方法,不必关心实现细节。 各模式的理解： 简单工厂：把对象的创建放到一个工厂类中，通过参数来创建不同的对象。 工厂方法：每种产品由一种工厂来创建。（不这样会有什么问题？） 抽象工厂：感觉只是工厂方法的复杂化，产品系列复杂化的工厂方法。 面向接口编程：设计模式的一个重要原则是 针对接口编程，不要依赖实现类。工厂模式遵循了这一个原则。 开闭原则（Open-Closed Principle,OCP） “Software entities should be open for extension,but closed for modification”。翻译过来就是：“软件实体应当对扩展开放，对修改关闭”。这句话说得略微有点专业，我们把它讲得更通俗一点，也就是：软件系统中包含的各种组件，例如模块（Modules）、类（Classes）以及功能（Functions）等等，应该在不修改现有代码的基础上，引入新功能。开闭原则中“开”，是指对于组件功能的扩展是开放的，是允许对其进行功能扩展的；开闭原则中“闭”，是指对于原有代码的修改是封闭的，即不应该修改原有的代码。 在Java中，什么叫观察者设计模式?观察者模式又叫做发布-订阅（Publish/Subscribe）模式、模型-视图（Model/View）模式、源-监听器（Source/Listener）模式或从属者（Dependents）模式。 一个软件系统常常要求在某一个对象的状态发生变化的时候，某些其它的对象做出相应的改变。做到这一点的设计方案有很多，但是为了使系统能够易于复用，应该选择低耦合度的设计方案。减少对象之间的耦合有利于系统的复用，但是同时设计师需要使这些低耦合度的对象之间能够维持行动的协调一致，保证高度的协作。 举一个用Java实现的装饰模式？它是作用于对象层次还是类层次？动态地给一个对象增加一些额外的职责，就增加对象功能来说，装饰模式比生成子类实现更为灵活。装饰模式是一种对象结构型模式。装饰模式是一种用于替代继承的技术,使用对象之间的关联关系取代类之间的继承关系。在装饰模式中引入了装饰类，在装饰类中既可以调用待装饰的原有类的方法，还可以增加新的方法，以扩充原有类的功能。 装饰原有对象、在不改变原有对象的情况下扩展增强新功能/新特征。当不能采用继承的方式对系统进行扩展或者采用继承不利于系统扩展和维护时可以使用装饰模式。 什么是MVC设计模式？举一个MVC设计模式的例子？MVC是一个设计模式，它强制性的使应用程序的输入、处理和输出分开。使用MVC应用程序被分成三个核心部件：模型、视图、控制器。它们各自处理自己的任务。 什么是责任链模式一个请求沿着一条“链”传递，直到该“链”上的某个处理者处理它为止。 一个请求可以被多个处理者处理或处理者未明确指定时。责任链模式非常简单异常好理解，相信我它比单例模式还简单易懂，其应用也几乎无所不在，甚至可以这么说,从你敲代码的第一天起你就不知不觉用过了它最原始的裸体结构：switch-case语句。 什么是适配器模式？举用Java实现适配器模式的例子？适配器必须实现原有的旧的接口 适配器对象中持有对新接口的引用，当调用旧接口时，将这个调用委托给实现新接口的对象来处理，也就是在适配器对象中组合一个新接口。 什么是代理模式？代理（proxy）模式：指目标对象给定代理对象，并由代理对象代替真实对象控制客户端对真实对象的访问。 代理模式模式有以下角色： 抽象主题（subject）角色：声明真实主题和代理主题的共同接口。 真实主题（real subject）角色：定义代理对象需要代理的真实对象。 代理主题（proxy subject）角色：代替真实对象来控制对真实对象的访问，代理对象持有真实对象的应用，从而可以随时控制客户端对真实对象的访问。 策略模式有什么好处？定义了一系列封装了算法、行为的对象，他们可以相互替换。 举例：Java.util.List就是定义了一个增（add）、删（remove）、改（set）、查（indexOf）策略，至于实现这个策略的ArrayList、LinkedList等类，只是在具体实现时采用了不同的算法。但因为它们策略一样，不考虑速度的情况下，使用时完全可以互相替换使用。 进阶级程序员的面试题举例说明你什么时候会用抽象类，什么时候更愿意使用接口？这是一个很常见的面试问题，并不算难。接口和抽象类都按照“不为实现写代码”的设计原则，这是为了增加代码的灵活性，以应付不断变化的要求。下面是一些帮助你回答这个问题的指南： 在Java中，你只能继承一个类，但实现多个接口。所以你继承一个类的时候就无法再继承别的类了。 接口是用来代表形容词或行为，例如Runnable、Clonable、Serializable等。因此，如果您使用一个抽象类来实现Runnable和Clonacle，你就不可以使你的类同时实现这两个功能，而如果接口的话就没问题。 抽象类是比接口稍快，所以很在乎时间的应用尽量使用抽象类。 如果多个继承层次的共同行为在在同一个地方编写更好，那么抽象类会是更好的选择。有时候可以在接口里定义函数但是在抽象类里默认功能就能实现接口和抽象类共同工作了。了解Java接口。 设计一个能接收不同硬币、出售不同货物的自动售货机。在Java中，什么时候用重载，什么时候用重写？重载和覆盖在Java里实现的都是同一个功能，但overload的输入变量不同，override则完全相同。 设计ATM机我们所有人都使用ATM(自动柜员机)。想想你会怎么设计一个ATM？就设计金融系统来说，必须知道它们应该在任何情况下都能够如期工作。不管是断电还是其他情况，ATM应该保持 正确的状态（事务） , 想想 加锁（locking）、事务（transaction）、错误条件（error condition）、边界条件（boundary condition） 等等。尽管你不能想到具体的设计，但如果你可以指出非功能性需求，提出一些问题，想到关于边界条件，这些都会是很好的一步。 在Java中，为什么不允许从静态方法中访问非静态变量？Java里不允许从静态方法中获取非静态变量仅仅是因为非静态变量会和特定的对象实例相关联，而静态变量不会。 有这样一段话，“由于静态方法可以不通过对象进行调用，因此在静态方法里，不能调用其他非静态变量，也不可以访问非静态变量成员。” 怎么深层次理解这句话呢？ 深层次的解释是，“静态方法可以不通过对象进行调用”也就是说这个方法在对象尚未创建的时候就可以调用，而此时对象尚未创建，（非静态）成员变量根本都还不存在，何谈访问？ 如果允许调用其他非静态变量，会引起什么后果么？ 不是允许不允许的问题，是这个时候非静态成员变量都还不存在（他是伴随着对象的创建而创建的），根本无法访问。 在Java中设计一个并发规则的pipeline？并发编程或并发设计这些天很火，它可以充分利用现在不断提升的高级处理器的处理能力，而Java成为一个多线程语言也从这种情况获益良多。设计一个并发系统需要记住的最关键的点是线程安全，不可变性，本地变量和避免使用static或者类变量（instance variables）。你只需要想着每一类都可以同时被多个线程同时执行，所以最好的做法就是每一个线程都处理自己的数据 ，不跟其他数据交互，并且运行时只需要最小的同步保证。这个问题可以涉及到从最初的讨论到完整的类和接口编码，但只要你记住并发中最重要的点和问题如，竞争条件（race condition）、死锁（deadlock）、内存交互问题（memory interference）、原子性、ThreadLocal变量等，你都可以回答它。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[单例模式的若干种写法]]></title>
    <url>%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2F%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F%E7%9A%84%E8%8B%A5%E5%B9%B2%E7%A7%8D%E5%86%99%E6%B3%95%2F</url>
    <content type="text"><![CDATA[设计一个类，我们只能生成该类的一个实例。 这是一道很简单也很基础的设计模式题，对不对？但是要真的在各种条件下完美的实现Singleton模式，却是需要一点思考的。 以前我在实习的时候，每次遇到需要用到单例来写一个处理器线程的时候，使用的都是最简单的单例实现模式，由我自己在代码中人为保证代码只会被调用一次。今天正好有机会，系统地学习一下如何正确地实现单例模式 不好的解法一：只适用于单线程环境由于 要求只能生成一个实例，因此我们必须把构造函数设置为私有函数以防止他人创建实例。我们可以定义一个静态的实例，在需要的时候创建该实例。 1234567891011121314public class Singleton1 &#123; private static Singleton1 instance = null; private Singleton1() &#123; &#125; public static Singleton1 getInstance() &#123; if (instance == null) instance = new Singleton1(); return instance; &#125;&#125; 上面的代码在Singleton的静态属性getInstance中，只有在instance为null的时候，才会创建一个实例以避免重复。同时我们把构造函数定义为一个私有函数，这样就能确保只会创建一个实例 不好的解法二：多线程中能工作但效率不高解法一中的代码在单线程的时候工作正常，但是在多线程的情况下就有问题了。设想如果两个线程同时运行到判断instance是否为null的if语句，并且instance的确没有创建，那么两个线程都会创建一个实例，此时Singleton1就不再满足单例模式的要求了。 为了保证在多线程环境下我们还是只能得到类型的一个实例，需要加上一个同步锁。稍微修改Singleton1的代码12345678910111213141516public class Singleton2 &#123; private static Singleton2 instance = null; private static Lock lock = new ReentrantLock(); private Singleton2() &#123; &#125; public static Singleton2 getInstance() &#123; lock.lock(); if (instance == null) &#123; instance = new Singleton2(); &#125; lock.unlock(); return instance; &#125;&#125; 我们假设还是有两个线程同时想创建一个实例，由于在同一时刻只有一个线程能够得到同步锁，当第一个线程加上锁时，第二个线程只能等待。当第一个线程发现实例还没有创建时，它创建出一个实例。接着第一个线程释放同步锁，此时第二个线程可以加上同步锁，并运行接下来的代码。这个时候由于实例已经被第一个线程创建出来了，第二个线程就不会重复创建实例了，这样就保证了我们在多线程环境中也只有一个实例。 但是，Singleton2还是不是很完美。我们每次通过属性getInstance来获取实例时，都会试图加上一个同步锁，而加锁是一个非常耗时的操作，在没有必要的时候应该尽量避免。 可行的解法：加同步锁前后两次判断实例是否已存在我们只是在实例还没有创建之前需要加锁操作，以保证只有一个线程创建出实例。而当实例已经创建之后，我们已经不需要再做加锁操作了。我们可以把解法二中的代码再做进一步的改进123456789101112131415161718public class Singleton3 &#123; private volatile static Singleton3 instance = null; // 注意 volatile 关键字 private static Lock lock = new ReentrantLock(); private Singleton3() &#123; &#125; public static Singleton3 getInstance() &#123; if (instance == null) &#123; lock.lock(); if (instance == null) &#123; instance = new Singleton3(); &#125; lock.unlock(); &#125; return instance; &#125;&#125; Singleton3中只有当instance为null即没有创建时，需要加锁操作。当instance已经创建出来之后，则无需加锁。因为只有在第一次的时候instance为null，因此只有在第一次试图创建实例的时候需要加锁。这样Singleton3的时间效率比Singleton2好很多。 Singleton3这种实现机制又比称为“双重检查加锁”，它的实现需要依赖于volatile关键字，它的意思是：被volatile修饰的变量的值，将不会被本地线程缓存，所以对该变量的读写都是直接操作共享内存，从而确保多个线程能正确处理该变量。 注意：在Java1.4及以前版本中，很多JVM对于volatile关键字的实现的问题，会导致“双重检查加锁”的失败，因此“双重检查加锁”机制只能用在Java5及以上的版本 挖个坑，关于volatile关键字的具体知识，另开博文解释。 Singleton3用加锁机制来确保在多线程环境下只创建一个实例，并且用两个if判断来提高效率。这样的代码实现起来比较复杂，容易出错，我们还有更加优秀的解法。 强烈推荐的解法一：静态内部类延迟加载由于volatile关键字可能会屏蔽掉虚拟机中一些必要的代码优化，所以运行效率并不是很高。因此一般建议，没有特别的需要，不要使用。也就是说，虽然可以使用“双重检查加锁”机制来实现线程安全的单例，但并不建议大量采用，可以根据情况来选用。 下面介绍一种静态内部类延迟加载的方式。这种方式综合使用了Java的类级内部类和多线程缺省同步锁的知识，很巧妙的实现了延迟加载和多线程安全。 在看具体实现前，我们先来回顾一下基础知识。 1. 什么是类级内部类 简单点说，类级内部类指的是，有static修饰的成员式内部类。如果没有static修饰的成员式内部类被称为对象级内部类。 类级内部类相当于其外部类的static成分，它的对象与外部类对象间不存在依赖关系，因此可直接创建。而对象级内部类的实例，是绑定在外部对象实例中的。 类级内部类中，可以定义静态的方法。在静态方法中只能够引用外部类中的静态成员方法或者静态成员变量。 类级内部类相当于其外部类的成员，只有在第一次被使用的时候才被会装载。 2. 多线程缺省同步锁的知识 大家都知道，在多线程开发中，为了解决并发问题，主要是通过使用synchronized来加互斥锁进行同步控制。但是在某些情况中，JVM已经隐含地执行了同步，这些情况下就不用自己再来进行同步控制了。这些情况包括： 由静态初始化器（在静态字段上或static{}块中的初始化器）初始化数据时 访问final字段时 在创建线程之前创建对象时 线程可以看见它将要处理的对象时 要想很简单地实现线程安全，可以采用静态初始化器，它可以由JVM来保证线程安全。但是这样一来会浪费一定的空间，因为在类装载的时候就会初始化对象，不管你需不需要。 如果有一种方式能让类装载的时候不要初始化对象，就可以解决空间浪费的问题了。一种可行的解决方式就是采用类级内部类，在这个类级内部类里面去创建对象实例。这样一来，只要不使用到这个类级内部类，那就不会创建对象实例，从而同时实现延迟加载和线程安全。12345678910111213141516171819public class Singleton4 &#123; private Singleton4() &#123; &#125; /** * 类级的内部类，也就是静态的成员式内部类，该内部类的实例与外部类的实例 * 没有绑定关系，而且只有被调用到时才会装载，从而实现了延迟加载。 */ private static class SingletonHolder &#123; /** * 静态初始化器，由JVM来保证线程安全 */ private static Singleton4 instance = new Singleton4(); &#125; public static Singleton4 getInstance() &#123; return SingletonHolder.instance; &#125;&#125; 当getInstance方法第一次被调用的时候，它第一次读取SingletonHolder.instance，导致SingletonHolder类得到初始化；而这个类在装载并被初始化的时候，会初始化它的静态域，从而创建Singleton4的实例，由于是静态的域，因此只会在虚拟机装载类的时候初始化一次，并由虚拟机来保证它的线程安全性。 这个模式的优势在于，getInstance方法并没有被同步，并且只是执行一个域的访问，因此延迟初始化并没有增加任何访问成本。 强烈推荐的解法二：枚举单元素的枚举类型已经成为实现Singleton的最佳方法。用枚举来实现单例非常简单，只需要编写一个包含单个元素的枚举类型即可。123456789101112public enum Singleton5 &#123; /** * 定义一个枚举的元素，它就代表了Singleton的一个实例。 */ INSTANCE; /** * 单例可以有自己的操作 */ public void operation() &#123; &#125;&#125; 使用枚举来实现单实例控制会更加简洁，而且无偿地提供了序列化机制，并由JVM从根本上提供保障，绝对防止多次实例化，是更简洁、高效、安全的实现单例的方式。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用HashMap，如果key是自定义的类，就必须重写hashcode()和equals()]]></title>
    <url>%2FJava%2FJava%E5%9F%BA%E7%A1%80%2F%E4%BD%BF%E7%94%A8HashMap%EF%BC%8C%E5%A6%82%E6%9E%9Ckey%E6%98%AF%E8%87%AA%E5%AE%9A%E4%B9%89%E7%9A%84%E7%B1%BB%EF%BC%8C%E5%B0%B1%E5%BF%85%E9%A1%BB%E9%87%8D%E5%86%99hashcode()%E5%92%8Cequals()%2F</url>
    <content type="text"><![CDATA[hashcode()和equals()都继承于Object，并且Object都提供了默认实现，具体可以参考Java根类Object的方法说明。关于Java中HashMap的相关原理可以参考前面的两篇文章，HashMap源码阅读和HashMap为什么线程不安全。 在实际使用中，如果HashMap中的key是自定义的类，一般我们都会重写hashcode()和equals()，这是为什么呢？？ 首先我们先回顾一下Object中hashcode()和equals()两个方法的默认实现。123456public boolean equals(Object obj)&#123; return (this == obj);&#125;// 是一个本地方法，返回的对象的地址值。public native int hashCode(); 默认的根类Object提供了两个方法的实现，为什么我们还需要重写它们呢？解答这个问题，需要从两个方面展开。 1. hashcode()和equals()是在哪里被用到的？什么用的？ HashMap是基于散列函数，以数组和链表的方式实现的。 而对于每一个对象，通过其hashCode()方法可为其生成一个整形值（散列码），该整型值被处理后，将会作为数组下标，存放该对象所对应的Entry（存放该对象及其对应值）。 equals()方法则是在HashMap中插入值或查询时会使用到。当HashMap中插入值或查询值对应的散列码与数组中的散列码相等时，则会通过equals方法比较key值是否相等，所以想以自建对象作为HashMap的key，必须重写该对象继承object的equals方法。 2. 本来不就有hashcode()和equals()了么？干嘛要重写，直接用原来的不行么？ HashMap中，如果要比较key是否相等，要同时使用这两个函数！因为自定义的类的hashcode()方法继承于Object类，其hashcode码为默认的内存地址，这样即便有相同含义的两个对象，比较也是不相等的，例如，123Student st1 = new Student("wei","man");Student st2 = new Student("wei","man"); 正常理解这两个对象再存入到hashMap中应该是相等的，但如果你不重写 hashcode（）方法的话，比较是其地址，不相等！ HashMap中的比较key是这样的，先求出key的hashcode(),比较其值是否相等，若相等再比较equals(),若相等则认为他们是相等 的。若equals()不相等则认为他们不相等。如果只重写hashcode()不重写equals()方法，当比较equals()时只是看他们是否为 同一对象（即进行内存地址的比较）,所以必定要两个方法一起重写。HashMap用来判断key是否相等的方法，其实是调用了HashSet判断加入元素 是否相等。]]></content>
      <categories>
        <category>Java</category>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>HashMap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[单链表翻转]]></title>
    <url>%2F%E7%AE%97%E6%B3%95%2F%E5%8D%95%E9%93%BE%E8%A1%A8%E7%BF%BB%E8%BD%AC%2F</url>
    <content type="text"><![CDATA[单链表翻转也是面试中经常会被问到的一个经典算法。解决这个问题，最常想到的是递归，或者借助外部数组村组来实现。但是这两种方式的复杂度都偏高。 本文主要介绍使用指针来实现复杂度为O(1)的单链表翻转方法。 定义链表节点首先定义链表节点123456789101112131415161718192021222324252627282930313233343536373839404142class Node &#123; private int val; private Node next; public Node(int val) &#123; this.val = val; this.next = null; &#125; public Node(int val, Node next) &#123; this.val = val; this.next = next; &#125; public int getVal() &#123; return val; &#125; public void setVal(int val) &#123; this.val = val; &#125; public Node getNext() &#123; return next; &#125; public void setNext(Node next) &#123; this.next = next; &#125; public String print() &#123; StringBuilder sb = new StringBuilder(); if (next == null) &#123; sb.append(val); &#125; else &#123; sb.append(val); sb.append("-&gt;"); sb.append(next.print()); &#125; return sb.toString(); &#125;&#125; 递归法反转当前节点之前先反转后续节点。这样从头结点开始，层层深入直到尾结点才开始反转指针域的指向。简单的说就是从尾结点开始，逆向反转各个结点的指针域指向。 12345678910public Node recursion(Node head) &#123; if (head == null || head.getNext() == null) &#123; return head; &#125; Node next = head.getNext(); Node reHead = recursion(next);// 先翻转后续节点 next.setNext(head);// 将当前节点的指针域指向前一个节点 head.setNext(null);// 将前一个节点的指针域置空 return reHead; // 翻转后新链表的节点&#125; 三指针法递归反转法是从后往前逆序反转指针域的指向，而三指针法是从前往后反转各个结点的指针域的指向。基本思路是：将当前节点cur的下一个节点 cur.getNext()缓存到temp后，然后更改当前节点指针指向上一结点pre。也就是说在反转当前结点指针指向前，先把当前结点的指针域用tmp临时保存，以便下一次使用。1234567891011121314151617181920public Node nonRecursion(Node head) &#123; if (head == null || head.getNext() == null) &#123; return head; &#125; Node pre = head; Node cur = pre.getNext(); Node next; while (cur != null) &#123; next = cur.getNext(); cur.setNext(pre); pre = cur; cur = next; &#125; // 最后将原链表的头节点的指针域置为null，还回新链表的头结点，即原链表的尾结点 head.setNext(null); return pre; &#125;]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>链表</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA虚拟机关闭钩子(Shutdown Hook)]]></title>
    <url>%2FJava%2FJVM%2FJAVA%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%85%B3%E9%97%AD%E9%92%A9%E5%AD%90(Shutdown%20Hook)%2F</url>
    <content type="text"><![CDATA[前几天看到蚂蚁开源的sofa框架，其中提供了应用关闭后的回调方法，看了原理之后发现是利用了JAVA虚拟机关闭钩子(Shutdown Hook)来实现的。 Java程序经常也会遇到进程挂掉的情况，一些状态没有正确的保存下来，这时候就需要在JVM关掉的时候执行一些清理现场的代码。JAVA中的ShutdownHook提供了比较好的方案。 JDK提供了Java.Runtime.addShutdownHook(Thread hook)方法，可以注册一个JVM关闭的钩子，这个钩子可以在一下几种场景中被调用： 程序正常退出 使用System.exit() 终端使用Ctrl+C触发的中断 系统关闭 OutOfMemory宕机 使用Kill pid命令干掉进程（注：在使用kill -9 pid时，是不会被调用的） 下面是JDK1.7中关于钩子的定义：1234567891011public void addShutdownHook(Thread hook)param： hook - An initialized but unstarted Thread object throw： IllegalArgumentException - If the specified hook has already been registered, or if it can be determined that the hook is already running or has already been run IllegalStateException - If the virtual machine is already in the process of shutting down SecurityException - If a security manager is present and it denies RuntimePermission("shutdownHooks")from： 1.3 see： removeShutdownHook(java.lang.Thread), halt(int), exit(int) 直接上测试代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public class ShutdownHookTest &#123; private void addHook() &#123; Runtime.getRuntime().addShutdownHook(new Thread(() -&gt; System.out.println("Execute Hook....."))); &#125; public void case1() &#123; System.out.println("case1: The Application is doing something"); try &#123; TimeUnit.MILLISECONDS.sleep(5000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; public void case2() &#123; System.out.println("case2: The Application is doing something"); System.exit(0); &#125; public void case3() &#123; Thread thread = new Thread(() -&gt; &#123; while (true) &#123; System.out.println("thread is running...."); try &#123; TimeUnit.MILLISECONDS.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;); thread.start(); &#125; public void case5() &#123; System.out.println("case5: The Application is doing something"); byte[] b = new byte[500 * 1024 * 1024]; System.out.println("init finish"); &#125; public static void main(String[] args) &#123; ShutdownHookTest hookTest = new ShutdownHookTest(); hookTest.addHook(); // hookTest.case1(); // hookTest.case2(); // hookTest.case3(); hookTest.case5(); &#125;&#125; 首先来测试第一种，程序正常退出的情况：1234case1: The Application is doing somethingExecute Hook.....Process finished with exit code 0 如上可以看到，当main线程运行结束之后就会调用关闭钩子。 再看第二种情况：1234case2: The Application is doing somethingExecute Hook.....Process finished with exit code 0 当系统推出后，也会调用关闭钩子。 接着看第三种情况，程序启动之后过一会儿关闭程序。输出：123456thread is running....thread is running....thread is running....Execute Hook.....Process finished with exit code 130 (interrupted by signal 2: SIGINT) 最后我们看下第五种情况，运行参数设置为：-Xmx20M 这样可以保证会有OutOfMemoryError的发生。结果：12345case5: The Application is doing somethingException in thread &quot;main&quot; java.lang.OutOfMemoryError: Java heap space at com.winsky.logs.ShutdownHookTest.case5(ShutdownHookTest.java:47) at com.winsky.logs.ShutdownHookTest.main(ShutdownHookTest.java:59)Execute Hook..... 其他还有几种情况就不一一演示了，有兴趣可以自行撸代码尝试一下。]]></content>
      <categories>
        <category>Java</category>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>JVM</tag>
        <tag>关闭钩子</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[notify和notifyAll的区别和相同]]></title>
    <url>%2FJava%2FJava%E5%9F%BA%E7%A1%80%2Fnotify%E5%92%8CnotifyAll%E7%9A%84%E5%8C%BA%E5%88%AB%E5%92%8C%E7%9B%B8%E5%90%8C%2F</url>
    <content type="text"><![CDATA[今天被问到一道题目，如何实现多个线程同时进行，谷歌之，发现网上有篇文章提到可以用wait和notifyall来实现，想着以前看过wait和notify的区别，今天正好有机会来看下notifyall。 本文记录了notify和notifyAll的区别和相同，以便不时之时查阅。 123456wait,notify,notifyAll：此方法只应由作为此对象监视器的所有者的线程来调用。通过以下三种方法之一，线程可以成为此对象监视器的所有者：- 通过执行此对象的同步实例方法。 - 通过执行在此对象上进行同步的`synchronized`语句的正文。 - 对于`Class`类型的对象，可以通过执行该类的同步静态方法。 一次只能有一个线程拥有对象的监视器。 以上说法，摘自javadoc。意思即，在调用中，必须持有对象监视器(即锁），我们可以理解为需要在synchronized方法内运行。 那么由此话的隐含意思，即如果要继续由同步块包含的代码块，需要重新获取锁才可以。这句话，在javadoc中这样描述：1234567891011wait：此方法导致当前线程（称之为T）将其自身放置在对象的等待集中，然后放弃此对象上的所有同步要求。出于线程调度目的，在发生以下四种情况之一前，线程T被禁用，且处于休眠状态：- 其他某个线程调用此对象的 notify 方法，并且线程 T 碰巧被任选为被唤醒的线程。 - 其他某个线程调用此对象的 notifyAll 方法。 - 其他某个线程中断线程T。 - 大约已经到达指定的实际时间。但是，如果 timeout 为零，则不考虑实际时间，在获得通知前该线程将一直等待。 然后，从对象的等待集中删除线程T，并重新进行线程调度。该线程以常规方式与其他线程竞争，以获得在该对象上同步的权利；一旦获得对该对象的控制权，该对象上的所有其同步声明都将被恢复到以前的状态，这就是调用wait方法时的情况。然后，线程 T 从 wait 方法的调用中返回。所以，从 wait 方法返回时，该对象和线程 T 的同步状态与调用 wait 方法时的情况完全相同。 即必须重新进行获取锁，这样对于notifyAll来说，虽然所有的线程都被通知了。但是这些线程都会进行竞争，且只会有一个线程成功获取到锁，在这个线程没有执行完毕之前，其他的线程就必须等待了（只是这里不需要再notifyAll通知了，因为已经notifyAll了，只差获取锁了）有如下一个代码，可以重现这个现象。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class RunningGame &#123; public static final Object RACE_TRACK = new Object(); public static void main(String[] args) &#123; for (int i = 0; i &lt; 8; i++) &#123; Runner runner = new Runner(i + 1); runner.start(); &#125; Referee referee = new Referee(); referee.start(); &#125;&#125;class Runner extends Thread &#123; private int index; public Runner(int index) &#123; this.index = index; &#125; @Override public void run() &#123; synchronized (RunningGame.RACE_TRACK) &#123; System.out.println(index + "号选手准备就位"); try &#123; RunningGame.RACE_TRACK.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(index + "号选手出发，时间：" + System.currentTimeMillis()); try &#123; sleep(5000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125;class Referee &#123; public void start() &#123; synchronized (RunningGame.RACE_TRACK) &#123; System.out.println(); System.out.println("Ready!Go!"); RunningGame.RACE_TRACK.notifyAll(); &#125; &#125;&#125; 注意上面的run方法内部，我们在wait()之后，打印一句话，然后将当前代码，暂停5秒。关于sleep方法，该线程不丢失任何监视器的所属权，即仍然持有锁。 输出：1234567891011121314151617181号选手准备就位2号选手准备就位3号选手准备就位4号选手准备就位5号选手准备就位6号选手准备就位7号选手准备就位8号选手准备就位Ready!Go!8号选手出发，时间：15327903887387号选手出发，时间：15327903937426号选手出发，时间：15327903987465号选手出发，时间：15327904037514号选手出发，时间：15327904087543号选手出发，时间：15327904137582号选手出发，时间：15327904187611号选手出发，时间：1532790423765 在上面的输出中，在wait之后，只有一个线程输出了”在运行了”语句，并且在一段时间内（这里为5秒），不会有其他输出。即表示，在当前代码持有锁之间，其他线程是不会输出的。 最后结论就是：被wait的线程，想要继续运行的话，它必须满足2个条件： 由其他线程notify或notifyAll了，并且当前线程被通知到了 经过和其他线程进行锁竞争，成功获取到锁了 2个条件，缺一不可。 其实在实现层面，notify和notifyAll都达到相同的效果，都只会有一个线程继续运行。但notifyAll免去了线程运行完了通知其他线程的必要，因为已经通知过了。 什么时候用notify，什么时候使用notifyAll，这就得看实际的情况了。 同时，我们使用notifyall来实现所有的线程同时开始执行就没有任何意义了，因为这些线程还需要争夺锁资源，其仍然是顺序执行的。]]></content>
      <categories>
        <category>Java</category>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>多线程</tag>
        <tag>notify</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring ConcurrentReferenceHashMap简单测试]]></title>
    <url>%2FSpring%2FSpring%20ConcurrentReferenceHashMap%E7%AE%80%E5%8D%95%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[这周在写代码的时候，由于配置了IDE的快捷提示，一不留神使用了ConcurrentReferenceHashMap这个新奇的类，虽然不会引发什么bug，但是还是在CR的时候被师兄发现了。 本文就来探讨一下ConcurrentReferenceHashMap这个map具体是什么类。 ConcurrentReferenceHashMap是自spring3.2后增加的一个同步的软(虚)引用Map。关于软引用(SoftRefrence)和虚引用(WeakRefrence可以参见Java四种引用类型。废话不多说直接上测试代码: 123456789101112@Testpublic void test() throws InterruptedException &#123; String key = new String("key"); String value = new String("val"); Map&lt;String, String&gt; map = new ConcurrentReferenceHashMap&lt;&gt;(8, ReferenceType.WEAK); map.put(key, value); System.out.println(map); key = null; System.gc(); TimeUnit.SECONDS.sleep(5); System.out.println(map);&#125; 神奇的事发生了。通过代码我们可以看到。我先构建了一个虚引用的map对象（也就是本文主角ConcurrentReferenceHashMap），然后新建对象key,value并将两个对象放入Map中进行保存。然后使key对象的强引用置为null。然后调用系统GC。由于系统GC的特殊性质并不能保证系统立马进行GC操作所已紧接着让主线程睡眠5s。接着打印我们的map对象发现map中的对象自动被移除了。 接下来我不置空key而将value置空发现结果相同。 结论： 查看ConcurrentReferenceHashMap源码发现起底层实现依赖的是RefrenceQueue完成自动移除操作。时间有限就写到这里。有时间再进行完善。]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[对象、数组复制的几种方法(深拷贝)]]></title>
    <url>%2FJavaScript%2F%E5%AF%B9%E8%B1%A1%E3%80%81%E6%95%B0%E7%BB%84%E5%A4%8D%E5%88%B6%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E6%B3%95(%E6%B7%B1%E6%8B%B7%E8%B4%9D)%2F</url>
    <content type="text"><![CDATA[工作所需，我这个后端程序猿又开始操起我那半吊子的前端技术，开始写前端。这次在实现前端页面逻辑的时候，碰到了一个JS深拷贝的问题。原先默认都是浅拷贝，或者是第一层深拷贝，但是到里面的嵌套对象就是浅拷贝了。踩了这个坑之后，遂记录本文以供后续参考。 本文重点介绍JS中如何实现深拷贝，避免在实际开发中踩坑。当然，考虑到部分同学对深拷贝和浅拷贝的概念可能不太熟，我们也会先介绍一下深拷贝和浅拷贝的原理。 深拷贝 VS 浅拷贝数据类型首先我们了解下两种数据类型： 基本类型：像Number、String、Boolean等这种为基本类型 复杂类型：Object和Array 浅拷贝与深拷贝的概念接着我们分别来了解下浅拷贝和深拷贝，深拷贝和浅拷贝是只针对Object和Array这样的复杂类型的。 浅拷贝1234567var a = &#123; myname: 'yana'&#125;;var b = a;b.myname = '小雅';console.log(b.myname); // 小雅console.log(a.myname); // 小雅 12345var a = ['myname', 'yana'];var b = a;b[1] = '小雅';console.log(a); // ["myname", "小雅"]console.log(b); // ["myname", "小雅"] 可以看出，对于对象或数组类型，当我们将a赋值给b，然后更改b中的属性，a也会随着变化。 也就是说a和b指向了同一块内存，所以修改其中任意的值，另一个值都会随之变化，这就是浅拷贝。 JavaScript 并没有直接提供对象或者数组的复制方法，默认采用浅拷贝。 深拷贝刚刚我们了解了什么是浅拷贝，那么相应的，如果给b放到新的内存中，将a的各个属性都复制到新内存里，就是深拷贝。也就是说，当b中的属性有变化的时候，a内的属性不会发生变化。 如何实现深拷贝对象的深拷贝通过序列化实现深拷贝我们可以将对象序列化再解析回来实现对象拷贝（注意：对象中的函数 function 不会被复制）12345var a = &#123;v1:1, v2:2&#125;;var b = JSON.parse(JSON.stringify(a));b.v1 = 3;console.log("对象a：",a);console.log("对象b：",b); 通过递归遍历实现 我们定义一个 clone 方法实现深度复制功能（Deep Copy），其内部实现原理就是将对象的属性遍历一遍，赋给一个新的对象。 123456789101112131415//自定义的复制方法function clone(obj) &#123; var copy = &#123;&#125;; for (var attr in obj) &#123; copy[attr] = typeof(obj[attr])==='object' ? clone(obj[attr]) : obj[attr]; &#125; return copy;&#125; //测试样例var a = &#123;v1:1, v2:2&#125;;var b = clone(a);b.v1 = 3;console.log("对象a：",a);console.log("对象b：",b); 也可以直接给 Object 增加个 clone 方法，其内部实现原来同上面是一样的 1234567891011121314151617//自定义的复制方法Object.prototype.clone = function() &#123; var copy = (this instanceof Array) ? [] : &#123;&#125;; for (var attr in this) &#123; if (this.hasOwnProperty(attr))&#123; copy[attr] = typeof(this[attr])==='object' ? clone(this[attr]) : this[attr]; &#125; &#125; return copy;&#125;; //测试样例var a = &#123;v1:1, v2:2&#125;;var b = a.clone();b.v1 = 3;console.log("对象a：",a);console.log("对象b：",b); 使用 jQuery 复制jQuery 自带的 extend 方法可以用来实现对象的复制。123456var a = &#123;v1:1, v2:2&#125;;var b = &#123;&#125;;$.extend(b,a);b.v1 = 3;console.log("对象a：",a);console.log("对象b：",b); 数组的拷贝使用 slice 方法实现12345var a = [1, 2];var b = a.slice(0);b.push(3);console.log("数组a：", a);console.log("数组b：", b); 使用 concat 方法实现12345var a = [1, 2];var b = a.concat();b.push(3);console.log("数组a：", a);console.log("数组b：", b);]]></content>
      <categories>
        <category>JavaScript</category>
      </categories>
      <tags>
        <tag>JavaScript</tag>
        <tag>深拷贝</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL text类型的最大长度]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E5%BA%93%2FMySQL%20text%E7%B1%BB%E5%9E%8B%E7%9A%84%E6%9C%80%E5%A4%A7%E9%95%BF%E5%BA%A6%2F</url>
    <content type="text"><![CDATA[MySQL 3种text类型的最大长度如下： TEXT：65,535 bytes ~64kb MEDIUMTEXT：16,777,215 bytes ~16Mb LONGTEXT：4,294,967,295 bytes ~4Gb]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>text类型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大白话聊聊分布式系统]]></title>
    <url>%2F%E5%88%86%E5%B8%83%E5%BC%8F%2F%E5%A4%A7%E7%99%BD%E8%AF%9D%E8%81%8A%E8%81%8A%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[一提起“分布式系统”，大家的第一感觉就是好高大上啊，深不可测，看各类大牛关于分布式系统的演讲或者书籍，也大多是一脸懵逼。本文期望用浅显易懂的大白话来就什么是分布式系统、分布式系统有哪些优势、分布式系统会面临哪里挑战、如何来设计分布式等方面的话题来展开讨论。 什么是分布式系统关于“分布式系统”的定义，我们先看下书中是怎么说的。《分布式系统原理和范型》一书中是这样定义分布式系统的：“分布式系统是若干独立计算机的集合，这些计算机对于用户来说就像是单个相关系统”。 关于这个定义，我们直观的感受就是： 首先，这种系统相对来说比较牛逼，起码由好几台主机组成。以谷歌、亚马逊等服务商而言，他们的数据中心都由上万台主机支撑起来的。 其次，虽然很牛逼，但对于外人来说，是感觉不到这些主机的存在。也就是说，我们只看到是一个系统在运作。以”亚马逊S3宕机事件”为例，平时，我们压根不知道亚马逊所提供的服务背后是由多少台主机组成，但是等到S3宕机才知道，这货已经是占了互联网世界的半壁江山了。 从进程角度看，两个程序分别运行在两个台主机的进程上，它们相互协作最终完成同一个服务（或者功能），那么理论上这两个程序所组成的系统，也可以称作是“分布式系统”。 当然，这个两个程序可以是不同的程序，也可以是相同的程序。如果是相同的程序，我们又可以称之为“集群”。所谓集群，就是将相同的程序，通过不断横向扩展，以提高服务能力的方式。 “分布式系统”和“集群”的定义够都简单吧。 分布式系统的优势那么，为啥我们要用分布式系统？ 说起分布式系统，我们就不得不说下分布式系统的祖先——集中式系统。集中式系统跟分布式系统是完全相反的两个概念。集中式系统就是把所有的程序、功能都集中到一台主机上，从而往外提供服务的方式。 集中式系统最容易理解了。比如，我们主机的PC电脑，或者手机，我们把各种软件都安装在一台机子上，当我需要什么功能，我就从这台机子上去获取。再比如，我们在学生时代做的课程设计或者开发时的小应用，我们把Web服务器、数据库等都会安装到一台电脑上。好处是，易于理解、方便维护，想要的东西我都放到了一个地方，东西好找啊。当然弊端也是显而易见的，如果这台机子崩了，或者硬盘坏了，那相当与整个系统就奔溃了，而且如果备份也是在这个硬盘上，那相当于招了灭顶之灾。 所以巴菲特有个关于投资的名言，就是“不要把鸡蛋放在一个篮子里”。对于系统而言也是如此。厂商的机子不可能永远保证永远不坏，我们也无法保证黑客不会来对我们的系统搞基，最为关键的是，我们自己无法保证自己的程序不会出bug。所以问题无法避免，错误也不可避免。我们只能鸡蛋分散到不同的篮子里，来减轻一锅端的风险。这就是为什么需要分布式系统的原因。 使用分布式系统的另外一个理由是可扩展性。毕竟任何主机（哪怕是小型机、超级计算机）都会有性能的极限。而分布式系统可以通过不断扩张主机的数量以实现横向水平性能的扩展。大家也都了解到 Google 的服务器主机，大多是淘汰的二线机子拼凑的吧。 分布式系统面临的挑战毫无疑问，分布式系统对于集中式系统而言，在实现上会更加复杂。分布式系统将会是更难理解、设计、构建和管理的，同时意味着应用程序的根源问题更难发现。 设计分布式系统时，经常需要考虑如下的挑战： 异构性：分布式系统由于基于不同的网络、操作系统、计算机硬件和编程语言来构造，必须要考虑一种通用的网络通信协议来屏蔽异构系统之间的差异。一般交由中间件来处理这些差异。 缺乏全球时钟：在程序需要协作时，它们通过交换消息来协调它们的动作。紧密的协调经常依赖于对程序动作发生时间的共识，但是，实际上网络上计算机同步时钟的准确性受到极大的限制，即没有一个正确时间的全局概念。这是通过网络发送消息作为唯一的通信方式这一事实带来的直接结果。 一致性：数据被分散或者复制到不同的机器上，如何保证各台主机之间的数据的一致性将成为一个难点。 故障的独立性：任何计算机都有可能故障，且各种故障不尽相同。他们之间出现故障的时机也是相互独立的。一般分布式系统要设计成被允许出现部分故障而不影响整个系统的正常使用。 并发：分布式系统的目的，是为了更好的共享资源。那么系统中的每个资源都必须被设计成在并发环境中是安全的。 透明性：分布式系统中任何组件的故障、或者主机的升级、迁移对于用户来说都是透明的，不可见的。 开放性：分布式系统由不同的程序员来编写不同的组件，组件最终要集成成为一个系统，那么组件所发布的接口必须遵守一定的规范且能够被互相理解。 安全性：加密用于给共享资源提供适当的保护，在网络上所有传递的敏感信息，都需要进行加密。拒绝服务攻击仍然是一个有待解决的问题。 可扩展性：系统要设计成随着业务量的增加，相应的系统也必须要能扩展来提供对应的服务。 如何来设计分布式设计分布式系统的本质就是“如何合理将一个系统拆分成多个子系统部署到不同机器上”。所以首要考虑的问题是如何合理的将系统进行拆分。由于拆分后的各个子系统不可能孤立的存在，必然是通过网络进行连接交互，所以它们之间如何通信变得尤为重要。当然在通信过程要识别“敌我”，防止信息在传递过程中被拦截和窜改，这就涉及到安全问题了。分布式系统要适应不断增长的业务需求，那么就需要考虑其扩展性。分布式系统还必须要保证可靠性和数据的一致性。 概况起来，在设计分布式系统时，应考虑以下几个问题： 系统如何拆分为子系统？ 如何规划子系统间的通信？ 通信过程中的安全如何考虑？ 如何让子系统可以扩展？ 子系统的可靠性如何保证？ 数据的一致性是如何实现的？ 实际上，上面的每一个问题都不是简单的问题。还好，我们要感谢开源，让这个时代的技术可以共享，让实现复杂系统的成本越来越低。 比如，我们在设计通信时，我们可以采用面向消息的中间件，比如Apache ActiveMQ、RabbitMQ、Apache RocketMQ、Apache Kafka等，也有类似与 Google Protocol Buffer、Thrift等 RPC框架。 在设计分布式计算时，我们分布式计算可以采用 MapReduce、Apache Hadoop、Apache Spark 等。 在大数据和分布式存储方面，我们可以选择 Apache HBase、Apache Cassandra、Memcached、Redis、MongoDB等。 在分布式监控方面，常用的技术包括Nagios、Zabbix、Consul、ZooKeeper等。 当然，本文也只是抛砖引玉，不可能面面俱到。望各位读者有不同的见解，欢迎讨论。]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Junit测试延伸——私有方法测试]]></title>
    <url>%2F%E6%B5%8B%E8%AF%95%2FJunit%E6%B5%8B%E8%AF%95%E5%BB%B6%E4%BC%B8%E2%80%94%E2%80%94%E7%A7%81%E6%9C%89%E6%96%B9%E6%B3%95%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[之前上软件测试课的时候，曾经听闻某些同学在考试的时候使用黑科技，直接通过反射的方式直接调用private方法，提高测试用例覆盖率。当时只是觉得这个好高大上，但是也没在意去学习一下。因为类私有方法只允许被本类访问，而其他类无权调用，我只要通过给其他public的方法写好测试用例就行了。 然鹅，前几天公司的研发流程中需要变更行覆盖率达到一定程度，可是由于逻辑太复杂的，导致部分private方法很难被覆盖到。时间不多了，只能出绝招了——通过Junit测试私有方法。 Talk is easy. Show me the code. 我们举个栗子来学习如何通过Junit测试私有方法。 我们先定义一个Dog类12345678910/** * author: winsky * date: 2018/7/21 * description: */public class Dog &#123; private String bark(String name) &#123; return name + " 汪汪汪"; &#125;&#125; OK，现在我们来写上面这个私有方法的测试类：12345678910111213141516171819/** * author: winsky * date: 2018/7/21 * description: */public class DogTest &#123; private Dog dog = new Dog(); @Test public void testBark() throws Exception &#123; Class&lt;Dog&gt; clazz = Dog.class; Method declaredMethod = clazz.getDeclaredMethod("bark", String.class); declaredMethod.setAccessible(true); Object invoke = declaredMethod.invoke(dog, "妞妞"); declaredMethod.setAccessible(false); Assert.assertEquals("妞妞 汪汪汪", invoke); &#125;&#125; 我们运行上面的测试，来看下Junit绿条情况。测试通过，没问题。]]></content>
      <categories>
        <category>测试</category>
      </categories>
      <tags>
        <tag>Junit测试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac、Linux完美解决SSH连接GBK服务器]]></title>
    <url>%2FLinux%2FMac%E3%80%81Linux%E5%AE%8C%E7%BE%8E%E8%A7%A3%E5%86%B3SSH%E8%BF%9E%E6%8E%A5GBK%E6%9C%8D%E5%8A%A1%E5%99%A8%2F</url>
    <content type="text"><![CDATA[目前我们开发的项目最终都是部署到服务器上，而服务器一般而言都是Linux的操作系统。大部分情况下，服务器字符编码都是UTF-8的，开发环境也基本都是UTF-8。然鹅，最近待的公司，一些服务器字符编码是GBK，我本地默认的UTF-8登录上去中文直接乱码。 本文介绍了如何快速无痛地解决本地UTF-8环境登录GBK的服务器。由于现在是Mac的重度使用者，所以本文也就介绍了Mac下如何完美解决SSH连接GBK服务器，Windows的机器SecureCRT可以完美解决这个问题，自行谷歌一下。 我一直使用的是iterm2终端（当然，Mac自带的terminal终端也很好用）。 一开始的时候，我是使用创建不同的profile来指定iterm2的编码方式，用着也还算OK，唯一蛋疼的就是，每当需要GBK编码时候，都得右键重新开一个iterm2的窗口，而且通过command+t新开出来的页面还是采用了default profile。 所以就需要浓重地推出本文的主角——luit luit介绍luit 命令是一个过滤器，在任意应用程序和UTF-8终端仿真器之间运行。luit命令将应用程序输出从语言环境的编码转换为UTF-8，并将终端输入从UTF-8转换为语言环境的编码。 多语言应用程序必须设置为仅生成UTF-8代码。不得使用命令生成UTF-8之外的输出。 luit命令由终端以透明方式调用。有关从命令行运行luit命令的信息，请参见示例。 SSH执行过程图示在终端中执行命令，通信过程示意图如图1或者图2所示。 执行命令时，使用终端模拟器配置的编码方案，对原命令字符串进行编码，将得到的字节流传递给“命令执行单元”； 展现命令执行结果时，获取字节流形式的命令执行结果，使用终端模拟器配置的编码方案，对其进行解码，得到字符串形式的命令执行结果。 使用luit后的执行过程通过使用luit命令，现在在终端模拟器中执行命令，通信过程示意图如图3或者图4所示。 此时执行命令时，使用终端模拟器配置的编码方案，对原命令字符串进行编码，得到字节流，再使用UTF-8编码方案进行解码得到字符流，最后使用luit命令中指定的编码方案（比如luit -encoding GBK program [args]中的GBK）进行编码，将得到的字节流传递给“命令执行单元” ； 展现命令执行结果时，获取字节流形式的命令执行结果，使用luit命令中指定的编码方案进行解码得到字符流，再使用UTF-8编码方案进行编码得到字节流，最后对得到的字节流使用终端模拟器配置的编码方案进行解码，得到字符串形式的命令执行结果。 安装上面介绍了一堆SSH和luit的原理，下面抓紧进入正题，安装使用luit。 从luit官网下载相应的源码，然后解压编译安装123456789101112# 解压tar zxvf luit.tar.gz# 进入源码文件夹，可能版本会不一样cd luit-20180508 # 配置./configure# 编译make# 安装make install# 卸载make uninstall 安装完成后，最简单的使用方式就是luit -encoding gbk ssh了，这已经能满足我们目前的需求了，当然为了学深学透，我们还是在下一章节介绍一下luit命令的全部语法 使用语法1luit [ options ] [ -- ] [ program [ args ] ] options 项目 描述 -h 显示帮助摘要。 -list 列示受支持的字符集和编码。 -v 详细。 -c 将标准输入转换为标准输出。 -x 在子函数终止时立即退出。这可能会导致 luit 命令在子函数的输出结束时丢失数据。 -argv0 name 设置随 argv[0] 命令传递的子代的名称。 -encoding encoding 指定 luit 命令使用当前语言环境编码之外的编码。 +oss 禁止在应用程序输出中解释单个 shift。 +ols 禁止在应用程序输出中解释锁定 shift。 +osl 禁止在应用程序输出中解释字符集选择序列。 +ot 禁止解释所有序列并将应用程序输出中的所有序列原样传递到终端。 -k7 为键盘输入生成 7 位字符。 +kss 禁止为键盘输入生成单个 shift。 +kssgr 在键盘输入的单个 shift 后使用 GL 代码。缺省情况下，会在生成 8 位键盘输入时在单个 shift 后生成 GR 代码。 -kls 为键盘输入生成锁定 shift (SO/SI)。 -gl gn 设置 GL 的初始分配。参数必须为 g0、g1、g2 或 g3。缺省值取决于语言环境，通常为 g0。 -gr gk 设置 GR 的初始分配。缺省值取决于语言环境，通常为 g2（EUC 语言环境除外，这种语言环境的缺省值为 g1）。 -g0 charset 设置在 G0 中最初选择的 charset 的值。缺省值取决于语言环境，但通常为 ASCII。 -g1 charset 设置在 G1 中最初选择的 charset 的值。缺省值取决于语言环境。 -g2 charset 设置在 G2 中最初选择的 charset 的值。缺省值取决于语言环境。 -g3 charset 设置在 G3 中最初选择的 charset 的值。缺省值取决于语言环境。 -ilog filename 将从子代接收的所有字节记录到 filename 文件中。 -olog filename 将发送到终端仿真器的所有字节记录到 filename 文件中。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[应用系统之间数据传输的几种方式]]></title>
    <url>%2Fweb%E5%BC%80%E5%8F%91%2F%E5%BA%94%E7%94%A8%E7%B3%BB%E7%BB%9F%E4%B9%8B%E9%97%B4%E6%95%B0%E6%8D%AE%E4%BC%A0%E8%BE%93%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[随着近年来SOA（面向服务技术架构）的兴起，越来越多的应用系统开始进行分布式的设计和部署。系统由原来单一的技术架构变成面向服务的多系统架构。原来在一个系统之间可以完成的业务流程，通过多系统的之间多次交互来实现。这里不打算介绍如何进行SOA架构的设计，而是介绍一下应用系统之间如何进行数据的传输。 应用系统之间数据传输有三个要素：传输方式，传输协议，数据格式 socket方式描述 Socket方式是最简单的交互方式。是典型才c/s交互模式。一台客户机，一台服务器。服务器提供服务，通过ip地址和端口进行服务访问。而客户机通过连接服务器指定的端口进行消息交互。其中传输协议可以是tcp/UDP 协议。而服务器和约定了请求报文格式和响应报文格式。如下图所示： 目前我们常用的http调用，java远程调用，webserivces 都是采用的这种方式，只不过不同的就是传输协议以及报文格式。 优点 易于编程，目前java提供了多种框架，屏蔽了底层通信细节以及数据传输转换细节。 容易控制权限。通过传输层协议https，加密传输的数据，使得安全性提高 通用性比较强，无论客户端是.net架构，java，python 都是可以的。尤其是webservice规范，使得服务变得通用 缺点 服务器和客户端必须同时工作，当服务器端不可用的时候，整个数据交互是不可进行。 当传输数据量比较大的时候，严重占用网络带宽，可能导致连接超时。使得在数据量交互的时候，服务变的很不可靠。 ftp/文件共享服务器方式描述对于大数据量的交互，采用这种文件的交互方式最适合不过了。系统A和系统B约定文件服务器地址，文件命名规则,文件内容格式等内容，通过上传文件到文件服务器进行数据交互。 最典型的应用场景是批量处理数据：例如系统A把今天12点之前把要处理的数据生成到一个文件，系统B第二天凌晨1点进行处理，处理完成之后，把处理结果生成到一个文件，系统A 12点在再进行结果处理。这种状况经常发生在A是事物处理型系统，对响应要求比较高，不适合做数据分析型的工作，而系统B是后台系统，对处理能力要求比较高，适合做批量任务系统。 以上只是说明通过文件方式的数据交互，实际情况B完成任务之后，可能通过socket的方式通知A，不一定是通过文件方式。 优点 在数据量大的情况下，可以通过文件传输，不会超时，不占用网络带宽。 方案简单，避免了网络传输，网络协议相关的概念。 缺点 不太适合做实时类的业务 必须有共同的文件服务器，文件服务器这里面存在风险。因为文件可能被篡改，删除，或者存在泄密等。 必须约定文件数据的格式，当改变文件格式的时候，需要各个系统都同步做修改。 数据库共享数据方式描述系统A和系统B通过连接同一个数据库服务器的同一张表进行数据交换。当系统A请求系统B处理数据的时候，系统A Insert一条数据，系统B select 系统A插入的数据进行处理。 优点 相比文件方式传输来说，因为使用的同一个数据库，交互更加简单。 由于数据库提供相当做的操作，比如更新，回滚等。交互方式比较灵活,而且通过数据库的事务机制，可以做成可靠性的数据交换。 缺点 当连接B的系统越来越多的时候，由于数据库的连接池是有限的，导致每个系统分配到的连接不会很多，当系统越来越多的时候，可能导致无可用的数据库连接。 一般情况，来自两个不同公司的系统，不太会开放自己的数据库给对方连接，因为这样会有安全性影响。 message方式描述Java消息服务（Java Message Service）是message数据传输的典型的实现方式。系统A和系统B通过一个消息服务器进行数据交换。系统A发送消息到消息服务器，如果系统B订阅系统A发送过来的消息，消息服务器会消息推送给B。双方约定消息格式即可。目前市场上有很多开源的jms消息中间件，比如 ActiveMQ, OpenJMS 。 优点 由于jms定义了规范，有很多的开源的消息中间件可以选择，而且比较通用。接入起来相对也比较简单。 通过消息方式比较灵活，可以采取同步，异步，可靠性的消息处理，消息中间件也可以独立出来部署。 缺点 学习jms相关的基础知识，消息中间件的具体配置，以及实现的细节对于开发人员来说还是有一点学习成本的。 在大数据量的情况下，消息可能会产生积压，导致消息延迟，消息丢失，甚至消息中间件崩溃。 应用举例目前业务人员需要导入一个大文件到系统A，系统A保存文件信息，而文件里面的明细信息需要导入到系统B进行分析，当系统B分析完成之后，需要把分析结果通知系统A。 系统A先保存文件到文件服务器。 系统A 通过webservice 调用系统B提供的服务器，把需要处理的文件名发送到系统B。由于文件很大，需要处理很长时间，所以B不及时处理文件，而是保存需要处理的文件名到数据库，通过后台定时调度机制去处理。所以B接收请求成功，立刻返回系统A成功。 系统B定时查询数据库记录，通过记录查找文件路径，找到文件进行处理。这个过程很长。 系统B处理完成之后发送消息给系统A，告知系统A文件处理完成。 系统A 接收到系统B请求来的消息，进行展示任务结果]]></content>
      <categories>
        <category>web开发</category>
      </categories>
      <tags>
        <tag>web开发</tag>
        <tag>应用交互</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tair入门教程(3)：使用demo]]></title>
    <url>%2F%E4%B8%AD%E9%97%B4%E4%BB%B6%2Ftair%2FTair%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B(3)%EF%BC%9A%E4%BD%BF%E7%94%A8demo%2F</url>
    <content type="text"><![CDATA[在Tair入门教程(1)：Tair介绍我们介绍了Tair的主要功能和使用场景，以及其架构和各部分组件的作用。 在Tair入门教程(2):Tair环境搭建中我们搭建好了Tair的服务器环境。 接下来就是编写实际的应用代码了。本文给出了一个小demo，以供学习。详细的项目可以参见github源代码。 构建maven项目首先构建MAVEN项目，导入所需要的jar包依赖 需要导入的有tair-client等jar包。 编写客户端程序直接给我示例demo，很容易理解123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960package com.winsky;import com.taobao.tair.DataEntry;import com.taobao.tair.Result;import com.taobao.tair.ResultCode;import com.taobao.tair.impl.DefaultTairManager;import java.util.ArrayList;import java.util.List;/** * author: winsky * date: 2018/5/16 * description: */public class TairClientTest &#123; public static void main(String[] args) &#123; // 创建config server列表 List&lt;String&gt; confServers = new ArrayList&lt;&gt;(); confServers.add("172.17.68.153:5198"); // confServers.add("10.10.7.144:51980"); // 可选 // 创建客户端实例 DefaultTairManager tairManager = new DefaultTairManager(); tairManager.setConfigServerList(confServers); // 设置组名 tairManager.setGroupName("group_1"); // 初始化客户端 tairManager.init(); // put 10 items for (int i = 0; i &lt; 10; i++) &#123; // 第一个参数是namespace，第二个是key，第三是value，第四个是版本，第五个是有效时间 ResultCode result = tairManager.put(0, "k" + i, "v" + i, 0, 10); System.out.println("put k" + i + ":" + result.isSuccess()); System.out.println(result.getMessage()); if (!result.isSuccess()) break; &#125; // get one // 第一个参数是namespce，第二个是key Result&lt;DataEntry&gt; result = tairManager.get(0, "k3"); System.out.println("get:" + result.isSuccess()); if (result.isSuccess()) &#123; DataEntry entry = result.getValue(); if (entry != null) &#123; // 数据存在 System.out.println("value is " + entry.getValue().toString()); &#125; else &#123; // 数据不存在 System.out.println("this key doesn't exist."); &#125; &#125; else &#123; // 异常处理 System.out.println(result.getRc().getMessage()); &#125; &#125;&#125; 运行结果123456789101112131415log4j:WARN No appenders could be found for logger (com.taobao.tair.impl.ConfigServer).log4j:WARN Please initialize the log4j system properly.log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.put k0:trueput k1:trueput k2:trueput k3:trueput k4:trueput k5:trueput k6:trueput k7:trueput k8:trueput k9:trueget:truevalue is v3 注意事项：测试如果不是在config server或data server上进行，那么一定要确保测试端系统与config server和data server能互相通信，即ping通。否则有可能会报下面这样的错误：123Exception in thread &quot;main&quot; java.lang.RuntimeException: init config failed at com.taobao.tair.impl.DefaultTairManager.init(DefaultTairManager.java:80) at tair.client.TairClientTest.main(TairClientTest.java:27)]]></content>
      <categories>
        <category>中间件</category>
        <category>tair</category>
      </categories>
      <tags>
        <tag>中间件</tag>
        <tag>tair</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tair入门教程(2)：Tair环境搭建]]></title>
    <url>%2F%E4%B8%AD%E9%97%B4%E4%BB%B6%2Ftair%2FTair%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B(2)%EF%BC%9ATair%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[上一篇文章Tair入门教程(1)：Tair介绍我们介绍了Tair的主要功能和使用场景，以及其架构和各部分组件的作用。 本文以CentOS为例，介绍如何搭建一个Tair环境，采用Zookeeper作为注册中心。 基于阿里云学生机CentOS 7.3系统 安装参照官方开源的GitHub中的方法，我们采用编译源码的方式来安装tair。 安装步骤1234567891011121314# clone 代码到本地git clone https://github.com/alibaba/tair.git# 安装必要依赖sudo yum install -y openssl-devel libcurl-devel# 编译依赖./bootstrap.sh# 检测和生成 Makefile (默认安装位置是 ~/tair_bin, 修改使用 --prefix=目标目录)./configure# 编译和安装到目标目录make -j &amp;&amp; make install 遇到的问题上面安装步骤都很简单，看上去是分分钟就能安装完，但是在安装的时候还是会有各种各样的问题。这里记录一下我遇到的一个问题。123g++: internal compiler error: Killed (program cc1plus)Please submit a full bug report, 经过谷歌，最终发现主要原因是内存不足，g++编译时需要大量内存，临时使用交换分区来解决吧12345sudo dd if=/dev/zero of=/swapfile bs=64M count=16sudo mkswap /swapfilesudo swapon /swapfile 上面三行命令临时挂载了一个64M的Swap内存。这样操作之后我就顺利编译完成了。 当然，编译完成后如果想卸载Swap内存，可以使用如下的命令123sudo swapoff /swapfilesudo rm /swapfile 启动部署配置tair的运行, 至少需要一个 config server 和一个 data server。推荐使用两个config server 多个data server的方式。两个config server有主备之分。 tair有三个配置文件，分别是对config server、data server及group信息的配置，在tair_bin安装目录下的etc目录下有这三个配置文件的样例，我们将其复制一下，成为我们需要的配置文件。123cp configserver.conf.default configserver.confcp dataserver.conf.default dataserver.confcp group.conf.default group.conf 配置文件详解 在配置之前，请查阅官网给出的配置文件字段详解，下面直接贴出我自己的配置并加以简单的说明。 配置ConfigServer12345678910111213141516## tair 2.3 --- configserver config#[public]config_server=172.17.68.153:5198config_server=172.17.68.153:5198[configserver]port=5198log_file=/root/tair_bin/logs/config.logpid_file=/root/tair_bin/logs/config.pidlog_level=warngroup_file=/root/tair_bin/etc/group.confdata_dir=/root/tair_bin/data/datadev_name=eth0 首先需要配置config server的服务器地址和端口号，端口号可以默认，服务器地址改成自己的，有一主一备两台configserver，这里仅为测试使用就设置为一台了。 log_file/pid_file等的路径设置最好用绝对路径，默认的是相对路径，而且是不正确的相对路径（没有返回上级目录），因此这里需要修改。注意data文件和log文件非常重要，data文件不可缺少，而log文件是部署出错后能给你详细的出错原因。 dev_name很重要，需要设置为你自己当前网络接口的名称，默认为eth0。 配置data server123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270## tair 2.3 --- tairserver config#[public]config_server=172.17.68.153:5198config_server=172.17.68.153:5198[tairserver]##storage_engine:## mdb# ldb#storage_engine=mdblocal_mode=0##mdb_type:# mdb# mdb_shm#mdb_type=mdb_shm# shm file prefix, located in /dev/shm/, the leading &apos;/&apos; is mustmdb_shm_path=/mdb_shm_inst# (1&lt;&lt;mdb_inst_shift) would be the instance countmdb_inst_shift=3# (1&lt;&lt;mdb_hash_bucket_shift) would be the overall bucket count of hashtable# (1&lt;&lt;mdb_hash_bucket_shift) * 8 bytes memory would be allocated as hashtablemdb_hash_bucket_shift=24# milliseconds, time of one round of the checking in mdb lasts before having a breakmdb_check_granularity=15# increase this factor when the check thread of mdb incurs heavy load# cpu load would be around 1/(1+mdb_check_granularity_factor)mdb_check_granularity_factor=10#tairserver listen portport=5191supported_admin=0process_thread_num=12io_thread_num=12dup_io_thread_num=1##mdb size in MB#slab_mem_size=4096log_file=/root/tair_bin/logs/server.logpid_file=/root/tair_bin/logs/server.pidis_namespace_load=1is_flowcontrol_load=1tair_admin_file = /root/tair_bin/etc/admin.confput_remove_expired=0# set same number means to disable the memory merge, like 5-5mem_merge_hour_range=5-5# 1ms copy 300 itemsmem_merge_move_count=300log_level=warndev_name=eth0ulog_dir=/root/tair_bin/data/ulogulog_file_number=3ulog_file_size=64check_expired_hour_range=2-4check_slab_hour_range=5-7dup_sync=1dup_timeout=500do_rsync=0rsync_io_thread_num=1rsync_task_thread_num=4rsync_listen=1# 0 mean old version# 1 mean new versionrsync_version=0rsync_config_service=http://localhost:8080/hangzhou/group_1rsync_config_update_interval=60# much resemble json format# one local cluster config and one or multi remote cluster config.# &#123;local:[master_cs_addr,slave_cs_addr,group_name,timeout_ms,queue_limit],remote:[...],remote:[...]&#125;# rsync_conf=&#123;local:[10.0.0.1:5198,10.0.0.2:5198,group_local,2000,1000],remote:[10.0.1.1:5198,10.0.1.2:5198,group_remote,2000,800]&#125;# if same data can be updated in local and remote cluster, then we need care modify time to# reserve latest update when do rsync to each other.rsync_mtime_care=0# rsync data directory(retry_log/fail_log..)rsync_data_dir=./data/remote# max log file size to record failed rsync data, rotate to a new file when over the limitrsync_fail_log_size=30000000# when doing retry, size limit of retry log&apos;s memory usersync_retry_log_mem_size=100000000# depot duplicate update when one server downdo_dup_depot=0dup_depot_dir=./data/dupdepot[flow_control]# default flow control settingdefault_net_upper = 30000000default_net_lower = 15000000default_ops_upper = 30000default_ops_lower = 20000default_total_net_upper = 75000000default_total_net_lower = 65000000default_total_ops_upper = 50000default_total_ops_lower = 40000[ldb]#### ldb manager config## data dir prefix, db path will be data/ldbxx, &quot;xx&quot; means db instance index.## so if ldb_db_instance_count = 2, then leveldb will init in## /data/ldb1/ldb/, /data/ldb2/ldb/. We can mount each disk to## data/ldb1, data/ldb2, so we can init each instance on each disk.data_dir=data/ldb## leveldb instance count, buckets will be well-distributed to instancesldb_db_instance_count=1## whether load backup version when startup.## backup version may be created to maintain some db data of specifid version.ldb_load_backup_version=0## whether support version strategy.## if yes, put will do get operation to update existed items&apos;s meta info(version .etc),## get unexist item is expensive for leveldb. set 0 to disable if nobody even care version stuff.ldb_db_version_care=1## time range to compact for gc, 1-1 means do no compaction at allldb_compact_gc_range = 3-6## backgroud task check compact interval (s)ldb_check_compact_interval = 120## use cache count, 0 means NOT use cache,`ldb_use_cache_count should NOT be larger## than `ldb_db_instance_count, and better to be a factor of `ldb_db_instance_count.## each cache mdb&apos;s config depends on mdb&apos;s config item(mdb_type, slab_mem_size, etc)ldb_use_cache_count=1## cache stat can&apos;t report configserver, record stat locally, stat file size.## file will be rotate when file size is over this.ldb_cache_stat_file_size=20971520## migrate item batch size one time (1M)ldb_migrate_batch_size = 3145728## migrate item batch count.## real batch migrate items depends on the smaller size/countldb_migrate_batch_count = 5000## comparator_type bitcmp by default# ldb_comparator_type=numeric## numeric comparator: special compare method for user_key sorting in order to reducing compact## parameters for numeric compare. format: [meta][prefix][delimiter][number][suffix]## skip meta size in compare# ldb_userkey_skip_meta_size=2## delimiter between prefix and number# ldb_userkey_num_delimiter=:###### use blommfilterldb_use_bloomfilter=1## use mmap to speed up random acess file(sstable),may cost much memoryldb_use_mmap_random_access=0## how many highest levels to limit compactionldb_limit_compact_level_count=0## limit compaction ratio: allow doing one compaction every ldb_limit_compact_interval## 0 means limit all compactionldb_limit_compact_count_interval=0## limit compaction time interval## 0 means limit all compactionldb_limit_compact_time_interval=0## limit compaction time range, start == end means doing limit the whole day.ldb_limit_compact_time_range=6-1## limit delete obsolete files when finishing one compactionldb_limit_delete_obsolete_file_interval=5## whether trigger compaction by seekldb_do_seek_compaction=0## whether split mmt when compaction with user-define logic(bucket range, eg)ldb_do_split_mmt_compaction=0## do specify compact## time range 24 hoursldb_specify_compact_time_range=0-6ldb_specify_compact_max_threshold=10000## score threshold default = 1ldb_specify_compact_score_threshold=1#### following config effects on FastDump ###### when ldb_db_instance_count &gt; 1, bucket will be sharded to instance base on config strategy.## current supported:## hash : just do integer hash to bucket number then module to instance, instance&apos;s balance may be## not perfect in small buckets set. same bucket will be sharded to same instance## all the time, so data will be reused even if buckets owned by server changed(maybe cluster has changed),## map : handle to get better balance among all instances. same bucket may be sharded to different instance based## on different buckets set(data will be migrated among instances).ldb_bucket_index_to_instance_strategy=map## bucket index can be updated. this is useful if the cluster wouldn&apos;t change once started## even server down/up accidently.ldb_bucket_index_can_update=1## strategy map will save bucket index statistics into file, this is the file&apos;s directoryldb_bucket_index_file_dir=./data/bindex## memory usage for memtable sharded by bucket when batch-put(especially for FastDump)ldb_max_mem_usage_for_memtable=3221225472######## leveldb config (Warning: you should know what you&apos;re doing.)## one leveldb instance max open files(actually table_cache_ capacity, consider as working set, see `ldb_table_cache_size)ldb_max_open_files=65535## whether return fail when occure fail when init/load db, and## if true, read data when compactiong will verify checksumldb_paranoid_check=0## memtable sizeldb_write_buffer_size=67108864## sstable sizeldb_target_file_size=8388608## max file size in each level. level-n (n &gt; 0): (n - 1) * 10 * ldb_base_level_sizeldb_base_level_size=134217728## sstable&apos;s block size# ldb_block_size=4096## sstable cache size (override `ldb_max_open_files)ldb_table_cache_size=1073741824##block cache sizeldb_block_cache_size=16777216## arena used by memtable, arena block size#ldb_arenablock_size=4096## key is prefix-compressed period in block,## this is period length(how many keys will be prefix-compressed period)# ldb_block_restart_interval=16## specifid compression method (snappy only now)# ldb_compression=1## compact when sstables count in level-0 is over this triggerldb_l0_compaction_trigger=1## whether limit write with l0&apos;s filecount, if falseldb_l0_limit_write_with_count=0## write will slow down when sstables count in level-0 is over this trigger## or sstables&apos; filesize in level-0 is over trigger * ldb_write_buffer_size if ldb_l0_limit_write_with_count=0ldb_l0_slowdown_write_trigger=32## write will stop(wait until trigger down)ldb_l0_stop_write_trigger=64## when write memtable, max level to below maybeldb_max_memcompact_level=3## read verify checksumldb_read_verify_checksums=0## write sync log. (one write will sync log once, expensive)ldb_write_sync=0## bits per key when use bloom filter#ldb_bloomfilter_bits_per_key=10## filter data base logarithm. filterbasesize=1&lt;&lt;ldb_filter_base_logarithm#ldb_filter_base_logarithm=12[extras]######## RT-related #########rt_oplist=1,2# Threashold of latency beyond which would let the request be dumped out.rt_threshold=8000# Enable RT Module at startuprt_auto_enable=0# How many requests would be subject to RT Modulert_percent=100# Interval to reset the latency statistics, by secondsrt_reset_interval=10######## HotKey-related ########hotk_oplist=2# Sample counthotk_sample_max=50000# Reap counthotk_reap_max=32# Whether to send client feedback responsehotk_need_feedback=0# Whether to dump out packets, caches or hot keyshotk_need_dump=0# Whether to just Do Hot one roundhotk_one_shot=0# Whether having hot key depends on: sigma &gt;= (average * hotk_hot_factor)hotk_hot_factor=0.8 config_server的配置与之前必须完全相同。 这里面的port和heartbeat_port是data server的端口号和心跳端口号，必须确保系统能给你使用这些端口号。一般默认的即可 data文件、log文件等很重要，与前一样，最好用绝对路径 配置group信息12345678910111213141516171819202122232425262728293031323334#group name[group_1]# data move is 1 means when some data serve down, the migrating will be start.# default value is 0_data_move=0#_min_data_server_count: when data servers left in a group less than this value, config server will stop serve for this group#default value is copy count._min_data_server_count=1#_plugIns_list=libStaticPlugIn.so_build_strategy=1 #1 normal 2 rack_build_diff_ratio=0.6 #how much difference is allowd between different rack# diff_ratio = |data_sever_count_in_rack1 - data_server_count_in_rack2| / max (data_sever_count_in_rack1, data_server_count_in_rack2)# diff_ration must less than _build_diff_ratio_pos_mask=65535 # 65535 is 0xffff this will be used to gernerate rack info. 64 bit serverId &amp; _pos_mask is the rack info,_copy_count=1_bucket_number=1023# accept ds strategy. 1 means accept ds automatically_accept_strategy=1_allow_failover_server=0# data center A_server_list=172.17.68.153:5191#_server_list=192.168.1.2:5191#_server_list=192.168.1.3:5191#_server_list=192.168.1.4:5191# data center B#_server_list=192.168.2.1:5191#_server_list=192.168.2.2:5191#_server_list=192.168.2.3:5191#_server_list=192.168.2.4:5191#quota info_areaCapacity_list=0,1124000; 这个文件我只配置了data server列表，我只有一个dataserver，因此只需配置一个。 启动集群在完成安装配置之后, 可以启动集群了. 启动的时候需要先启动data server 然后再启动cofnig server. 进入tair_bin目录后，按顺序启动：12sudo sbin/tair_server -f etc/dataserver.conf # 在dataserver端启动sudo sbin/tair_cfg_svr -f etc/configserver.conf # 在config server端启动 执行启动命令后，在两端通过ps aux | grep tair查看是否启动了，这里启动起来只是第一步，还需要测试看是否真的启动成功，通过下面命令测试：1234567sudo sbin/tairclient -c 172.17.68.153:5198 -g group_1TAIR&gt; put k1 v1 put: successTAIR&gt; put k2 v2put: successTAIR&gt; get k2KEY: k2, LEN: 2 其中172.17.68.153:5198是config server IP:PORT，group_1是group name，在group.conf里配置的。 遗留问题按照上面的步骤，可以配置一个可用的Tair测试环境。但是，经过我的测试，这个集群只是在这台机器上可用，不能远程访问。参照网上的教程，把ip换成公网ip或者公网ip和私有ip混合的模式，都不行，不能远程访问。只能把程序发送到机器上去执行。]]></content>
      <categories>
        <category>中间件</category>
        <category>tair</category>
      </categories>
      <tags>
        <tag>中间件</tag>
        <tag>tair</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tair入门教程(1)：Tair介绍]]></title>
    <url>%2F%E4%B8%AD%E9%97%B4%E4%BB%B6%2Ftair%2FTair%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B(1)%EF%BC%9ATair%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[Tair是由淘宝网自主开发的Key/Value结构数据存储系统，在淘宝网有着大规模的应用。您在登录淘宝、查看商品详情页面或者在淘江湖和好友“捣浆糊”的时候，都在直接或间接地和Tair交互。 Tair于2010年6月30号在淘宝开源平台上正式对外开源，本文较详细地介绍了Tair提供的功能及其实现的细节，希望对大家进一步了解Tair有所帮助。 Tair是提供快速访问的内存（MDB引擎）/持久化（LDB引擎）存储服务，基于高性能、高可用的分布式集群架构，满足读写性能要求高及容量可弹性伸缩的业务需求。 功能Tair是一个Key/Value结构数据的解决方案，它默认支持基于内存和文件的两种存储方式，分别和我们通常所说的缓存和持久化存储对应。 Tair除了普通Key/Value系统提供的功能，比如get、put、delete以及批量接口外，还有一些附加的实用功能，使得其有更广的适用场景，包括： Version支持 原子计数器 Item支持 Version支持Tair中的每个数据都包含版本号，版本号在每次更新后都会递增。这个特性有助于防止由于数据的并发更新导致的问题。 比如，系统有一个value为a,b,c，A和B同时get到这个value。A执行操作，在后面添加一个d，value为a,b,c,d。B执行操作添加一个e，value为a,b,c,e。如果不加控制，无论A和B谁先更新成功，它的更新都会被后到的更新覆盖。 Tair无法解决这个问题，但是引入了version机制避免这样的问题。还是拿刚才的例子，A和B取到数据，假设版本号为10，A先更新，更新成功后，value为a,b,c,d，与此同时，版本号会变为11。当B更新时，由于其基于的版本号是10，服务器会拒绝更新，从而避免A的更新被覆盖。B可以选择get新版本的value，然后在其基础上修改，也可以选择强行更新。 原子计数器Tair从服务器端支持原子的计数器操作，这使得Tair成为一个简单易用的分布式计数器。 Item支持Tair还支持将value视为一个item数组，对value中的部分item进行操作。比如有个key的value为[1,2,3,4,5]，我们可以只获取前两个item，返回[1,2]，也可以删除第一个item，还支持将数据删除，并返回被删除的数据，通过这个接口可以实现一个原子的分布式FIFO的队列。 架构Tair整体架构图 系统架构一个Tair集群主要包括3个必选模块：ConfigServer、Dataserver和Client. 通常情况下，一个Tair集群中包含2台Configserver及多台DataServer。其中两台Configserver互为主备。通过和Dataserver之间的心跳检测获取集群中存活可用的Dataserver，构建数据在集群中的分布信息（对照表）。Dataserver负责数据的存储，并按照Configserver的指示完成数据的复制和迁移工作。Client在启动的时候，从Configserver获取数据分布信息，根据数据分布信息，和相应的Dataserver进行交互，完成用户的请求。 从架构上看，Configserver的角色类似于传统应用系统的中心节点，整个集群服务依赖于Configserver的正常工作。而实际上，Tair的Configserver是非常轻量级的，当正在工作的Configserver宕机的时候，另一台会在秒级别时间内自动接管。而且，即使出现两台ConfigServer同时宕机的恶劣情况，只要DataServer没有新的变化，Tair依然服务正常。应用在使用时只需要连接Configserver，而不需要知道内部节点的情况。 ConfigServer两台Configserver互为主备 通过和Dataserver之间的心跳检测来获取集群中存活、可用的Dataserver节点信息 根据获取的Dataserver节点信息构建数据在集群中的分布表 提供数据分布表的查询服务 调度Dataserver之间的数据迁移、复制 DataServers提供存储引擎 接受Client发起的put/get/remove等操作 执行数据迁移、复制 访问统计 client提供访问Tair集群的API 更新并缓存数据分布表 LocalCache，避免过热的数据访问影响Tair集群服务。 流量控制 产品功能分布式架构采用分布式集群架构，具备自动容灾及故障迁移能力。 支持负载均衡，数据均匀分布。 支持弹性扩展系统的存储空间及吞吐性能，突破海量数据高 QPS 性能瓶颈。 丰富易用的接口数据结构丰富，支持单级 key-value 结构，同时也支持二级索引结构。 可支持多种应用场景，支持计数器模式。 支持数据过期和版本控制。 应用场景数据库缓存随着业务量上升，数据库收到的并发请求数会不断增加，随之而来的问题就是数据库系统的负载升高，响应延迟下降，严重的时候，甚至有可能因此而导致服务中断。为了解决这类问题，可以将Tair MDB产品与数据库产品搭配使用，组成高吞吐、低延迟的存储解决方案。 MDB响应速度快，通常在几毫秒内即可完成请求。另一方面，MDB可支持更高的QPS，处理比数据库更多的并发请求。用户通过业务观察，将热点数据放置在MDB中，可以极大缓解数据库的负载，不仅可以节省数据库成本，而且提高了系统的可用性。 临时数据存储应用程序需要维护大量临时数据，例如社交网络、电子商务、游戏、广告等，将临时数据存储在MDB中，可以降低内存管理的开销，改进应用程序工作负载。在分布式环境中，可以将MDB作为全局统一存储，避免单点故障造成的数据丢失，同时解决多个应用程序之间的同步问题。 常用的例子是将MDB作为session manager使用，如果网站采用分布式部署，且访问量很高，那么同一个用户的不同请求可能会发送到不同的web服务器上，这时可以用MDB作为全局存储，用于保存Session数据、用户的Token、权限信息等数据。 数据存储推荐或广告类业务通常需要离线计算大量数据。LDB支持持久化存储，且性能优异，支持数据的在线化服务，因此，用户可将离线数据定期导入LDB提供在线服务。 榜单类业务也可通过计算后，将最终的榜单信息存储在LDB，并直接展示给前端应用。既可满足存储需求，也可以满足快速访问的需求。 黑白名单安全类应用有较多黑白名单场景，该类黑白名单场景具有命中率低，访问量大，数据丢失业务有损等特性。LDB支持数据持久化功能，同时也支持高访问量，因此被广泛应用于此类场景。 分布式锁为防止因多线程并发造成的数据不一致或逻辑混乱，分布式锁是比较常见的处理方式。利用Tair的version特性或者计数功能可以实现分布式锁。由于LDB具有持久化功能，当服务有出现宕机的情况，也不会因此出现锁丢失或者锁不可释放的情况。 Tair GitHub]]></content>
      <categories>
        <category>中间件</category>
        <category>tair</category>
      </categories>
      <tags>
        <tag>中间件</tag>
        <tag>tair</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dubbo入门教程(3)：使用demo]]></title>
    <url>%2F%E4%B8%AD%E9%97%B4%E4%BB%B6%2Fdubbo%2FDubbo%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B(3)%EF%BC%9A%E4%BD%BF%E7%94%A8demo%2F</url>
    <content type="text"><![CDATA[在Dubbo入门教程(1)：Dubbo介绍我们从电子商务系统的演变历史，引出了什么是Dubbo，介绍了Dubbo的架构各个部分组件的作用。 在Dubbo入门教程(2):Dubbo环境搭建中我们搭建好了dubbo的服务器环境。 接下来就是编写实际的应用代码了。本文给出了一个Spring-Dubbo-Zookeeper的小demo，以供学习。详细的项目可以参见github源代码。 构建maven项目首先构建MAVEN项目，导入所需要的jar包依赖 需要导入的有spring, dubbo, zookeeper等jar包。 创建dubbo-apidubbo-api是一个MAVEN项目(有独立的pom.xml，用来打包供提供者消费者使用) 在项目中定义服务接口：该接口需单独打包，在服务提供方和消费方共享。 123456789101112package com.winsky.dubbo.demo;import java.util.List;/** * author: winsky * date: 2018/5/14 * description: */public interface DemoService &#123; List&lt;String&gt; getPermissions(Long id);&#125; 创建dubbo-providerdubbo-provider是一个MAVEN项目(有独立的pom.xml，用来打包供消费者使用) 实现公共接口，此实现对消费者隐藏： 1234567891011121314151617181920212223package com.winsky.dubbo.demo.impl;import com.winsky.dubbo.demo.DemoService;import java.util.ArrayList;import java.util.List;/** * author: winsky * date: 2018/5/14 * description: */public class DemoServiceImpl implements DemoService &#123; @Override public List&lt;String&gt; getPermissions(Long id) &#123; List&lt;String&gt; demo = new ArrayList&lt;&gt;(); demo.add(String.format("Permission_%d", id - 1)); demo.add(String.format("Permission_%d", id)); demo.add(String.format("Permission_%d", id + 1)); return demo; &#125;&#125; 注意，需要在dubbo-provider的maven依赖中加入公共接口所在的依赖1234567&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.winsky&lt;/groupId&gt; &lt;artifactId&gt;dubbo-api&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 用Spring配置声明暴露服务12345678910111213141516171819&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:dubbo="http://code.alibabatech.com/schema/dubbo" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://code.alibabatech.com/schema/dubbo http://code.alibabatech.com/schema/dubbo/dubbo.xsd"&gt; &lt;!--定义了提供方应用信息，用于计算依赖关系；在 dubbo-admin 或 dubbo-monitor 会显示这个名字，方便辨识--&gt; &lt;dubbo:application name="demotest-provider" owner="programmer" organization="dubbox"/&gt; &lt;!--使用 zookeeper 注册中心暴露服务，注意要先开启 zookeeper--&gt; &lt;dubbo:registry address="zookeeper://test.ufeng.top:2181"/&gt; &lt;!-- 用dubbo协议在20880端口暴露服务 --&gt; &lt;dubbo:protocol name="dubbo" port="20880"/&gt; &lt;!--使用 dubbo 协议实现定义好的 api.PermissionService 接口--&gt; &lt;dubbo:service interface="com.winsky.dubbo.demo.DemoService" ref="demoService" protocol="dubbo"/&gt; &lt;!--具体实现该接口的 bean--&gt; &lt;bean id="demoService" class="com.winsky.dubbo.demo.impl.DemoServiceImpl"/&gt;&lt;/beans&gt; 启动远程服务：1234567891011121314151617181920package com.winsky.dubbo.demo.impl;import org.springframework.context.support.ClassPathXmlApplicationContext;import java.io.IOException;/** * author: winsky * date: 2018/5/14 * description: */public class Provider &#123; public static void main(String[] args) throws IOException &#123; ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext("provider.xml"); System.out.println(context.getDisplayName() + ": here"); context.start(); System.out.println("服务已经启动..."); System.in.read(); &#125;&#125; 创建dubbo-consumerdubbo-consumer是一个MAVEN项目(可以有多个consumer，但是需要配置好)。 调用所需要的远程服务： 通过Spring配置引用远程服务：123456789101112&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:dubbo="http://code.alibabatech.com/schema/dubbo" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://code.alibabatech.com/schema/dubbo http://code.alibabatech.com/schema/dubbo/dubbo.xsd"&gt; &lt;dubbo:application name="demotest-consumer" owner="programmer" organization="dubbox"/&gt; &lt;!--向 zookeeper 订阅 provider 的地址，由 zookeeper 定时推送--&gt; &lt;dubbo:registry address="zookeeper://test.ufeng.top:2181"/&gt; &lt;!--使用 dubbo 协议调用定义好的 api.PermissionService 接口--&gt; &lt;dubbo:reference id="permissionService" interface="com.winsky.dubbo.demo.DemoService"/&gt;&lt;/beans&gt; 启动Consumer,调用远程服务：123456789101112131415161718192021package com.winsky.dubbo.consumer;import com.winsky.dubbo.demo.DemoService;import org.springframework.context.support.ClassPathXmlApplicationContext;/** * author: winsky * date: 2018/5/14 * description: */public class Consumer &#123; public static void main(String[] args) &#123; //测试常规服务 ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext("consumer.xml"); context.start(); System.out.println("consumer start"); DemoService demoService = context.getBean(DemoService.class); System.out.println("consumer"); System.out.println(demoService.getPermissions(1L)); &#125;&#125; 运行项目先确保provider已被运行后再启动consumer模块 运行提供者： 消费者成功调用提供者所提供的远程服务： 当然，这只是一个模拟的项目，实际中有多提供者多消费者情况，比这要复杂的多，当然只有这样才能体现dubbo的特性。]]></content>
      <categories>
        <category>中间件</category>
        <category>dubbo</category>
      </categories>
      <tags>
        <tag>中间件</tag>
        <tag>dubbo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dubbo入门教程(2)：Dubbo环境搭建]]></title>
    <url>%2F%E4%B8%AD%E9%97%B4%E4%BB%B6%2Fdubbo%2FDubbo%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B(2)%EF%BC%9ADubbo%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[上一篇文章Dubbo入门教程(1)：Dubbo介绍我们从电子商务系统的演变历史，引出了什么是Dubbo，介绍了Dubbo的架构各个部分组件的作用。 本文以CentOS为例，介绍如何搭建一个Dubbo环境，采用Zookeeper作为注册中心。 基于阿里云学生机CentOS 7.3系统 Dubbo环境的的搭建，主要分为以下几个步骤 安装Zookeeper,启动； 安装maven，方便编译Dubbo-admin 安装Dubbo-admin，实现监控。 Zookeeper安装本Demo中的Dubbo注册中心采用的是Zookeeper。为什么采用Zookeeper呢？ Zookeeper是一个分布式的服务框架，是树型的目录服务的数据存储，能做到集群管理数据，这里能很好的作为Dubbo服务的注册中心。 Dubbo能与Zookeeper做到集群部署，当提供者出现断电等异常停机时，Zookeeper注册中心能自动删除提供者信息，当提供者重启时，能自动恢复注册数据，以及订阅请求 首先下载zookeeper项目，我安装时最新版本是zookeeper-3.4.12。你也可以去官网查看并下载最新的版本。 将下载下来的压缩包解压tar -xzvf zookeeper-3.4.12.tar.gz 将conf目录下的zoo_sample.cfg复制成zookeeper默认读取的配置文件zoo.cfg；（cp zoo_sample.cfg zoo.cfg） 修改zoo.cfg文件内容： 如果不需要集群，需要配置的参数为 12345tickTime=2000initLimit=10syncLimit=5dataDir=/home/zookeeper/dataclientPort=2181 如果需要配置集群环境，则需要配置的参数为 1234567tickTime=2000initLimit=10syncLimit=5dataDir=/home/zookeeper/dataclientPort=2181server.1=127.0.0.1:2888:3888（两个server的端口不能一样，第一端口是server间通讯用，第二个是选举leader用）server.2=127.0.0.2:2889:3889 需要保证dataDir指向的目录实际是存在； 如果是集群环境，则要在这个目录里加一个myid名字的文件，文件内容为server.x这个x值。 到此为止，zookeeper已经安装配置完毕，可以启动了。启动命令为：$ZKPATH/bin/zkServer.sh start maven安装 下载最新的安装包，我安装时的最新版本是apache-maven-3.5.3。你也可以去官网查看并下载最新的版本。 将下载下来的压缩包解压tar -xzvf apache-maven-3.5.3-bin.tar.gz 配置环境变量vi /etc/profile，加入以下内容12export M2_HOME=/usr/local/apache-mavenexport PATH=$PATH:$M2_HOME/bin 使环境变量shengxiaosource /etc/profile 验证是否安装成功mvn -version dubbo安装 在github上获得dubbo-admin源码git clone https://github.com/apache/incubator-dubbo-ops.git 将dubbo-admin编译并打包 进入incubator-dubbo-ops\dubbo-admin目录，输入下面命令mvn clean package 打包成功之后将在incubator-dubbo-ops\dubbo-admin\target目录下生成dubbo-admin-2.0.0.war 将dubbo-admin-2.0.0.war复制到Tomcat的webapps目录下，并重命名为dubbo-admin.war 修改WEB-INF下的dubbo.properties文件，配置zookeeper服务器和dubbo的管理后台的帐号密码 如果是多个zookeeper服务器，那服务器的值可设置为：zookeeper://127.0.0.1:2181?backup=127.0.0.2:2181 重新启动tomcat服务器 此时dubbo的管理后台就配置完了,可通过访问：http://test.ufeng.top/dubbo-admin/访问了]]></content>
      <categories>
        <category>中间件</category>
        <category>dubbo</category>
      </categories>
      <tags>
        <tag>中间件</tag>
        <tag>dubbo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dubbo入门教程(1)：Dubbo介绍]]></title>
    <url>%2F%E4%B8%AD%E9%97%B4%E4%BB%B6%2Fdubbo%2FDubbo%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B(1)%EF%BC%9ADubbo%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[Dubbo 是阿里巴巴公司开源的一个高性能优秀的服务框架，使得应用可通过高性能的 RPC 实现服务的输出和输入功能，可以和 Spring框架无缝集成。 在dubbo的官方网站上，是这样来介绍的。DUBBO是一个分布式服务框架，致力于提供高性能和透明化的RPC远程服务调用方案，是阿里巴巴SOA服务化治理方案的核心框架，每天为2,000+个服务提供3,000,000,000+次访问量支持，并被广泛应用于阿里巴巴集团的各成员站点。从这样一段介绍中，有这样几个值得我们关注的点，分布式服务框架以及提供SOA的服务化治理。 背景Dubbo开始于电商系统，因此在这里先从电商系统的演变讲起。 演变阶段单一应用框架(ORM)当网站流量很小时，只需一个应用，将所有功能如下单支付等都部署在一起，以减少部署节点和成本。 缺点：单一的系统架构，使得在开发过程中，占用的资源越来越多，而且随着流量的增加越来越难以维护 垂直应用框架(MVC)垂直应用架构解决了单一应用架构所面临的扩容问题，流量能够分散到各个子系统当中，且系统的体积可控，一定程度上降低了开发人员之间协同以及维护的成本，提升了开发效率。 缺点：但是在垂直架构中相同逻辑代码需要不断的复制，不能复用。 分布式应用架构(RPC)当垂直应用越来越多，应用之间交互不可避免，将核心业务抽取出来，作为独立的服务，逐渐形成稳定的服务中心 流动计算架构(SOA)随着服务化的进一步发展，服务越来越多，服务之间的调用和依赖关系也越来越复杂，诞生了面向服务的架构体系(SOA)，也因此衍生出了一系列相应的技术，如对服务提供、服务调用、连接处理、通信协议、序列化方式、服务发现、服务路由、日志输出等行为进行封装的服务框架 演变过程从以上是电商系统的演变可以看出架构演变的过程： 单一应用架构当网站流量很小时，只需一个应用，将所有功能都部署在一起，以减少部署节点和成本。 此时，用于简化增删改查工作量的 数据访问框架(ORM) 是关键。 垂直应用架构当访问量逐渐增大，单一应用增加机器带来的加速度越来越小，将应用拆成互不相干的几个应用，以提升效率。 此时，用于加速前端页面开发的Web框架(MVC) 是关键。 分布式服务架构当垂直应用越来越多，应用之间交互不可避免，将核心业务抽取出来，作为独立的服务，逐渐形成稳定的服务中心，使前端应用能更快速的响应多变的市场需求。 此时，用于提高业务复用及整合的 分布式服务框架(RPC) 是关键。 流动计算架构当服务越来越多，容量的评估，小服务资源的浪费等问题逐渐显现，此时需增加一个调度中心基于访问压力实时管理集群容量，提高集群利用率。 此时，用于提高机器利用率的 资源调度和治理中心(SOA) 是关键。 Dubbo是什么Dubbo是： 一款分布式服务框架 高性能和透明化的RPC远程服务调用方案 SOA服务治理方案 每天为2千多个服务提供大于30亿次访问量支持，并被广泛应用于阿里巴巴集团的各成员站点以及别的公司的业务中。 Dubbo架构 Provider：暴露服务的服务提供方。 Consumer：调用远程服务的服务消费方。 Registry：服务注册与发现的注册中心。 Monitor：统计服务的调用次数和调用时间的监控中心。 调用流程 服务容器负责启动，加载，运行服务提供者。 服务提供者在启动时，向注册中心注册自己提供的服务。 服务消费者在启动时，向注册中心订阅自己所需的服务。 注册中心返回服务提供者地址列表给消费者，如果有变更，注册中心将基于长连接推送变更数据给消费者。 服务消费者，从提供者地址列表中，基于软负载均衡算法，选一台提供者进行调用，如果调用失败，再选另一台调用。 服务消费者和提供者，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心 Dubbo注册中心对于服务提供方，它需要发布服务，而且由于应用系统的复杂性，服务的数量、类型也不断膨胀； 对于服务消费方，它最关心如何获取到它所需要的服务，而面对复杂的应用系统，需要管理大量的服务调用。 而且，对于服务提供方和服务消费方来说，他们还有可能兼具这两种角色，即既需要提供服务，有需要消费服务。 通过将服务统一管理起来，可以有效地优化内部应用对服务发布/使用的流程和管理。服务注册中心可以通过特定协议来完成服务对外的统一。 Dubbo提供的注册中心有如下几种类型可供选择： Multicast注册中心 Zookeeper注册中心 Redis注册中心 Simple注册中心 Dubbo优缺点优点： 透明化的远程方法调用 像调用本地方法一样调用远程方法 只需简单配置，没有任何API侵入。 软负载均衡及容错机制 可在内网替代nginx、lvs等硬件负载均衡器。 服务注册中心自动注册 &amp; 配置管理 不需要写死服务提供者地址，注册中心基于接口名自动查询提供者ip。 使用类似zookeeper等分布式协调服务作为服务注册中心，可以将绝大部分项目配置移入zookeeper集群。 服务接口监控与治理-Dubbo-admin与Dubbo-monitor提供了完善的服务接口管理与监控功能，针对不同应用的不同接口，可以进行多版本，多协议，多注册中心管理。 缺点：只支持JAVA语言]]></content>
      <categories>
        <category>中间件</category>
        <category>dubbo</category>
      </categories>
      <tags>
        <tag>中间件</tag>
        <tag>dubbo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx服务配置]]></title>
    <url>%2FNginx%2FNginx%E6%9C%8D%E5%8A%A1%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[前面一篇博文我们介绍了Nginx的安装，并以默认配置成功启动了Nginx服务。需要的同学可以参考Windows服务器安装Nginx并配置服务 当然，我对Nginx的理解还处于一知半解的地步，对Nginx的原理部分、甚至配置文件的详细配置，也有待进一步提高。这篇博文主要是介绍Nginx的各种配置。本文的写作思路，是根据平时自己的生产实践中所用到的一些配置，加以整合修改，所以随着对Nginx的深入使用，本文也将持续更新。 不同域名提供不同服务这种方式是我在接触到Nginx的时候学会的最简单的转发设置方式。配置文件格式如下：参照本站博客的配置情况12345678910111213server&#123; listen 80; server_name blog.winsky.wang ; location / &#123; proxy_set_header HOST $host; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://localhost:4000/; &#125;&#125; listen表示监听的端口 server_name表示监听的访问域名 location表示资源位置，其中具体的内容表示将来自blog.winsky.wang域名80端口的服务转到本机4000端口上。 上面四行proxy_set_header是配置本地服务可以正确得到访问者的来源ip等信息。 这种方式一般适用于在服务器上开一个服务，监听某个端口并提供服务。但是他也有一个缺点，那就是每新来一个服务，都要新建一个域名解析，这会带来不必要的操作 不同路径转发不同服务之前我一直都是用的根据域名来转发的这种挫挫的方式，直到，学院有台电脑，只有ip，没有映射到外网的域名，所以只能采用根据不同路径抓发不同的路径的方式。 这种方式的参考配置文件如下，基本是在默认配置的基础上更改了location的位置123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960#user nobody;worker_processes 1;#error_log logs/error.log;#error_log logs/error.log notice;#error_log logs/error.log info;#pid logs/nginx.pid;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; #log_format main &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &apos; # &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos; # &apos;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&apos;; #access_log logs/access.log main; sendfile on; #tcp_nopush on; #keepalive_timeout 0; keepalive_timeout 65; #gzip on; server &#123; listen 80; server_name localhost; #charset koi8-r; #access_log logs/host.access.log main; location / &#123; root html; index index.html index.htm; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; location /t/ &#123; proxy_set_header HOST $host; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://localhost:8080/; &#125; &#125;&#125;]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[将自己的程序配置成Windows服务]]></title>
    <url>%2FWindows%2F%E5%B0%86%E8%87%AA%E5%B7%B1%E7%9A%84%E7%A8%8B%E5%BA%8F%E9%85%8D%E7%BD%AE%E6%88%90Windows%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[现在互联网企业中一般都是使用的是Linux服务器，但是也有少部分企业单位，比如学校，使用Windows服务器偏多。 一般在Linux服务器上，我们可以简单地通过在/etc/rc.local文件中加入启动命令来配置服务自启。 在Windows Server上，我们当然也希望服务能自启了，本文就教大家如何在Windows服务器上将自己的程序配置成Windows服务。 服务器采用Windows Server 2012 R2 网上看到这篇文章分享了几种解决方案，这里也列出来备忘。 介绍几种常用的注册window服务工具 本文采用的是winsw在Windows上将应用程序安装为系统服务，如果有其他更好的方式，欢迎评论留言。 下载首先要下载winsw。它是一个单个的可执行文件，我们到Github release这里就可以下载winsw了。 一般来说当然是下载最新的。winsw可以运行在.NET2和.NET4两个版本上，当然如果使用Win10等比较新的系统，最好下载更新版本的.NET。 下载完之后最好把文件改成一个有特定意义的名字，例如nginx-service.exe这样的，方便后面输入跟配置文件对应上。 编写配置文件我们需要编写一个和程序同名的XML文件作为winsw的配置文件。文件大体上长这样，这是我们配置Nginx服务的例子。1234567891011&lt;service&gt; &lt;id&gt;Nginx&lt;/id&gt; &lt;name&gt;Nginx&lt;/name&gt; &lt;description&gt;nginx service&lt;/description&gt; &lt;executable&gt;G:\nginx-1.14.0\nginx.exe&lt;/executable&gt; &lt;logpath&gt;G:\nginx-1.14.0\logs\&lt;/logpath&gt; &lt;logmode&gt;roll&lt;/logmode&gt; &lt;depend&gt;&lt;/depend&gt; &lt;startargument&gt;-p G:\nginx-1.14.0&lt;/startargument&gt; &lt;stopargument&gt;-p G:\nginx-1.14.0 -s stop&lt;/stopargument&gt; &lt;/service&gt; 其中name为 服务名，executable为可执行程序路径，logpath为程序运行日志路径，其他大家看到XML的标签名应该就能知道是做什么的了，这里我就不再描述了。 注册服务编写好配置文件之后，记得把配置文件和可执行文件放在一起，这样winsw才能正确识别配置文件。一般建议将这两个文件一起放到要注册的应用程序的目录下。 然后我们打开一个管理员权限的命令提示符或Powershell窗口，然后输入下面的命令 nginx-service.exe install 注：nginx-service.exe uninstall命令可删除对应的系统服务 nginx-service.exe stop命令可停止对应的系统服务 nginx-service.exe start命令可启动对应的系统服务 查看服务是否安装成功在计算机管理 -&gt; 服务中寻找刚刚命名的服务，如果能找到就是注册成功 如服务为未运行状态，可在此启动服务，或设置为自动启动]]></content>
      <categories>
        <category>Windows</category>
      </categories>
      <tags>
        <tag>Windows服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Windows服务器安装Nginx并配置服务]]></title>
    <url>%2FNginx%2FWindows%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AE%89%E8%A3%85Nginx%E5%B9%B6%E9%85%8D%E7%BD%AE%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[Nginx是俄罗斯人编写的十分轻量级的HTTP服务器。Nginx，它的发音为“engine X”，是一个高性能的HTTP和反向代理服务器，同时也是一个IMAP/POP3/SMTP代理服务器。 Nginx以事件驱动的方式编写，所以有非常好的性能，同时也是一个非常高效的反向代理、负载平衡。目前，国内越来越多的站点采用Nginx作为Web服务器，如国内知名的新浪、163、腾讯、Discuz、豆瓣等。 因为学校的服务器大多数是Windows Server的环境，所以本文总结了几次Windows上Nginx安装配置的过程，以作记录。 服务器采用Windows Server 2012 R2 安装首先去Nginx官网下载最新稳定版的Nginx程序 下载之后解压到服务器上某个具体文件夹，比如这里我放在了G:/目录下 启动注意不要直接双击nginx.exe，这样会导致修改配置后重启、停止nginx无效，需要手动关闭任务管理器内的所有nginx进程 在nginx.exe目录，打开命令行工具，用命令 启动/关闭/重启nginx start nginx : 启动nginx nginx -s reload ：修改配置后重新加载生效 nginx -s reopen ：重新打开日志文件 nginx -t -c /path/to/nginx.conf ：测试nginx配置文件是否正确 关闭nginx：12nginx -s stop :快速停止nginxnginx -s quit ：完整有序的停止nginx 如果遇到报错：bash: nginx: command not found 有可能是你再linux命令行环境下运行了windows命令， 如果你之前是运行nginx -s reload报错， 试下./nginx -s reload，或者 用windows系统自带命令行工具运行 访问上面的启动过程前，我们并没有更改Nginx的配置文件，所以我们采用了Nginx默认的配置文件，启动了Nginx服务 通过访问http://127.0.0.1/我们可以看到Nginx的默认主页 至此，Nginx安装过程到此结束。 配置成Windows服务一般在Linux服务器上，我们可以简单地通过在/etc/rc.local文件中加入启动命令来配置服务自启。在Windows Server上，我们当然也希望服务能自启了，本节就教大家如何在Windows 服务器上将自己的程序配置成Windows服务。 考虑到文章结构和内容的需要，特将本段单独整理成一篇博文，大家可以点击下面的链接阅读。 将自己的程序配置成Windows服务 Nginx配置前面我们只是使用了默认的配置启动了Nginx服务，一般来说，这不能满足我们实际生产环境中的需求，这就需要我们自己配置Nginx的配置文件。 具体配置方法，可以参照下面这篇博文 Nginx配置教程]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
        <tag>Windows服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python2.x和3.x版本区别]]></title>
    <url>%2FPython%2FPython2.x%E4%B8%8E3%E2%80%8B%E2%80%8B.x%E7%89%88%E6%9C%AC%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[Python的3.0版本，常被称为Python 3000，或简称Py3k。相对于Python的早期版本，这是一个较大的升级 为了不带入过多的累赘，Python 3.0在设计的时候没有考虑向下相容 许多针对早期Python版本设计的程式都无法在Python 3.0上正常执行 为了照顾现有程式，Python 2.6作为一个过渡版本，基本使用了Python 2.x的语法和库，同时考虑了向Python 3.0的迁移，允许使用部分Python 3.0的语法与函数 新的Python程式建议使用Python 3.0版本的语法 除非执行环境无法安装Python 3.0或者程式本身使用了不支援Python 3.0的第三方库。目前不支援Python 3.0的第三方库有Twisted, py2exe, PIL等 大多数第三方库都正在努力地相容Python 3.0版本。即使无法立即使用Python 3.0，也建议编写相容Python 3.0版本的程式，然后使用Python 2.6, Python 2.7来执行 Python 3.0的变化主要在以下几个方面 print 函数 print语句没有了，取而代之的是print()函数 Python 2.6与Python 2.7部分地支持这种形式的print语法 在Python 2.6与Python 2.7里面，以下三种形式是等价的 123print "fish"print ("fish") #注意print后面有个空格print("fish") #print()不能带有任何其它参数 然而，Python 2.6实际已经支持新的print()语法 12from __future__ import print_functionprint("fish", "panda", sep=', ') Unicode Python 2 有 ASCII str() 类型，unicode() 是单独的，不是 byte 类型 现在， 在 Python 3，我们最终有了 Unicode (utf-8) 字符串，以及一个字节类：byte 和 bytearrays 由于 Python3.X 源码文件默认使用utf-8编码，这就使得以下代码是合法的: 123&gt;&gt;&gt; 中国 = 'china' &gt;&gt;&gt;print(中国) china Python2.X： 123456&gt;&gt;&gt; str = "我爱北京天安门"&gt;&gt;&gt; str'\xe6\x88\x91\xe7\x88\xb1\xe5\x8c\x97\xe4\xba\xac\xe5\xa4\xa9\xe5\xae\x89\xe9\x97\xa8'&gt;&gt;&gt; str = u"我爱北京天安门"&gt;&gt;&gt; stru'\u6211\u7231\u5317\u4eac\u5929\u5b89\u95e8' Python3.X 123&gt;&gt;&gt; str = "我爱北京天安门"&gt;&gt;&gt; str'我爱北京天安门' 除法运算 Python中的除法较其它语言显得非常高端，有套很复杂的规则 Python中的除法有两个运算符，/和// /除法 在python 2.x中/除法就跟我们熟悉的大多数语言，比如Java啊C啊差不多，整数相除的结果是一个整数，把小数部分完全忽略掉，浮点数除法会保留小数点的部分得到一个浮点数的结果 在python 3.x中/除法不再这么做了，对于整数之间的相除，结果也会是浮点数 Python 2.x: 1234&gt;&gt;&gt; 1 / 20&gt;&gt;&gt; 1.0 / 2.00.5 Python 3.x: 12&gt;&gt;&gt; 1/20.5 //除法 这种除法叫做floor除法 会对除法的结果自动进行一个floor操作 在python 2.x和python 3.x中是一致的 Python 2.x: 12&gt;&gt;&gt; -1 // 2-1 Python 3.x: 12&gt;&gt;&gt; -1 // 2-1 注意的是并不是舍弃小数部分，而是执行floor操作，如果要截取小数部分，那么需要使用math模块的trunc函数 python 3.x:1234&gt;&gt;&gt; import math&gt;&gt;&gt; math.trunc(1 / 2)0&gt;&gt;&gt; math.trunc(-1 / 2) 异常 在 Python 3 中处理异常也轻微的改变了，在 Python 3 中我们现在使用 as 作为关键词 捕获异常的语法由 except exc, var改为 except exc as var 使用语法except (exc1, exc2) as var可以同时捕获多种类别的异常。 Python 2.6已经支持这两种语法。 在2.x时代，所有类型的对象都是可以被直接抛出的，在3.x时代，只有继承自BaseException的对象才可以被抛出。 2.x raise语句使用逗号将抛出对象类型和参数分开，3.x取消了这种奇葩的写法，直接调用构造函数抛出对象即可。 在2.x时代，异常在代码中除了表示程序错误，还经常做一些普通控制结构应该做的事情 在3.x中可以看出，设计者让异常变的更加专一，只有在错误发生的情况才能去用异常捕获语句来处理 xrange 在Python 2中xrange() 创建迭代对象的用法是非常流行的。比如： for 循环或者是列表/集合/字典推导式 这个表现十分像生成器（比如。”惰性求值”）。但是这个 xrange-iterable 是无穷的，意味着你可以无限遍历。 由于它的惰性求值，如果你不得仅仅不遍历它一次，xrange() 函数 比 range() 更快（比如 for 循环）。尽管如此，对比迭代一次，不建议你重复迭代多次，因为生成器每次都从头开始 在 Python 3 中，range() 是像 xrange() 那样实现以至于一个专门的 xrange() 函数都不再存在（在 Python 3 中 xrange() 会抛出命名异常） 12345678910import timeitn = 10000def test_range(n): return for i in range(n): passdef test_xrange(n): for i in xrange(n): pass Python2 12345678910111213141516print 'Python', python_version()print '\ntiming range()' %timeit test_range(n)print '\n\ntiming xrange()' %timeit test_xrange(n)Python 2.7.6timing range()1000 loops, best of 3: 433 µs per looptiming xrange()1000 loops, best of 3: 350 µs per loop Python 3 123456789print('Python', python_version())print('\ntiming range()')%timeit test_range(n)Python 3.4.1timing range()1000 loops, best of 3: 520 µs per loop 1234567print(xrange(10))---------------------------------------------------------------------------NameError Traceback (most recent call last)&lt;ipython-input-5-5d8f9b79ea70&gt; in &lt;module&gt;()----&gt; 1 print(xrange(10))NameError: name 'xrange' is not defined 八进制字面量表示 八进制数必须写成0o777，原来的形式0777不能用了 二进制必须写成0b111 新增了一个bin()函数用于将一个整数转换成二进制字串。 Python 2.6已经支持这两种语法 在Python 3.x中，表示八进制字面量的方式只有一种，就是0o1000 python 2.x 1234&gt;&gt;&gt; 0o1000512&gt;&gt;&gt; 01000512 Python 3.x 1234567&gt;&gt;&gt; 01000 File "&lt;stdin&gt;", line 1 01000 ^SyntaxError: invalid token&gt;&gt;&gt; 0o1000512 不等运算符 Python 2.x中不等于有两种写法 != 和 &lt;&gt; Python 3.x中去掉了&lt;&gt;, 只有!=一种写法，还好，我从来没有使用&lt;&gt;的习惯 去掉了repr表达式 Python 2.x 中反引号相当于repr函数的作用 Python 3.x 中去掉了这种写法，只允许使用repr函数，这样做的目的是为了使代码看上去更清晰么？不过我感觉用repr的机会很少，一般只在debug的时候才用，多数时候还是用str函数来用字符串描述对象 数据类型 Py3.X去除了long类型，现在只有一种整型——int，但它的行为就像2.X版本的long 新增了bytes类型，对应于2.X版本的八位串，定义一个bytes字面量的方法如下: 123&gt;&gt;&gt; b = b'china' &gt;&gt;&gt; type(b) &lt;type 'bytes'&gt; str对象和bytes对象可以使用.encode() (str -&gt; bytes) or .decode() (bytes -&gt; str)方法相互转化 123456&gt;&gt;&gt; s = b.decode() &gt;&gt;&gt; s 'china' &gt;&gt;&gt; b1 = s.encode() &gt;&gt;&gt; b1 b'china' dict的.keys()、.items 和.values()方法返回迭代器，而之前的iterkeys()等函数都被废弃。同时去掉的还有 dict.has_key()，用 in替代它吧]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python高级教程]]></title>
    <url>%2FPython%2FPython%E9%AB%98%E7%BA%A7%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[本文在菜鸟教程的基础上，介绍了Python的高级语法，主要是做个记录，以备后续的查阅。 Python面向对象创建第一个对象1234567891011121314class Employee: '所有员工的基类' empCount = 0 def __init__(self, name, salary): self.name = name self.salary = salary Employee.empCount += 1 def displayCount(self): print "Total Employee %d" % Employee.empCount def displayEmployee(self): print "Name : ", self.name, ", Salary: ", self.salary empCount 变量是一个类变量，它的值将在这个类的所有实例之间共享。你可以在内部类或外部类使用 Employee.empCount 访问。 注：就相当于类的静态变量 第一种方法__init__()方法是一种特殊的方法，被称为类的构造函数或初始化方法，当创建了这个类的实例时就会调用该方法 self 代表类的实例，self 在定义类的方法时是必须有的，虽然在调用时不必传入相应的参数。 Python内置内属性 __dict__ : 类的属性（包含一个字典，由类的数据属性组成） __doc__ :类的文档字符串 __name__: 类名 __module__: 类定义所在的模块（类的全名是__main__.className，如果类位于一个导入模块mymod中，那么className.__module__ 等于 mymod） __bases__ : 类的所有父类构成元素（包含了一个由所有父类组成的元组） Python对象销毁（垃圾回收） Python使用了引用计数这一简单技术来跟踪和回收垃圾 在 Python 内部记录着所有使用中的对象各有多少引用。 一个内部跟踪变量，称为一个引用计数器。 当对象被创建时， 就创建了一个引用计数， 当这个对象不再需要时， 也就是说， 这个对象的引用计数变为0 时， 它被垃圾回收。但是回收不是”立即”的， 由解释器在适当的时机，将垃圾对象占用的内存空间回收。 1234567a = 40 # 创建对象 &lt;40&gt;b = a # 增加引用， &lt;40&gt; 的计数c = [b] # 增加引用. &lt;40&gt; 的计数del a # 减少引用 &lt;40&gt; 的计数b = 100 # 减少引用 &lt;40&gt; 的计数c[0] = -1 # 减少引用 &lt;40&gt; 的计数 Python的垃圾回收机制不仅针对引用计数为0的对象，同样也可以处理循环引用的情况 循环引用是指，两个对象互相引用，但是没有其他变量引用他们。这种情况下仅仅使用引用计数是不够的 Python的垃圾收集器实际上是一个引用计数器和一个循环垃圾收集器。 作为引用计数器的补充，垃圾收集器也会留意被分配的总量很大（及未通过引用计数销毁的那些）对象。在这种情况下，解释器会暂停下来，试图清理所有未引用的循环 析构函数 __del__ 析构函数在对象销毁的时候被调用123456789101112131415class Point: def __init__( self, x=0, y=0): self.x = x self.y = y def __del__(self): class_name = self.__class__.__name__ print class_name, "销毁" pt1 = Point()pt2 = pt1pt3 = pt1print id(pt1), id(pt2), id(pt3) # 打印对象的iddel pt1del pt2del pt3 类的继承 继承语法：class 派生类名(基类名):// … 基类名写在括号里，基本类是在类定义的时候，在元组中指明的 Python中继承的一些特点： 在继承中基类的构造（__init__()方法）不会被自动调用，他需要在其派生类的构造中亲自专门调用 在调用基类的方法时，需要加上基类的类名前缀，且需要带上self参数变量。区别在于，类中调用普通函数时并需要带上self参数 Python总是首先查找对应类型的方法，如果他不能在派生类中找到对应的方法，他才开始到基类中逐个查找 如果在继承元组中列了一个以上的类，那就被称为“多重继承” 123456789101112131415161718192021222324252627282930313233class Parent: # 定义父类 parentAttr = 100 def __init__(self): print "调用父类构造函数" def parentMethod(self): print '调用父类方法' def setAttr(self, attr): Parent.parentAttr = attr def getAttr(self): print "父类属性 :", Parent.parentAttr class Child(Parent): # 定义子类 def __init__(self): print "调用子类构造方法" def childMethod(self): print '调用子类方法 child method' c = Child() # 实例化子类c.childMethod() # 调用子类的方法c.parentMethod() # 调用父类方法c.setAttr(200) # 再次调用父类的方法c.getAttr() # 再次调用父类的方法"""调用子类构造方法调用子类方法 child method调用父类方法父类属性 : 200""" 可以使用issubclass()和isinstance()方法来检测 issubclass()：布尔函数判断一个类是另一个类的子类或者子孙类，语法：issubclass(sub,sup) isinstance(obj,Class)：布尔函数，如果obj是Class类的实例对象或者一个Class子类的实例对象则返回true 方法重写 如果父类方法的功能不能满足需求，子类可以重写父类的方法 在调用时，根据前面提到的原则，Python会优先查找对应类型中的方法，如果不能在派生类中找到对应的方法，才会逐个查找基类中的方法。所以会优先调用子类中重写后的方法 基础的重载方法： 序号 方法，描述&amp;简单的调用 1 __init__(self[,args...])，构造函数，简单的调用方法：obj=className(args) 2 __del__(self)，析构函数，删除一个对象，简单的调用方法：del obj 3 __repr__(self)，转换为供解释器读取的形式，简单的调用方法：repr(obj) 4 __str__(self)，用于将值转为适于人阅读的形式，简单的调用方法：str(obj) 5 __cmp__(self,x)，对象比较，简单的调用方法：cmp(obj,x) 运算符重载 Python同样支持运算符重载，实例如下：1234567891011121314class Vector: def __init__(self, a, b): self.a = a self.b = b def __str__(self): return &apos;Vector (%d, %d)&apos; % (self.a, self.b) def __add__(self,other): return Vector(self.a + other.a, self.b + other.b) v1 = Vector(2,10)v2 = Vector(5,-2)print v1 + v2 类属性与方法类的私有属性 __private_attrs:两个下划线开头，生命该属性为私有，不能在类的外部被使用或者直接访问。在类内部的方法中使用时self.__private_attrs 类的方法 在类的内部，使用def关键字可以为类定义一个方法，与一般函数定义不同，类方法必须包含参数self，且为第一个参数 类的私有方法： __private_methond:两个下划线开头，声明该方法为私有方法，不能在类的外部调用。在类的内部调用self.__private_method Python不允许实例化的类访问私有数据，但是可以使用object._className__attrName访问属性 单下划线、双下划线、头尾双下划线说明： __foo__：定义的是特别方法，类似__init__()方法 _foo：以单划线开头的表示protected类型的变量，即保护类型只能允许其本身与子类进行访问，不用用于from module import * __foo：双下划线表示的是私有类型的变量，只能允许类本身进行访问 Python正则表达式 re模块提供Perl风格的正则表达式模式 re模块使得Python语言拥有全部的正则表达式功能 compile函数根据一个模式字符串和可选的标志参数生成一个正则表达式对象。该对象拥有一些列方法用于正则表达式的匹配和替换 re模块也提供了与这些方法功能完全一致的函数，这些函数使用一个模式字符串作为第一个参数 re.match函数 re.match函数尝试从字符串的起始位置匹配一个模式，如果不是起始位置匹配成功的话，就返回一个None 函数语法：re.match(pattern, string, flags=0) 参数说明： 参数 | 描述 —|— pattern | 匹配的正则表达式 string | 要匹配的字符串 flags | 标志位，用于控制正则表达式的匹配方式，如是否区分大小写，多行匹配等 匹配成功的re.match方法返回一个匹配的对象，否则返回None 可以使用group(num)或者groups匹配对象函数来获取匹配表达式 匹配对象方法 | 描述 — | — group(num=0) | 匹配的整个表达式的字符串，group()可以一次输入多个组号，在这种情况下它将返回一个包含那些组对应值的元组 groups() | 返回一个包含所有小组字符串的元祖，从1到所含的小组号 re.search方法 re.search扫描整个字符串并返回第一个成功的匹配 函数语法：re.search(pattern, string, flags=0) 参数说明： 参数 | 描述 —|— pattern | 匹配的正则表达式 string | 要匹配的字符串 flags | 标志位，用于控制正则表达式的匹配方式，如是否区分大小写，多行匹配等 匹配成功的re.search方法返回一个匹配的对象，否则返回None 可以使用group(num)或者groups匹配对象函数来获取匹配表达式 匹配对象方法 | 描述 — | — group(num=0) | 匹配的整个表达式的字符串，group()可以一次输入多个组号，在这种情况下它将返回一个包含那些组对应值的元组 groups() | 返回一个包含所有小组字符串的元祖，从1到所含的小组号 re.match和re.search的区别 re.match只匹配字符串的开始，如果字符串的开始不符合正则表达式，则匹配失败，函数返回None re.search匹配整个字符串，直到找到一个匹配 检索和替换 Python的re模块提供了re.sub用于替换字符串中的匹配项 语法：re.sub(pattern, repl, string, count=0, flags=0) 参数： 参数 | 描述 — | — pattern | 正则中的模式字符串 repl | 替换的字符串，也可为一个函数 string | 要被查找替换的字符串 count | 模式匹配后替换的最大次数，默认0表示替换所有的匹配 正则表达式的修饰符 正则表达式可以包含一些可选标志修饰符来控制匹配的模式 修饰符被指定为一个可选的标志 多个标志可以通过按位OR(|)它们来指定 修饰符 描述 re.I 使匹配对大小写不敏感 re.L 做本地化识别(locale-aware)匹配 re.M 多行匹配，影响^和$ re.S 使.匹配包括换行在内的所有字符 re.U 根据Unicode字符串集解析字符，这个标志位影响\w,\W,\b,\B re.X 该标志通过给予更加灵活的形式以便将正则表达式写得更易于理解 Python多线程线程的使用方式函数式 调用thread模块中的start_new_thread()函数来产生新线程 thread.start_new_thread ( function, args[, kwargs] ) function - 线程函数 args - 传递给线程函数的参数,他必须是个tuple类型 kwargs - 可选参数1234567891011121314151617181920212223#!/usr/bin/python# -*- coding: UTF-8 -*- import threadimport time # 为线程定义一个函数def print_time( threadName, delay): count = 0 while count &lt; 5: time.sleep(delay) count += 1 print "%s: %s" % ( threadName, time.ctime(time.time()) ) # 创建两个线程try: thread.start_new_thread( print_time, ("Thread-1", 2, ) ) thread.start_new_thread( print_time, ("Thread-2", 4, ) )except: print "Error: unable to start thread" while 1: pass 上面的示例，输出 12345678910Thread-1: Thu Jan 22 15:42:17 2009Thread-1: Thu Jan 22 15:42:19 2009Thread-2: Thu Jan 22 15:42:19 2009Thread-1: Thu Jan 22 15:42:21 2009Thread-2: Thu Jan 22 15:42:23 2009Thread-1: Thu Jan 22 15:42:23 2009Thread-1: Thu Jan 22 15:42:25 2009Thread-2: Thu Jan 22 15:42:27 2009Thread-2: Thu Jan 22 15:42:31 2009Thread-2: Thu Jan 22 15:42:35 2009 线程的结束一般依靠线程函数的自然结束 也可以在线程函数中调用thread.exit()，他抛出SystemExit exception，达到退出线程的目的 用类来包装线程对象线程模块 Python通过两个标准库thread和threading提供对线程的支持 thread提供了低级别的、原始的线程以及一个简单的锁 thread模块提供的其他方法 threading.currentThread(): 返回当前的线程变量 threading.enumerate(): 返回一个包含正在运行的线程的list。正在运行指线程启动后、结束前，不包括启动前和终止后的线程 threading.activeCount(): 返回正在运行的线程数量，与len(threading.enumerate())有相同的结果 除了使用方法外，线程模块同样提供了Thread类来处理线程，Thread类提供了以下方法: run(): 用以表示线程活动的方法 start():启动线程活动 join([time]): 等待至线程中止。这阻塞调用线程直至线程的join() 方法被调用中止-正常退出或者抛出未处理的异常-或者是可选的超时发生isAlive(): 返回线程是否活动的 getName(): 返回线程名 setName(): 设置线程名 使用Threading模块创建线程 使用Threading模块创建线程，直接从threading.Thread继承，然后重写__init__方法和run方法 123456789101112131415161718192021222324252627282930313233343536#!/usr/bin/python# -*- coding: UTF-8 -*- import threadingimport time exitFlag = 0 class myThread (threading.Thread): #继承父类threading.Thread def __init__(self, threadID, name, counter): threading.Thread.__init__(self) self.threadID = threadID self.name = name self.counter = counter def run(self): #把要执行的代码写到run函数里面 线程在创建后会直接运行run函数 print "Starting " + self.name print_time(self.name, self.counter, 5) print "Exiting " + self.name def print_time(threadName, delay, counter): while counter: if exitFlag: threading.Thread.exit() time.sleep(delay) print "%s: %s" % (threadName, time.ctime(time.time())) counter -= 1 # 创建新线程thread1 = myThread(1, "Thread-1", 1)thread2 = myThread(2, "Thread-2", 2) # 开启线程thread1.start()thread2.start() print "Exiting Main Thread" 上面的示例，输出 123456789101112131415Starting Thread-1Starting Thread-2Exiting Main ThreadThread-1: Thu Mar 21 09:10:03 2013Thread-1: Thu Mar 21 09:10:04 2013Thread-2: Thu Mar 21 09:10:04 2013Thread-1: Thu Mar 21 09:10:05 2013Thread-1: Thu Mar 21 09:10:06 2013Thread-2: Thu Mar 21 09:10:06 2013Thread-1: Thu Mar 21 09:10:07 2013Exiting Thread-1Thread-2: Thu Mar 21 09:10:08 2013Thread-2: Thu Mar 21 09:10:10 2013Thread-2: Thu Mar 21 09:10:12 2013Exiting Thread-2 线程同步 如果多个线程共同对某个数据修改，则可能出现不可预料的结果，为了保证数据的正确性，需要对多个线程进行同步 使用Thread对象的Lock和Rlock可以实现简单的线程同步，这两个对象都有acquire方法和release方法，对于那些需要每次只允许一个线程操作的数据，可以将其操作放到acquire和release方法之间 多线程的优势在于可以同时运行多个任务（至少感觉起来是这样） 但是当线程需要共享数据时，可能存在数据不同步的问题 考虑这样一种情况：一个列表里所有元素都是0，线程”set”从后向前把所有元素改成1，而线程”print”负责从前往后读取列表并打印 那么，可能线程”set”开始改的时候，线程”print”便来打印列表了，输出就成了一半0一半1，这就是数据的不同步 为了避免这种情况，引入了锁的概念 锁有两种状态：锁定和未锁定。 每当一个线程比如”set”要访问共享数据时，必须先获得锁定； 如果已经有别的线程比如”print”获得锁定了，那么就让线程”set”暂停，也就是同步阻塞； 等到线程”print”访问完毕，释放锁以后，再让线程”set”继续。 经过这样的处理，打印列表时要么全部输出0，要么全部输出1，不会再出现一半0一半1的尴尬场面1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#!/usr/bin/python# -*- coding: UTF-8 -*- import threadingimport time class myThread (threading.Thread): def __init__(self, threadID, name, counter): threading.Thread.__init__(self) self.threadID = threadID self.name = name self.counter = counter def run(self): print "Starting " + self.name # 获得锁，成功获得锁定后返回True # 可选的timeout参数不填时将一直阻塞直到获得锁定 # 否则超时后将返回False threadLock.acquire() print_time(self.name, self.counter, 3) # 释放锁 threadLock.release() def print_time(threadName, delay, counter): while counter: time.sleep(delay) print "%s: %s" % (threadName, time.ctime(time.time())) counter -= 1 threadLock = threading.Lock()threads = [] # 创建新线程thread1 = myThread(1, "Thread-1", 1)thread2 = myThread(2, "Thread-2", 2) # 开启新线程thread1.start()thread2.start() # 添加线程到线程列表threads.append(thread1)threads.append(thread2) # 等待所有线程完成for t in threads: t.join()print "Exiting Main Thread" 线程优先级队列（ Queue） Python的Queue模块中提供了同步的、线程安全的队列类 FIFO（先入先出)队列Queue LIFO（后入先出）队列LifoQueue 优先级队列PriorityQueue 这些队列都实现了锁原语，能够在多线程中直接使用 可以使用队列来实现线程间的同步 Queue模块中的常用方法: Queue.qsize() 返回队列的大小 Queue.empty() 如果队列为空，返回True,反之False Queue.full() 如果队列满了，返回True,反之False Queue.full 与 maxsize 大小对应 Queue.get([block[, timeout]])获取队列，timeout等待时间 Queue.get_nowait() 相当Queue.get(False) Queue.put(item) 写入队列，timeout等待时间 Queue.put_nowait(item) 相当Queue.put(item, False) Queue.task_done() 在完成一项工作之后，Queue.task_done()函数向任务已经完成的队列发送一个信号 Queue.join() 实际上意味着等到队列为空，再执行别的操作1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162#!/usr/bin/python# -*- coding: UTF-8 -*- import Queueimport threadingimport time exitFlag = 0 class myThread (threading.Thread): def __init__(self, threadID, name, q): threading.Thread.__init__(self) self.threadID = threadID self.name = name self.q = q def run(self): print "Starting " + self.name process_data(self.name, self.q) print "Exiting " + self.name def process_data(threadName, q): while not exitFlag: queueLock.acquire() if not workQueue.empty(): data = q.get() queueLock.release() print "%s processing %s" % (threadName, data) else: queueLock.release() time.sleep(1) threadList = ["Thread-1", "Thread-2", "Thread-3"]nameList = ["One", "Two", "Three", "Four", "Five"]queueLock = threading.Lock()workQueue = Queue.Queue(10)threads = []threadID = 1 # 创建新线程for tName in threadList: thread = myThread(threadID, tName, workQueue) thread.start() threads.append(thread) threadID += 1 # 填充队列queueLock.acquire()for word in nameList: workQueue.put(word)queueLock.release() # 等待队列清空while not workQueue.empty(): pass # 通知线程是时候退出exitFlag = 1 # 等待所有线程完成for t in threads: t.join()print "Exiting Main Thread" 以上示例，执行结果 123456789101112Starting Thread-1Starting Thread-2Starting Thread-3Thread-1 processing OneThread-2 processing TwoThread-3 processing ThreeThread-1 processing FourThread-2 processing FiveExiting Thread-3Exiting Thread-1Exiting Thread-2Exiting Main Thread]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python基本语法]]></title>
    <url>%2FPython%2FPython%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[本文在菜鸟教程的基础上，介绍了Python的基本语法，主要是做个记录，以备后续的查阅。 基础语法py文件头 第一行表示Python的执行器 第二行表示Python文件的编码12#!/usr/bin/python# -*- coding: UTF-8 -*- 注释 多行注释 ‘’’或””” 单行注释 # 标识符 python中的标识符是区分大小写的 以下划线开头的标识符是有特殊意义的 以单下划线开头（_foo）的代表不能直接访问的类属性，需通过类提供的接口进行访问，不能用from xxx import *而导入 以双下划线开头的（foo）代表类的私有成员；以双下划线开头和结尾的`(foo__)代表python里特殊方法专用的标识，如init()`代表类的构造函数 变量类型 Python中的变量赋值不需要申明类型 每个变量在使用前都必须被赋值，变量赋值以后该变量才会被创建 同时为多个变量赋值 创建一个整型对象，值为1，三个变量被分配到相同的内存空间上 1a = b = c = 1 也可以为对个对象指定多个变量变量 1a, b, c = 1, 2, "john" 标准数据类型 Python有五个标准的数据类型 Numbers：数字 String：字符串 List：列表 Tuple：元祖 Dictionary：字典Python数字 支持四种不同的数字类型 int：有符号整型 long：长整型 Python使用”L”来显示长整型 float：浮点型 complex：复数 复数由实数部分和虚数部分构成 可以用a + bj,或者complex(a,b)表示 复数的实部a和虚部b都是浮点型Python字符串 字符串两种取值顺序 从左到右索引默认0开始的，最大范围是字符串长度少1 从右到左索引默认-1开始的，最大范围是字符串开头 从字符串中截取子串 可以使用变量 [头下标:尾下标]，包含下边界，不包含上边界 下标是从 0 开始算起，可以是正数或负数 下标可以为空表示取到头或尾 加号（+）是字符串连接运算符，星号（*）是重复操作 12print str * 2 # 输出字符串两次print str + "TEST" # 输出连接的字符串 Python三引号 python中三引号可以将复杂的字符串进行复制 Python三引号允许一个字符串跨多行，字符串中可以包含换行符、制表符以及其他特殊字符 典型的用例场景： 当一个标识符需要用来表示一段HTML或者SQL的时候，如果用字符串的组合，特殊字符的转义会非常繁琐 使用三引号就可以非常方便的表达123456789101112131415errHTML = '''&lt;HTML&gt;&lt;HEAD&gt;&lt;TITLE&gt;Friends CGI Demo&lt;/TITLE&gt;&lt;/HEAD&gt;&lt;BODY&gt;&lt;H3&gt;ERROR&lt;/H3&gt;&lt;B&gt;%s&lt;/B&gt;&lt;P&gt;&lt;FORM&gt;&lt;INPUT TYPE=button VALUE=BackONCLICK="window.history.back()"&gt;&lt;/FORM&gt;&lt;/BODY&gt;&lt;/HTML&gt;'''cursor.execute('''CREATE TABLE users ( login VARCHAR(8), uid INTEGER,prid INTEGER)''') Python列表 列表中值分隔也可以用变量[头下标：尾下标]来截取相应的列表 从左到右索引默认0开始的 从右到左索引默认-1开始 下标可以为空表示取到头或尾 加号（+）是列表连接运算符，星号（*）是重复操作 创建二维列表12# 将需要的参数写入cols和rows即可，0表示列表元素的默认值list_2d = [[0 for col in range(cols)] for row in range(rows)] Python元组 类似于List 元组用”()”标识 内部元素用逗号隔开 元组不能二次赋值，相当于只读列表 元组中只包含一个元素时，需要在元素后面添加逗号tup1 = (50,); 任意无符号的对象，以逗号隔开，默认为元组 Python字典 列表是有序的对象集合，字典是无序的对象集合 二者的区别： 字典中的元素是通过键来取值的，而不是通过偏移存取 字典用”{}”标识 每个键值(key=&gt;value)对用冒号(:)分割，每个对之间用逗号(,)分割 字典由索引key和他对应的value组成 访问字典里的值：把相应的键放入熟悉的方括弧 字典的键必须是不可变的，所以可以是用数字、字符串或者元组来充当，但是不可以是列表1234567891011121314#!/usr/bin/python# -*- coding: UTF-8 -*- dict = &#123;&#125;dict['one'] = "This is one"dict[2] = "This is two" tinydict = &#123;'name': 'john','code':6734, 'dept': 'sales'&#125; print dict['one'] # 输出键为'one' 的值print dict[2] # 输出键为 2 的值print tinydict # 输出完整的字典print tinydict.keys() # 输出所有键print tinydict.values() # 输出所有值 Python数据类型转换 数据类型的转换，只要将数据类型作为函数名即可 内置的数据类型转换函数 函数 | 描述 —|— int(x [,base]) | 将x转换为一个整数 long(x [,base] ) | 将x转换为一个长整数 complex(real [,imag]) | 创建一个复数 str(x) | 将对象 x 转换为字符串 repr(x) | 将对象 x 转换为表达式字符串 eval(str) | 用来计算在字符串中的有效Python表达式,并返回一个对象 tuple(s) | 将序列 s 转换为一个元组 list(s) | 将序列 s 转换为一个列表 set(s) | 转换为可变集合 dict(d) | 创建一个字典。d 必须是一个序列 (key,value)元组。 frozenset(s) | 转换为不可变集合 chr(x) | 将一个整数转换为一个字符 unichr(x) | 将一个整数转换为Unicode字符 ord(x) | 将一个字符转换为它的整数值 hex(x) | 将一个整数转换为一个十六进制字符串 oct(x) | 将一个整数转换为一个八进制字符串 查看变量的数据类型 所有数据类型都是类,可以通过 type() 查看该变量的数据类型 123456&gt;&gt;&gt; n=1&gt;&gt;&gt; type(n)&lt;type 'int'&gt;&gt;&gt;&gt; n="runoob"&gt;&gt;&gt; type(n)&lt;type 'str'&gt; 还可以使用isinstance来判断 123a = 111isinstance(a, int)True type和isinstance的区别 type()不会认为子类是一种父类类型 isinstance()会认为子类是一种父类类型运算符 Python语言支持一下的运算符 算数运算符 比较运算符 赋值运算符 逻辑运算符 位运算符 成员运算符 身份运算符算数运算符运算符 | 描述—|—/ | 除% | 取模** | 幂 返回x的y次幂// | 取整除 返回商的整数部分 位运算符 运算符 描述 &amp; 按位与运算符：参与运算的两个值,如果两个相应位都为1,则该位的结果为1,否则为0 \ 按位或运算符：只要对应的二个二进位有一个为1时，结果位就为1。 ^ 按位异或运算符：当两对应的二进位相异时，结果为1 ~ 按位取反运算符：对数据的每个二进制位取反,即把1变为0,把0变为1 &lt;&lt; 左移动运算符：运算数的各二进位全部左移若干位，由”&lt;&lt;”右边的数指定移动的位数，高位丢弃，低位补0。 &gt;&gt; 右移动运算符：把”&gt;&gt;”左边的运算数的各二进位全部右移若干位，”&gt;&gt;”右边的数指定移动的位数 成员运算符 运算符 描述 in 如果在指定的序列中找到值返回 True，否则返回 False not in 如果在指定的序列中没有找到值返回 True，否则返回 False 身份运算符 用于比较两个对象的存储单元 运算符 描述 is is判断两个标识符是不是引用自一个对象 is not is not是判断两个标识符是不是引用自不同对象 1234567891011121314151617181920212223242526272829303132333435#!/usr/bin/python# -*- coding: UTF-8 -*-a = 20b = 20if ( a is b ): print "1 - a 和 b 有相同的标识"else: print "1 - a 和 b 没有相同的标识"# 结果：1 - a 和 b 有相同的标识if ( id(a) is not id(b) ): print "2 - a 和 b 有相同的标识"else: print "2 - a 和 b 没有相同的标识"# 结果：2 - a 和 b 有相同的标识# 修改变量 b 的值b = 30if ( a is b ): print "3 - a 和 b 有相同的标识"else: print "3 - a 和 b 没有相同的标识"结果：3 - a 和 b 没有相同的标识if ( a is not b ): print "4 - a 和 b 没有相同的标识"else: print "4 - a 和 b 有相同的标识"结果：4 - a 和 b 没有相同的标识 运算符优先级 以下表格列出了从最高到最低优先级的所有 运算符 运算符 描述 ** 指数（最高优先级） ~ + - 按位翻转，一元加号和减号 * / % // 乘，除，取模，取整除 + - 加法，减法 &gt;&gt; &lt;&lt; 右移，左移 &amp; 与运算 ^ 位运算 &lt;= &lt; &gt; &gt;= 比较运算符 &lt;&gt; == != 等于运算符 = %= /= //= -= += *= **= 赋值运算符 is is not 身份运算符 in not in 成员运算符 not or and 逻辑运算符 条件语句 指定任何非0和非空（null）值为true，0 或者 null为false “判断条件”成立时（非零），则执行后面的语句，而执行内容可以多行，以缩进来区分表示同一范围 判断条件为多个值时： 12345678if 判断条件1: 执行语句1……elif 判断条件2: 执行语句2……elif 判断条件3: 执行语句3……else: 执行语句4…… Python不支持switch，所以多个条件的判断只能使用elif来实现 循环语句while循环语句 循环中使用else语句 while … else 在循环条件为 false 时执行 else 语句块12345678#!/usr/bin/pythoncount = 0while count &lt; 5: print count, " is less than 5" count = count + 1else: print count, " is not less than 5" for循环语句 for循环可以遍历任何序列的项目，如一个列表或者一个字符串 for循环的几种写法 常见写法： 1234567891011#!/usr/bin/python# -*- coding: UTF-8 -*- for letter in 'Python': # 第一个实例 print '当前字母 :', letter fruits = ['banana', 'apple', 'mango']for fruit in fruits: # 第二个实例 print '当前水果 :', fruit print "Good bye!" 通过序列索引迭代 12345678#!/usr/bin/python# -*- coding: UTF-8 -*- fruits = ['banana', 'apple', 'mango']for index in range(len(fruits)): print '当前水果 :', fruits[index] print "Good bye!" 使用内置 enumerate 函数进行遍历 12for index, item in enumerate(sequence): process(index, item) for循环使用else语句 for中的语句和普通的语句没有区别，else中的语句会在循环正常执行完的情况下执行 for不是通过break来跳出循环的情况下才会执行else语句，while-else也一样pass语句 pass语句是空语句，是为了保持程序结构的完整性 pass不做任何事情，一般用作占位语句1234567891011121314151617181920212223#!/usr/bin/python# -*- coding: UTF-8 -*- # 输出 Python 的每个字母for letter in 'Python': if letter == 'h': pass print '这是 pass 块' print '当前字母 :', letterprint "Good bye!""""输出结果：当前字母 : P当前字母 : y当前字母 : t这是 pass 块当前字母 : h当前字母 : o当前字母 : nGood bye!""" Python随机函数 函数 描述 choice(seq) 从序列的元素中随机挑选一个元素，比如random.choice(range(10))，从0到9中随机挑选一个整数 randrange([start,]stop[,step]) 从指定范围内，按执行基数递增的集合中获取一个随机数，基数step缺省值为1 random() 随机生成下一个示数，在[0,1)范围内 seed([x]) 改变随机数生成器的种子seed shuffle(list) 将序列的所有元素随机排序 uniform(x, y) 随机生成下一个实数，它在[x,y]范围内 Python日期和时间 Python 提供了一个 time 和 calendar 模块可以用于格式化日期和时间 时间间隔是以秒为单位的浮点小数 Python 的 time 模块下有很多函数可以转换常见日期格式 1234567#!/usr/bin/python# -*- coding: UTF-8 -*-import time; # 引入time模块ticks = time.time()print "当前时间戳为:", ticks #当前时间戳为: 1459994552.51 Python中经常用一个元组装起来的9组数字来处理时间，即struct_time元组，结构属性如下： 序号 属性 值 0 tm_year 2017 1 tm_mon 1到12 2 tm_mday 1到31 3 tm_hour 0到23 4 tm_min 0到59 5 tm_sec 0到61（60,61是闰秒） 6 tm_ wday 0到6（0是周一） 7 tm_yday 1到366 8 tm_isdst -1,0,1是决定是否是夏令时的旗帜 获取当前时间1234567891011#!/usr/bin/python# -*- coding: UTF-8 -*-import timelocaltime = time.localtime(time.time())print "本地时间为 :", localtime"""本地时间为 : time.struct_time(tm_year=2016, tm_mon=4, tm_mday=7, tm_hour=10, tm_min=3, tm_sec=27, tm_wday=3, tm_yday=98, tm_isdst=0)""" 格式化时间使用 time 模块的 strftime 方法1234567891011121314#!/usr/bin/python# -*- coding: UTF-8 -*-import time# 格式化成2016-03-20 11:45:39形式print time.strftime("%Y-%m-%d %H:%M:%S", time.localtime()) # 格式化成Sat Mar 28 22:24:24 2016形式print time.strftime("%a %b %d %H:%M:%S %Y", time.localtime()) # 将格式字符串转换为时间戳a = "Sat Mar 28 22:24:24 2016"print time.mktime(time.strptime(a,"%a %b %d %H:%M:%S %Y")) Python函数 定义函数的规则 函数代码块以 def 关键词开头，后接函数标识符名称和圆括号() 任何传入参数和自变量必须放在圆括号中间。圆括号之间可以用于定义参数 函数的第一行语句可以选择性地使用文档字符串—用于存放函数说明 函数内容以冒号起始，并且缩进 return [表达式] 结束函数，选择性地返回一个值给调用方。不带表达式的return相当于返回 None 默认情况下，参数值和参数名称是按函数声明中定义的的顺序匹配起来的 关键字参数 使用关键字参数允许函数调用时参数的顺序与声明时不一致，因为Python解释器能够用参数名匹配参数值123456789101112#!/usr/bin/python# -*- coding: UTF-8 -*- #可写函数说明def printinfo( name, age ): "打印任何传入的字符串" print "Name: ", name; print "Age ", age; return; #调用printinfo函数printinfo( age=50, name="miki" ); #能正确调用 缺省参数 调用函数时，缺省参数的值如果没有传入，则被认为是默认值12345678910111213#!/usr/bin/python# -*- coding: UTF-8 -*- #可写函数说明def printinfo( name, age = 35 ): # 参数age设置了缺省值 &quot;打印任何传入的字符串&quot; print &quot;Name: &quot;, name; print &quot;Age &quot;, age; return; #调用printinfo函数printinfo( age=50, name=&quot;miki&quot; );printinfo( name=&quot;miki&quot; ); lambda函数 python 使用 lambda 来创建匿名函数 lambda函数拥有自己的命名空间，且不能访问自有参数列表之外或全局命名空间里的参数123456789#!/usr/bin/python# -*- coding: UTF-8 -*- # 可写函数说明sum = lambda arg1, arg2: arg1 + arg2; # 调用sum函数print "相加后的值为 : ", sum( 10, 20 )print "相加后的值为 : ", sum( 20, 20 ) Python中的不可变对象 变量赋值 a=5 后再赋值 a=10，这里实际是新生成一个 int 值对象 10，再让 a 指向它，而 5 被丢弃，不是改变a的值，相当于新生成了a python 函数的参数传递： 不可变类型：类似 c++ 的值传递，如整数、字符串、元组。如fun（a），传递的只是a的值，没有影响a对象本身。比如在 fun（a）内部修改 a 的值，只是修改另一个复制的对象，不会影响 a 本身。 可变类型：类似 c++ 的引用传递，如 列表，字典。如 fun（la），则是将 la 真正的传过去，修改后fun外部的la也会受影响 实例：123456789#!/usr/bin/python# -*- coding: UTF-8 -*- def ChangeInt( a ): a = 10b = 2ChangeInt(b)print b # 结果是 2 Python模块 模块能定义函数，类和变量，模块里也能包含可执行的代码模块的引入import 模块定义好后，我们可以使用 import 语句来引入模块 1import module1[, module2[,... moduleN] 调用模块中的函数时，必须使用模块名.函数名来进行调用 不管执行多少次import，一个模块只会被导入一次 from……import 语句 from语句可以从模块中导入一个指定的部分到当前命名空间中1from modname import name1[, name2[, ... nameN]] from…import* 语句 把一个模块的所有内容全部导入到当前的命名空间1from modname import * 搜索路径 当导入一个模块时，Python解释器对模块位置的搜索顺序是： 当前目录 如果不在当前目录，则去搜索在shell变量PYTHONPATH下的每个目录 如果都找不到，Python会察看默认路径。UNIX下，默认路径一般为/usr/local/lib/python/ 模块搜索路径存储在 system 模块的 sys.path 变量中。变量里包含当前目录，PYTHONPATH和由安装过程决定的默认目录 命名空间和作用域 如果要在一个函数里给一个全局变量赋值，必须使用global语句 global VarName 的表达式会告诉 Python， VarName 是一个全局变量，这样 Python 就不会在局部命名空间里寻找这个变量了 dir函数 dir()函数是一个排好序的字符串列表，内容是一个模块里定义过的名字 返回的列表容纳了在一个模块里定义的所有模块、变量和函数 globals和locals函数 根据调用的地方的不同，globals() 和 locals() 函数可被用来返回全局和局部命名空间里的名字 如果在函数内部调用 locals()，返回的是所有能在该函数里访问的命名 如果在函数内部调用 globals()，返回的是所有在该函数里能访问的全局名字 两个函数的返回类型都是字典。所以名字们能用 keys() 函数摘取 reload函数 当一个模块被导入到一个脚本，模块顶层部分的 代码只会被执行一次 如果你想重新执行模块里顶层部分的代码，可以用 reload() 函数 该函数会重导之前导入过的模块 module_name要直接放模块的名字，而不是一个字符串形式1reload(module_name) Python中的包 简单来说，包就是文件夹，但该文件夹下必须存在 init.py 文件, 该文件的内容可以为空 int.py用于标识当前文件夹是一个包 Python文件IO读取键盘输入 Python提供了两个内置函数从标准输入读入一行文本，默认的标准输入是键盘 raw_input raw_input([prompt]) 函数从标准输入读取一个行，并返回一个字符串（去掉结尾的换行符） input input([prompt])函数和raw_input([prompt])函数基本类似，但是 input 可以接收一个Python表达式作为输入，并将运算结果返回12345678910#!/usr/bin/python# -*- coding: UTF-8 -*- str = input("请输入：");print "你输入的内容是: ", str"""请输入：[x*5 for x in range(2,10,2)]你输入的内容是: [10, 20, 30, 40]""" 打开和关闭文件open函数 先用Python内置的open()函数打开一个文件，创建一个file对象，相关的方法才可以调用它进行读写 语法file object = open(file_name [, access_mode][, buffering]) 各个参数细节如下： file_name：file_name变量是一个包含了你要访问的文件名称的字符串值 access_mode：access_mode决定了打开文件的模式：只读，写入，追加等。这个参数是非强制的，默认文件访问模式为只读(r) buffering:如果buffering的值被设为0，就不会有寄存。如果buffering的值取1，访问文件时会寄存行。如果将buffering的值设为大于1的整数，表明了这就是寄存区的缓冲大小。如果取负值，寄存区的缓冲大小则为系统默认 File对象的属性 属性 描述 file.closed 返回true如果文件已被关闭，否则返回false file.mode 返回被打开文件的访问模式 file.name 返回文件的名称 file.softspace 如果用print输出后，必须跟一个空格符，则返回false。否则返回true close() 方法 刷新缓冲区里任何还没有写入的信息，并关闭该文件，这之后便不能再进行写入 读写文件write()方法 write()方法可将任何字符串写入一个打开的文件 需要重点注意的是，Python字符串可以是二进制数据，而不是仅仅是文字 write()方法不会在字符串的结尾添加换行符(‘\n’) read()方法 从一个打开的文件中读取一个字符串 需要重点注意的是，Python字符串可以是二进制数据，而不是仅仅是文字 文件定位 tell()方法告诉你文件内的当前位置 换句话说，下一次的读写会发生在文件开头这么多字节之后 seek（offset [,from]）方法改变当前文件的位置 Offset变量表示要移动的字节数 From变量指定开始移动字节的参考位置 如果from被设为0，这意味着将文件的开头作为移动字节的参考位置 如果设为1，则使用当前的位置作为参考位置 如果它被设为2，那么该文件的末尾将作为参考位置 Python异常处理 BaseException 所有异常的基类 捕捉异常可以使用try/except语句 try/except语句用来检测try语句块中的错误，从而让except语句捕获异常信息并处理 如果你不想在异常发生时结束你的程序，只需在try里捕获它 12345678try:&lt;语句&gt; #运行别的代码except &lt;名字&gt;：&lt;语句&gt; #如果在try部份引发了'name'异常except &lt;名字&gt;，&lt;数据&gt;:&lt;语句&gt; #如果引发了'name'异常，获得附加的数据else:&lt;语句&gt; #如果没有异常发生 如果except后不带具体异常名称，那么会捕获发生的所有的异常 try-finally 语句无论是否发生异常都将执行最后的代码 异常的参数 12345try: 正常的操作 ......................except ExceptionType, Argument: 你可以在这输出 Argument 的值... 可以使用raise语句自己触发异常 触发异常后，后面的代码就不会被执行]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[彻底清除Github上某个文件的历史]]></title>
    <url>%2FGit%2F%E5%BD%BB%E5%BA%95%E6%B8%85%E9%99%A4Github%E4%B8%8A%E6%9F%90%E4%B8%AA%E6%96%87%E4%BB%B6%E7%9A%84%E5%8E%86%E5%8F%B2%2F</url>
    <content type="text"><![CDATA[这几天看学弟们写的项目，测试的时候为了方便连接数据库就直接把root密码都写进代码文件了，然后Commit再Sync，成功把密码暴露到Github上。 大家肯定不想自己数据库所在服务器的IP和密码暴露，本文提供了三个方法。 把Github上整个项目删了重新创建并上传不含密码的代码，这个对已经维护过很久的项目是致命的，此乃下策； 直接把数据库密码改了，暴露就暴露呗，别人看见了也连不上，此乃中策； 上策也就是我们今天要说的了，Github for Windows/Mac桌面应用以及网页版都没有提供清除某个文件操作记录的功能，就是说即便你删了这个文件重新Push，那么别人依然可以查看你上一个版本。所以我们需要的是把和这个文件有关的所有Commit等记录全部删掉当然也包括文件本身。 首先在Git Bash或者CMD或者PowerShell中cd进入到你的本地项目文件夹，然后依次执行下面6行命令即可：1234567891011git filter-branch --force --index-filter 'git rm --cached --ignore-unmatch FILE_PATH' --prune-empty --tag-name-filter cat -- --allgit push origin master --forcerm -rf .git/refs/original/git reflog expire --expire=now --allgit gc --prune=nowgit gc --aggressive --prune=now 注意上面的FILE_PATH要是路径而不只是文件名字，例如src/main/java/filename.java 亲测有效 Removing sensitive data from a repository]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[拥塞发生的原因 及 TCP拥塞控制]]></title>
    <url>%2F%E7%BD%91%E7%BB%9C%2F%E6%8B%A5%E5%A1%9E%E5%8F%91%E7%94%9F%E7%9A%84%E5%8E%9F%E5%9B%A0%20%E5%8F%8A%20TCP%E6%8B%A5%E5%A1%9E%E6%8E%A7%E5%88%B6%2F</url>
    <content type="text"><![CDATA[网络数据传输是我们日常开发中不可避免的问题，同时也是出现不确定性问题最多的地方。 本文介绍了网络数据传输的拥塞发生原因，以及在TCP中是如何控制拥塞、控制流量的。 什么是拥塞当大量的分组进入通信子网，超出了网络的处理能力时，就会引起网络局部或整体性能下降，这种现象称为拥塞。拥塞常常使问题趋于恶化。 另一种对拥塞的解释，即对资源的需求超过了可用的资源。若网络中许多资源同时供应不足，网络的性能就要明显变坏，整个网络的吞吐量随之负荷的增大而下降。 拥塞的发生与不可避免拥塞发生的主要原因网络能够提供的资源不足以满足用户的需求，这些资源包括缓存空间、链路带宽容量和中间节点的处理能力。由于互联网的设计机制导致其缺乏“接纳控制”能力，因此在网络资源不足时不能限制用户数量，而只能靠降低服务质量来继续为用户服务，也就是“尽力而为”的服务。 不可避免拥塞其实是一个动态问题，我们没有办法用一个静态方案去解决，从这个意义上来说，拥塞是不可避免的。 拥塞控制和流量控制的差别所谓拥塞控制就是防止过多的数据注入到网络中，这样可以使网络中的路由器或链路不致过载。拥塞控制所要做的都有一个前提，就是网络能承受现有的网络负荷。 流量控制往往指的是点对点通信量的控制，是个端到端的问题。流量控制所要做的就是控制发送端发送数据的速率，以便使接收端来得及接受。 TCP拥塞控制为什么需要TCP拥塞控制 由于TCP采用了超时重传机制，如果拥塞不加以控制，将导致大量的报文重传，并再度引起大量的数据报丢弃，直到整个网络瘫痪。这种现象称为拥塞崩溃。 在网络实际的传输过程中，会出现拥塞的现象，网络上充斥着非常多的数据包，但是却不能按时被传送，形成网络拥塞，其实就是和平时的堵车一个性质了。TCP设计中也考虑到这一点，使用了一些算法来检测网络拥塞现象，如果拥塞产生，便会调整发送策略，减少数据包的发送来缓解网络的压力。 拥塞控制：防止过多的数据注入到网络中，这样可以使网络中的路由器或链路不致过载。拥塞控制所要做的都有一个前提：网络能够承受现有的网络负荷。拥塞控制是一个全局性的过程，涉及到所有的主机、路由器，以及与降低网络传输性能有关的所有因素。 TCP的流量控制早期的TCP协议只有基于窗口的流控制（flow control）机制，我们简单介绍一下，并分析其不足。 在TCP中，为了实现可靠性，发送方发出一个数据段之后要等待接收方相应的确认信息，而不是直接发送下一个分组。 具体的技术是采用滑动窗口，以便通信双方能够充分利用带宽。滑动窗口允许发送方在收到接收方的确认之前发送多个数据段。窗口大小决定了在收到目的地确认之前，一次可以传送的数据段的最大数目。窗口大小越大，主机一次可以传输的数据段就越多。当主机传输窗口大小数目的数据段后，就必须等收到确认，才可以再传下面的数据段。 例如，若视窗的大小为1，则传完数据段后，都必须经过确认，才可以再传下一个数据段；当窗口大小等于3时，发送方可以一次传输3个数据段，等待对方确认后，再传输下面三个数据段。 窗口的大小在通信双方连接期间是可变的，通信双方可以通过协商动态地修改窗口大小。 在TCP的每个确认中，除了指出希望收到的下一个数据段的序列号之外，还包括一个窗口通告，通告中指出了接收方还能再收多少数据段（我们可以把通告看成接收缓冲区大小）。如果通告值增大，窗口大小也相应增大；通告值减小，窗口大小也相应减小。 但是我们可以发现，接收端并没有特别合适的方法来判断当前网络是否拥塞，因为它只是被动得接收，不像发送端，当发出一个数据段后，会等待对方得确认信息，如果超时，就可以认为网络已经拥塞了。 所以，改变窗口大小的唯一根据，就是接收端缓冲区的大小了。 流量控制作为接收方管理发送方发送数据的方式，用来防止接收方可用的数据缓存空间的溢出。 流控制是一种局部控制机制，其参与者仅仅是发送方和接收方，它只考虑了接收端的接收能力，而没有考虑到网络的传输能力； 而拥塞控制则注重于整体，其考虑的是整个网络的传输能力，是一种全局控制机制。正因为流控制的这种局限性，从而导致了拥塞崩溃现象的发生。 TCP拥塞控制的四大过程TCP的拥塞控制由4个核心算法组成：“慢启动”（Slow Start）、“拥塞避免”（Congestion voidance）、“快速重传 ”（Fast Retransmit）、“快速恢复”（Fast Recovery）。 为了方便起见，把发送端叫做client，接收端为server，每个segment长度为512字节，阻塞窗口长度为cwnd（简化起见，下面以segment为单位），sequence number为seq_num，acknowledges number为ack_num。通常情况下，TCP每接收到两个segment，发送一个ack。 慢启动早期开发的TCP应用在启动一个连接时会向网络中发送大量的数据包，这样很容易导致路由器缓存空间耗尽，网络发生拥塞，使得TCP连接的吞吐量急剧下降。 由于TCP源端一开始并不知道网络资源当前的利用状况，因此新建立的TCP连接不能一开始就发送大量数据，而只能逐步增加每次发送的数据量，以避免上述现象的发生，这里有一个“学习”的过程。 假设client要发送5120字节到server，慢启动过程如下： 初始状态，cwnd=1,seq_num=1；client发送第一个segment。 server接收到512字节（一个segment），回应ack_num=513。 client接收ack(513)，cwnd＝1+1=2;现在可以一次发送2个数据段而不必等待ack。 server接收到2个segment，回应ack_num=513+512*2=1537。 client接收ack(1537)，cwnd=2+1;一次发送3个数据段。 server接收到3个segment，回应2个ack，分别为ack_num=1537+1024=2561和ack_num=2561+512=3073。 client接收ack(2561)和ack(3073),cwnd=3+2=5；一次可以发送5个数据段，但是只用4个就满足要求了。 server接收到4个segment，回应2个ack，分别为4097,5121 已经发送5120字节，任务完成！ 总结一下：当建立新的TCP连接时，拥塞窗口（congestion window，cwnd）初始化为一个数据包大小。源端按cwnd大小发送数据，每收到一个ACK确认，cwnd就增加一个数据包发送量。 慢启动算法是一个在连接上发起数据流的方法 慢启动过程： 初始时将拥塞窗口的大小初始为一个MSS 每次收到一个报文段的确认时，发送方将拥塞窗口增大一个MSS，并发送两个最大长度的报文段 两个报文段被确认后，则发送方对每个报文段的确认增加一个MSS，使得拥塞窗口大小变为4个MSS 这样每经过一个RTT，发送速率就翻倍。 因此，慢启动阶段以指数增长 慢启动的结束 当出现一个由超时指示的丢包时间，发送方将cwnd设置为1，并重新开始慢启动 拥塞避免可以想象，如果按上述慢启动的逻辑继续下去而不加任何控制的话，必然会发生拥塞，引入一个慢启动阈值ssthresh的概念，当cwnd &lt; ssthresh的时候，tcp处于慢启动状态，否则，进入拥塞避免阶段。 拥塞避免的主要思想是加法增大，也就是cwnd的值不再指数级往上升，开始加法增加。此时当窗口中所有的报文段都被确认时，cwnd的大小加1，cwnd的值就随着RTT开始线性增加，这样就可以避免增长过快导致网络拥塞，慢慢的增加调整到网络的最佳值。 通常，ssthresh初始化为 64 Kbytes。 当cwnd = 65024 + 512 = 65536，进入拥塞避免阶段，假设此时seq_num = 101024 client一次发送cwnd，但是先考虑头两个segment。 server回应ack_num = 102048 client接收到ack(102048),cwnd = 65536 + [(512 * 512) /65536] = 65536 + 4 = 65540，也就是说，每接到一个ack，cwnd只增加4个字节 client发送一个segment，并开启ack timer,等待server对这个segment的ack，如果超时，则认为网络已经处于拥塞状态，则重设慢启动阀值ssthresh=当前cwnd/2＝65536/2=32768，并且，立刻把cwnd设为1，很极端的处理！ 此时，cwnd &lt; ssthresh，所以，恢复到慢启动状态。 总结一下：如果当前cwnd达到慢启动阀值，则试探性的发送一个segment，如果server超时未响应，TCP认为网络能力下降，必须降低慢启动阀值，同时，为了避免形势恶化，干脆采取极端措施，把发送窗口降为1。 拥塞避免算法是一种处理丢失分组的方法（当到达中间路由器的极限时，分组将被丢弃） 拥塞避免过程： 进入拥塞避免时，拥塞窗口的值大约是上次遇到拥塞时值的一半 每个RTT只将拥塞窗口的值增加1个MSS 拥塞避免结束条件： 超时或丢包 快重传和快恢复标准的重传，client会等待RTO时间再重传，但有时候，不必等这么久也可以判断需要重传， 快重传例如：client一次发送8个segment，seq_num起始值为100000，但是由于网络原因，100512丢失，其他的正常，则server会响应4个ack(100512)(为什么呢，tcp会把接收到的其他segment缓存起来，ack_num必须是连续的)，这时候，client接收到四个重复的ack，它完全有理由判断100512丢失，进而重传，而不必傻等RTO时间了。 快恢复我们通常认为client接收到3个重复的ack后，就会开始快速重传，但是，如果还有更多的重复ack呢，如何处理？这就是快速恢复要做的，事实上，我们可以把快速恢复看作是快速重传的后续处理，它不是一种单独存在的形态。 以下是具体的流程： 假设此时cwnd = 70000，client发送4096字节到server，也就是8个segment，起始seq_num = 100000： client发送seq_num = 100000。 seq_num =100512的segment丢失。 client发送seq_num = 101024。 server接收到两个segment，它意识到100512丢失，先把收到的这两个segment缓存起来。 server回应一个ack(100512),表示它还期待这个segment 。 client发送seq_num = 101536 。 server接收到一个segment，它判断不是100512，依旧把收到的这个segment缓存起来，并回应ack(100512) 。 以下同6、7,直到client收到3个ack(100512)，进入快速重发阶段。 重设慢启动阀值ssthresh=当前cwnd/2＝70000/2=35000 。 client发送seq_num = 100512，以下，进入快速恢复阶段。 重设cwnd = ssthresh + 3 segments =35000 + 3 * 512 = 36536，之所以要加3，是因为我们已经接收到3个ack(100512)了，根据前面说的，每接收到一个ack，cwnd加1。 client接收到第四个、第五个ack(100512)，cwnd=36536+2 * 512=37560。 server接收到100512,响应ack_num = 104096 此时，cwnd&gt;ssthresh，进入拥塞避免阶段。 执行的条件及过程当收到3个或3个以上的重复ACK时，就非常有可能是一个报文段丢失了。于是我们就重传丢失的数据报文段，而不等待超时定时器溢出。这就是快速重传。接下来执行的不是慢启动，而是拥塞避免，这就是快速恢复。 原因没有执行慢启动的原因是由于收到的重复的ACK不仅仅告诉我们一个分组丢失了。由于接收方只有在收到另一个报文段时才会产生重复的ACK，而该报文段已经离开网络并进入了接收方的缓存。也就是说在收发两端之间仍然有流动的数据，而我们不想执行慢启动来突然减少数据流。 牛客网Java刷题知识点之拥塞发生的主要原因、TCP拥塞控制、TCP流量控制、TCP拥塞控制的四大过程（慢启动、拥塞避免、快速重传、快速恢复）]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈Model、ORM、DAO之间的区别]]></title>
    <url>%2Fweb%E5%BC%80%E5%8F%91%2F%E6%B5%85%E8%B0%88Model%E3%80%81ORM%E3%80%81DAO%E4%B9%8B%E9%97%B4%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[在做web开发中，经常会碰到这样几个概念： Model DAO，data access object，数据访问对象 ORM，object-relational mapping，对象关系映射 这些概念都是和数据相关的，然而他们之间有怎样的区别呢？ 首先来看Model，模型。模型是MVC中的概念，指的是数据和改变数据的操作（业务逻辑）。模型通常指代现实生活中的某样实体。以订单为例，每个订单都包含许多数据，如客户、价格、明细等等，这些数据都叫做订单这个模型的属性，此外，和订单相关的一些列操作，比如当购买时，你可能需要先检查库存，给与一定的优惠，再更新账户余额和积分等等，这些就叫做业务逻辑，也是模型的一部分，从代码上来讲，是要放在模型中的。 当模型执行完业务逻辑后，我们便要把模型中的数据保存到数据库中。如果我们直接把和数据库相关的代码放在模型里，会使得以后的维护相当的麻烦。在我之前的一个项目中，我们用户的增长相当快，导致一台数据库无法支撑所有的访问，不得不使用分库来解决问题。然而前人把SQL语句直接写在了模型这一层里，这导致分库相当的麻烦，我们只能先把这些SQL语句抽出来，才能把分库进行下去。我们把这些抽出来的SQL代码放到单独的一层，这一层便是DAL，Data Access Layer，数据访问层，它由许多DAO组成，目的便是把和数据库相关的代码封装起来，这样当我们执行分库时，便只用调整DAO的代码了，模型根本不用关心它使用的数据是放在A库还是B库。 DAO其实是来源于J2EE的一个设计模式，当初的目的也是使得企业更换数据库时，不用影响模型层的代码。 与DAO类似，ORM也是一种封装数据访问的概念。然而ORM不像DAO只是一种软件设计的指导原则，强调的是系统应该层次分明。ORM更像是一种工具，有着成熟的产品，比如JAVA界非常有名的Hibernate，以及很多PHP框架里自带的ORM库。他们的好处在于能将你程序中的数据对象自动地转化为关系型数据库中对应的表和列，数据对象间的引用也可以通过这个工具转化为表之间的join，而Hibernate甚至提供一套他们自己的数据查询语言HQL来解决复杂的查询问题。 使用ORM的好处就是使得你的开发几乎不用接触到SQL语句。创建一张表，声明一个对应的类，然后你就只用和这个类的实例进行交互了，至于这个对象里的数据该怎么存储又该怎么获取，通通不用关心。]]></content>
      <categories>
        <category>web开发</category>
      </categories>
      <tags>
        <tag>web开发</tag>
        <tag>数据模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络面试问题集锦]]></title>
    <url>%2F%E7%BD%91%E7%BB%9C%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98%E9%9B%86%E9%94%A6%2F</url>
    <content type="text"><![CDATA[本文对面试/笔试过程中经常会被问到的一些关于计算机网络的问题进行了梳理和总结，一方面方便自己温故知新，另一方面也希望为找工作的同学们提供一个复习参考。 Http和Https的区别Http协议运行在TCP之上，明文传输，客户端与服务器端都无法验证对方的身份 Https是身披SSL(Secure Socket Layer)外壳的Http，运行于SSL上，SSL运行于TCP之上，是添加了加密和认证机制的HTTP。二者之间存在如下不同： 端口号不同：Http与Http使用不同的连接方式，用的端口也不一样，前者是80，后者是443 资源消耗：和HTTP通信相比，Https通信会由于加减密处理消耗更多的CPU和内存资源 开销：Https通信需要证书，而证书一般需要向认证机构购买 Https的加密机制是一种共享密钥加密和公开密钥加密并用的混合加密机制。 对称加密和非对称加密对称密钥加密是指加密和解密使用同一个密钥的方式，这种方式存在的最大问题就是密钥发送问题，即如何安全地将密钥发给对方； 非对称加密是指使用一对非对称密钥，即公钥和私钥，公钥可以随意发布，但私钥只有自己知道。发送密文的一方使用对方的公钥进行加密处理，对方接收到加密信息后，使用自己的私钥进行解密。 由于非对称加密的方式不需要发送用来解密的私钥，所以可以保证安全性；但是和对称加密比起来，它非常的慢，所以我们还是要用对称加密来传送消息，但对称加密所使用的密钥我们可以通过非对称加密的方式发送出去。 三次握手与四次挥手三次握手三次握手的过程 我要和你建立链接 你真的要和我建立链接么 我真的要和你建立链接 连接建立的前提是服务端需要事先启动并监听到某个端口。 第一次握手：Client将标志位SYN置为1，随机产生一个值seq=X，并将该数据包发送给Server，Client进入SYN_SENT状态，等待Server确认。 第二次握手：Server收到数据包后由标志位SYN=1知道Client请求建立连接，Server将标志位SYN置为1，ACK=X+1，随机产生一个值seq=Y，并将该数据包发送给Client以确认连接请求，Server进入SYN_RCVD状态。 第三次握手：Client收到确认后，检查ACK是否为X+1，如果正确则将标志位ACK置为Y+1，seq=Z并将该数据包发送给Server，Server检查ACK是否为Y+1，如果正确则连接建立成功，Client和Server进入ESTABLISHED状态，完成三次握手，随后Client与Server之间可以开始传输数据了。 为什么需要三次握手我们都知道，TCP是提供的是数据的可靠传输。为了达到这个目的，就需要考虑各种异常情况，然后避免异常情况，对于TCP连接的建立来说，三次握手可以避免已失效的连接请求报文段突然又传到了服务端，因而产生错误。 假设客户端向服务端发送了请求连接的报文，但是报文丢失了，从而也没收到服务端返回的确认报文。于是客户端再次发送一个请求连接报文，而这次的报文顺利到达了服务端，服务端返回了确认报文。但是如果第一次的报文由于网络原因又到达了服务端，而这个报文已经被认为是失效的了，但是服务端是无法确定的，所以服务端还是会返回给客户端确认报文，同意建立连接，如果不采用三次握手，那么刚才的情况就会建立两个连接，而实际上只需要一个连接，这样就会造成资源浪费。 三次握手就解决了上面的这个问题，因为服务端还需要客户端的第二次确认，而对于上面的这种情况，客户端是知道第一个报文是废弃的，所以它就不会发送给服务端确认报文，所以就不会建立第二个连接。 三次握手的过程中可以携带数据吗第一次握手和第二次握手都不能携带数据，但是都需要消耗掉一个序号。 第三次握手的ACK报文可以携带数据，但是如果不携带数据则不消耗序号。 更详细的底层描述可以参考：TCP连接建立的三次握手过程可以携带数据吗？ 四次挥手四次挥手的过程 我要和你断开链接 好的，断吧。我看看我还有没有数据要发送了 我已经全部处理完了，我也要和你断开连接 好的，断吧 第一次挥手：Client发送一个FIN，用来关闭Client到Server的数据传送，Client进入FIN_WAIT_1状态。 第二次挥手：Server收到FIN后，发送一个ACK给Client，确认序号为收到序号+1（与SYN相同，一个FIN占用一个序号），Server进入CLOSE_WAIT状态。Client收到ACK之后，进入FIN_WAIT_2状态。此时TCP链接处于半关闭状态，即客户端已经没有要发送的数据了，但服务端若发送数据，则客户端仍要接收 第三次挥手：Server发送一个FIN，用来关闭Server到Client的数据传送，Server进入LAST_ACK状态。 第四次挥手：Client收到FIN后，Client进入TIME_WAIT状态，接着发送一个ACK给Server，确认序号为收到序号+1，Server进入CLOSED状态，完成四次挥手。 主动关闭的一方等待2MSL的时间 为了保证最后发送的确认报文能够到达被动方，从而可靠地实现TCP全双工连接的终止。因为这个报文有可能会丢失，如果丢失了，被动方就要重传FIN这个报文，并且主动方要能够收到重传的这个报文，而如果重传，就需要等待2MSL的时间（ACK的时间+重传的时间），所以，如果主动方不到2MSL就关闭了，那么就可能收不到被动方重传的报文，就会导致被动方无法正常关闭。 为了防止”已失效的连接请求报文段”出现在本连接中（允许老的重复报文在网络中消失）。主动方发完最后一个ACK报文之后，等待2MSL，就可以使本连接持续时间内所产生的所有报文都在网络中消失，这样就可以在下一次连接中不会出现这种旧的连接请求报文段。 TCP连接期间客户端服务端状态 TCP协议如何来保证传输的可靠性TCP提供一种面向连接的、可靠的字节流服务。其中，面向连接意味着两个使用TCP的应用（通常是一个客户和一个服务器）在彼此交换数据之前必须先建立一个TCP连接。在一个TCP连接中，仅有两方进行彼此通信；而字节流服务意味着两个应用程序通过TCP链接交换8bit字节构成的字节流，TCP不在字节流中插入记录标识符。 对于可靠性，TCP通过以下方式进行保证： 数据包校验：目的是检测数据在传输过程中的任何变化，若校验出包有错，则丢弃报文段并且不给出响应，这时TCP发送数据端超时后会重发数据； 对失序数据包重排序：既然TCP报文段作为IP数据报来传输，而IP数据报的到达可能会失序，因此TCP报文段的到达也可能会失序。TCP将对失序数据进行重新排序，然后才交给应用层； 丢弃重复数据：对于重复数据，能够丢弃重复数据； 应答机制：当TCP收到发自TCP连接另一端的数据，它将发送一个确认。这个确认不是立即发送，通常将推迟几分之一秒； 超时重发：当TCP发出一个段后，它启动一个定时器，等待目的端确认收到这个报文段。如果不能及时收到一个确认，将重发这个报文段； 流量控制：TCP连接的每一方都有固定大小的缓冲空间。TCP的接收端只允许另一端发送接收端缓冲区所能接纳的数据，这可以防止较快主机致使较慢主机的缓冲区溢出，这就是流量控制。TCP使用的流量控制协议是可变大小的滑动窗口协议。 客户端不断进行请求链接会怎样？DDos(Distributed Denial of Service)攻击？服务器端会为每个请求创建一个链接，并向其发送确认报文，然后等待客户端进行确认 DDos攻击 客户端向服务端发送请求链接数据包 服务端向客户端发送确认数据包 客户端不向服务端发送确认数据包，服务器一直等待来自客户端的确认 DDos预防没有彻底根治的办法，除非不使用TCP 限制同时打开SYN半链接的数目 缩短SYN半链接的Time out 时间 关闭不必要的服务 GET和POST的区别 HTTP中get和post的区别 GET请求中URL编码的意义在GET请求中会对URL中非西文字符进行编码，这样做的目的就是为了避免歧义 比如：要传递的参数是name=value，而恰好此时，value是va&amp;lu=e1，那么实际在传输过程中就会变成这样“name1=va&amp;lu=e1”。这样，我们的本意是只有一个键值对，但是服务端却会解析成两个键值对，这样就产生了歧义。 为了解决上述问题带来的歧义，就要对参数进行URL编码。例如，我们对上述会产生歧义的字符进行URL编码后结果：“name1=va%26lu%3D”，这样服务端会把紧跟在“%”后的字节当成普通的字节，就是不会把它当成各个参数或键值对的分隔符。 TCP和UDP的区别TCP (Transmission Control Protocol)和UDP(User Datagram Protocol)协议属于传输层协议，它们之间的区别包括： TCP是面向连接的，UDP是无连接的； TCP是可靠的，UDP是不可靠的； TCP只支持点对点通信，UDP支持一对一、一对多、多对一、多对多的通信模式； TCP是面向字节流的，UDP是面向报文的； TCP有拥塞控制机制，UDP没有拥塞控制，适合媒体通信； TCP首部开销(20个字节)比UDP的首部开销(8个字节)要大； TCP的拥塞处理拥塞控制就是防止过多的数据注入网络中，这样可以使网络中的路由器或链路不致过载。注意，拥塞控制和流量控制不同，前者是一个全局性的过程，而后者指点对点通信量的控制。拥塞控制的方法主要有以下四种： 慢启动 拥塞避免 快重传 快恢复 慢启动不要一开始就发送大量的数据，先探测一下网络的拥塞程度，也就是说由小到大逐渐增加拥塞窗口的大小 拥塞避免拥塞避免算法让拥塞窗口缓慢增长，即每经过一个往返时间RTT就把发送方的拥塞窗口cwnd加1，而不是加倍，这样拥塞窗口按线性规律缓慢增长。 快重传快重传要求接收方在收到一个失序的报文段后就立即发出重复确认 为的是使发送方及早知道有报文段没有到达对方，而不要等到接收方发送数据时捎带确认。 快重传算法规定，发送方只要一连收到三个重复确认就应当立即重传对方尚未收到的报文段，而不必继续等待设置的重传计时器时间到期。 快恢复快重传配合使用的还有快恢复算法 当发送方连续收到三个重复确认时，就执行“乘法减小”算法，把ssthresh门限减半，但是接下去并不执行慢开始算法：因为如果网络出现拥塞的话就不会收到好几个重复的确认，所以发送方现在认为网络可能没有出现拥塞。所以此时不执行慢开始算法，而是将cwnd设置为ssthresh的大小，然后执行拥塞避免算法。 从输入网址到获得页面的过程 浏览器查询DNS，获取域名对应的IP地址 具体过程包括浏览器搜索自身的DNS缓存、搜索操作系统的DNS缓存、读取本地的Host文件和向本地DNS服务器进行查询等。 对于向本地DNS服务器进行查询，如果要查询的域名包含在本地配置区域资源中，则返回解析结果给客户机，完成域名解析(此解析具有权威性)； 如果要查询的域名不由本地DNS服务器区域解析，但该服务器已缓存了此网址映射关系，则调用这个IP地址映射，完成域名解析（此解析不具有权威性）。 如果本地域名服务器并未缓存该网址映射关系，那么将根据其设置发起递归查询或者迭代查询； 浏览器获得域名对应的IP地址以后，浏览器向服务器请求建立链接，发起三次握手 TCP/IP链接建立起来后，浏览器向服务器发送HTTP请求 服务器接收到这个请求，并根据路径参数映射到特定的请求处理器进行处理，并将处理结果及相应的视图返回给浏览器 浏览器解析并渲染视图，若遇到对js文件、css文件及图片等静态资源的引用，则重复上述步骤并向服务器请求这些资源 浏览器根据其请求到的资源、数据渲染页面，最终向用户呈现一个完整的页面 Session、CookieCookie和Session都是客户端与服务器之间保持状态的解决方案，具体来说，cookie机制采用的是在客户端保持状态的方案，而session机制采用的是在服务器端保持状态的方案。 CookieCookie实际上是一小段的文本信息。客户端请求服务器，如果服务器需要记录该用户状态，就使用response向客户端浏览器颁发一个Cookie，而客户端浏览器会把Cookie保存起来。当浏览器再请求该网站时，浏览器把请求的网址连同该Cookie一同提交给服务器，服务器检查该Cookie，以此来辨认用户状态。服务器还可以根据需要修改Cookie的内容。 Session同样地，会话状态也可以保存在服务器端。客户端请求服务器，如果服务器记录该用户状态，就获取Session来保存状态，这时，如果服务器已经为此客户端创建过session，服务器就按照sessionid把这个session检索出来使用；如果客户端请求不包含sessionid，则为此客户端创建一个session并且生成一个与此session相关联的sessionid，并将这个sessionid在本次响应中返回给客户端保存。 保存sessionid的方式可以采用cookie机制，这样在交互过程中浏览器可以自动的按照规则把这个标识发挥给服务器；若浏览器禁用Cookie的话，可以通过URL重写机制将sessionid传回服务器。 Session 与 Cookie 的对比 实现机制：Session的实现常常依赖于Cookie机制，通过Cookie机制回传SessionID； 大小限制：Cookie有大小限制并且浏览器对每个站点也有cookie的个数限制，Session没有大小限制，理论上只与服务器的内存大小有关； 安全性：Cookie存在安全隐患，通过拦截或本地文件找得到cookie后可以进行攻击，而Session由于保存在服务器端，相对更加安全； 服务器资源消耗：Session是保存在服务器端上会存在一段时间才会消失，如果session过多会增加服务器的压力。 XSS 攻击XSS是一种经常出现在web应用中的计算机安全漏洞，与SQL注入一起成为web中最主流的攻击方式。XSS是指恶意攻击者利用网站没有对用户提交数据进行转义处理或者过滤不足的缺点，进而添加一些脚本代码嵌入到web页面中去，使别的用户访问都会执行相应的嵌入代码，从而盗取用户资料、利用用户身份进行某种动作或者对访问者进行病毒侵害的一种攻击方式。 原因分析 主要原因：过于信任客户端提交的数据！ 解决办法：不信任任何客户端提交的数据，只要是客户端提交的数据就应该先进行相应的过滤处理然后方可进行下一步的操作。 进一步分析细节：客户端提交的数据本来就是应用所需要的，但是恶意攻击者利用网站对客户端提交数据的信任，在数据中插入一些符号以及javascript代码，那么这些数据将会成为应用代码中的一部分了，那么攻击者就可以肆无忌惮地展开攻击啦，因此我们绝不可以信任任何客户端提交的数据！ XSS 攻击分类反射性XSS攻击(非持久性XSS攻击)漏洞产生的原因是攻击者注入的数据反映在响应中。一个典型的非持久性XSS攻击包含一个带XSS攻击向量的链接(即每次攻击需要用户的点击)，例如，正常发送消息：1http://www.test.com/message.php?send=Hello,World！ 接收者将会接收信息并显示Hello,World；但是，非正常发送消息：1http://www.test.com/message.php?send=&lt;script&gt;alert(‘foolish!’)&lt;/script&gt;！ 接收者接收消息显示的时候将会弹出警告窗口！ 持久性XSS攻击 (留言板场景)XSS攻击向量(一般指XSS攻击代码)存储在网站数据库，当一个页面被用户打开的时候执行。也就是说，每当用户使用浏览器打开指定页面时，脚本便执行。与非持久性XSS攻击相比，持久性XSS攻击危害性更大。从名字就可以了解到，持久性XSS攻击就是将攻击代码存入数据库中，然后客户端打开时就执行这些攻击代码。例如，留言板表单中的表单域：1&lt;input type=“text” name=“content” value=“这里是用户填写的数据”&gt; 正常操作流程是：用户是提交相应留言信息 —— 将数据存储到数据库 —— 其他用户访问留言板，应用去数据并显示；而非正常操作流程是攻击者在value填写:1&lt;script&gt;alert(‘foolish!’)；&lt;/script&gt; &lt;!--或者html其他标签（破坏样式。。。）、一段攻击型代码--&gt; 并将数据提交、存储到数据库中；当其他用户取出数据显示的时候，将会执行这些攻击性代码。 修复漏洞方针漏洞产生的根本原因是太相信用户提交的数据，对用户所提交的数据过滤不足所导致的，因此解决方案也应该从这个方面入手，具体方案包括： 将重要的cookie标记为http only,这样的话Javascript中的document.cookie语句就不能获取到cookie了（如果在cookie中设置了HttpOnly属性，那么通过js脚本将无法读取到cookie信息，这样能有效的防止XSS攻击） 表单数据规定值的类型，例如：年龄应为只能为int、name只能为字母数字组合 对数据进行Html Encode 处理 过滤或移除特殊的Html标签，例如: &lt;script&gt;, &lt;iframe&gt; , &lt; for &lt;, &gt; for&gt;, &amp;quot for 过滤JavaScript 事件的标签，例如 “onclick=”, “onfocus” 等等。 需要注意的是，在有些应用中是允许html标签出现的，甚至是javascript代码出现。因此，我们在过滤数据的时候需要仔细分析哪些数据是有特殊要求（例如输出需要html代码、javascript代码拼接、或者此表单直接允许使用等等），然后区别处理！ 网络七层模型 OSI七层模型 网络层的ARP协议工作原理网络层的ARP协议完成了IP地址与物理地址的映射。 首先，每台主机都会在自己的ARP缓冲区中建立一个ARP列表，以表示IP地址和MAC地址的对应关系。 当源主机需要将一个数据包要发送到目的主机时，会首先检查自己ARP列表中是否存在该IP地址对应的MAC地址： 如果有，就直接将数据包发送到这个MAC地址； 如果没有，就向本地网段发起一个ARP请求的广播包，查询此目的主机对应的MAC地址。此ARP请求数据包里包括源主机的IP地址、硬件地址、以及目的主机的IP地址。 网络中所有的主机收到这个ARP请求后，会检查数据包中的目的IP是否和自己的IP地址一致。 如果不相同就忽略此数据包； 如果相同，该主机首先将发送端的MAC地址和IP地址添加到自己的ARP列表中，如果ARP表中已经存在该IP的信息，则将其覆盖，然后给源主机发送一个ARP响应数据包，告诉对方自己是它需要查找的MAC地址； 源主机收到这个ARP响应数据包后，将得到的目的主机的IP地址和MAC地址添加到自己的ARP列表中，并利用此信息开始数据的传输。 如果源主机一直没有收到ARP响应数据包，表示ARP查询失败。 IP地址的分类IP地址是指互联网协议地址，是IP协议提供的一种统一的地址格式，它为互联网上的每一个网络和每一台主机分配一个逻辑地址，以此来屏蔽物理地址的差异。IP地址编址方案将IP地址空间划分为A、B、C、D、E五类，其中A、B、C是基本类，D、E类作为多播和保留使用，为特殊地址。 A类地址：以0开头，第一个字节范围：0~127； B类地址：以10开头，第一个字节范围：128~191； C类地址：以110开头，第一个字节范围：192~223； D类地址：以1110开头，第一个字节范围为224~239； E类地址：以1111开头，保留地址 HTTP报文 HTTP报文 HTTP重定向301、302、303、307 ICMP协议ICMP是Internet Control Message Protocol，因特网控制报文协议。它是TCP/IP协议族的一个子协议，用于在IP主机、路由器之间传递控制消息。控制消息是指网络通不通、主机是否可达、路由器是否可用等网络本身的消息。这些控制消息虽然并不传输用户数据，但是对于用户数据的传递起着重要的作用。ICMP报文有两种：差错报告报文和询问报文。 DHCP协议动态主机配置协议，是一种让系统得以连接到网络上，并获取所需要的配置参数手段。通常被应用在大型的局域网络环境中，主要作用是集中的管理、分配IP地址，使网络环境中的主机动态的获得IP地址、Gateway地址、DNS服务器地址等信息，并能够提升地址的使用率。 网桥的作用网桥是一个局域网与另一个局域网之间建立连接的桥梁 网络接口卡（网卡）的功能 进行串行/并行转换。 对数据进行缓存。 在计算机的操作系统安装设备驱动程序。 实现以太网协议。 TTL是什么？作用是什么？哪些工具会用到它TTL是指生存时间，简单来说，它表示了数据包在网络中的时间，经过一个路由器后TTL就减一，这样TTL最终会减为0，当TTL为0时，则将数据包丢弃，这样也就是因为两个路由器之间可能形成环，如果没有TTL的限制，则数据包将会在这个环上一直死转，由于有了TTL，最终TTL为0后，则将数据包丢弃。 ping发送数据包里面有TTL，但是并非是必须的，即是没有TTL也是能正常工作的 traceroute正是因为有了TTL才能正常工作 ifconfig是用来配置网卡信息的，不需要TTL netstat是用来显示路由表的，也是不需要TTL的。 路由表的作用路由表是用来决定如何将一个数据包从一个子网传送到另一个子网的，换句话说就是用来决定从一个网卡接收到的包应该送到哪一个网卡上去。 路由表的每一行至少有目标网络号、子网掩码、到这个子网应该使用的网卡这三条信息。 当路由器从一个网卡接收到一个包时，它扫描路由表的每一行，用里面的子网掩码与数据包中的目标IP地址做逻辑与运算（&amp;）找出目标网络号。如果得出的结果网络号与这一行的网络号相同，就将这条路由表记录下来作为备用路由。如果已经有备用路由了，就在这两条路由里将网络号最长的留下来，另一条丢掉（这是用无分类编址CIDR的情况才是匹配网络号最长的，其他的情况是找到第一条匹配的行时就可以进行转发了）。如此接着扫描下一行直到结束。 如果扫描结束仍没有找到任何路由，就用默认路由。确定路由后，直接将数据包送到对应的网卡上去。在具体的实现中，路由表可能包含更多的信息为选路由算法的细节所用。 RARP 协议逆地址解析协议，作用是完成硬件地址到IP地址的映射，主要用于无盘工作站，因为给无盘工作站配置的IP地址不能保存。 运输层协议与网络层协议的区别网络层协议负责的是提供主机间的逻辑通信 运输层协议负责的是提供进程间的逻辑通信 静态路由和动态路由的区别静态路由是由管理员手工配置的，适合比较简单的网络或需要做路由特殊控制。而动态路由则是由动态路由协议自动维护的，不需人工干预，适合比较复杂大型的网络。 路由器能够自动地建立自己的路由表，并且能够根据实际实际情况的变化适时地进行调整。动态路由机制的运作依赖路由器的两个基本功能：对路由表的维护；路由器之间适时的路由信息交换。 HTTP的长连接和短连接HTTP的长连接和短连接本质上是TCP长连接和短连接。HTTP属于应用层协议。 短连接:浏览器和服务器每进行一次HTTP操作，就建立一次连接，但任务结束就中断连接。 长连接:一个网页打开完成后，客户端和服务器之间用于传输HTTP数据的 TCP连接不会关闭，如果客户端再次访问这个服务器上的网页，会继续使用这一条已经建立的连接。Keep-Alive不会永久保持连接，它有一个保持时间，可以在不同的服务器软件（如Apache）中设定这个时间。实现长连接要客户端和服务端都支持长连接。 TCP短连接:client向server发起连接请求，server接到请求，然后双方建立连接。client向server发送消息，server回应client，然后一次读写就完成了，这时候双方任何一个都可以发起close操作，不过一般都是client先发起 close操作.短连接一般只会在 client/server间传递一次读写操作 TCP长连接:client向server发起连接，server接受client连接，双方建立连接。Client与server完成一次读写之后，它们之间的连接并不会主动关闭，后续的读写操作会继续使用这个连接。 交换机与路由器的区别 工作所处的OSI层次不一样，交换机工作在OSI第二层数据链路层，路由器工作在OSI第三层网络层 寻址方式不同：交换机根据MAC地址寻址，路由器根据IP地址寻址 转发速度不同：交换机的转发速度快，路由器转发速度相对较慢。 计算机网络面试问题集锦]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS 7 下 MySQL 连接数被限制为214个]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E5%BA%93%2FCentOS%207%20%E4%B8%8B%20MySQL%20%E8%BF%9E%E6%8E%A5%E6%95%B0%E8%A2%AB%E9%99%90%E5%88%B6%E4%B8%BA214%E4%B8%AA%2F</url>
    <content type="text"><![CDATA[今天环院的同学反馈说，河流生态地图应用出bug了，我上去一看，MySQL数据库提示Too many connections。出现这个错，是由于连接数过多，需要增加连接数。我在配置系统的时候明明已经在/etc/my.cnf中添加了max_connections = 1000，但是，实际连接数一直被限制在214。 经过一番研究，终于发现了原因，本文特做记录。 本文基于 CentOS 7.3 和 MySQL 5.6 、Mariadb 5.6环境 阿里云1核1G服务器 各种尝试尝试一：更改 MySQL 在 Linux 的最大文件描述符限制原先服务器上装的是MySQL 5.6，Google说如果配置了max_connections = 1000不生效的话，可选的是更改 MySQL 在 Linux 的最大文件描述符限制 编辑/usr/lib/systemd/system/mysqld.service，在文件最后添加 12LimitNOFILE=65535LimitNPROC=65535 保存后，执行下面命令，使配置生效 12systemctl daemon-reloadsystemctl restart mysqld.service 实际连接数到 1000 了 1234567mysql&gt; show variables like &quot;max_connections&quot;;+-----------------+-------+| Variable_name | Value |+-----------------+-------+| max_connections | 1000 |+-----------------+-------+1 row in set 看上去问题是解决了，愉快地去玩耍了。 下午，环院的同学又来反馈问题依旧啊。我打算上服务器看看情况。然而，此时竟然连SSh登录服务器都无法登录了。走阿里云控制台VNC登录上去一看，sshd服务整个都挂了，提示分配内存不足。具体SSH无法登录的问题排查可以参照SSH 无法远程登录问题的处理思路，这里不赘述过程了，我这里纯粹的是因为内存不足导致sshd不能正常服务了。 通过free -m查看了一下机器内存使用情况，发现竟然可用内存只剩8M了。MySQL一个进程就占了500M左右的内存。这不太对啊。重启MySQL服务，竟然都无法成功启动。想着难道是更改 MySQL 在 Linux 的最大文件描述符限制的锅？把添加的两行配置注释掉之后，果然MySQL很快就启起来了。 可是，不改最大文件描述符的话，MySQL的最大连接数就只有214，无法达到文件中配置的1000。 尝试二：更换数据库为Mariadb想起来以前在点盈实习的时候，公司用的Mariadb，当时我也自己调整过最大连接数，一下就成功了。难道是因为MySQL的锅？ 卸载MySQL、安装Mariadb，重新按照上述配置调整配置文件。经过一番尝试，最大连接数仍然只有214。 尝试三：open_files_limit（解决）说实话，经过上面一番尝试我基本已经绝望了，我甚至都想让环院同学直接升级机器配置了。 询问了一下环院的同学，他们只有一个人在使用这个系统，那就应该不是机器配置的问题呀。没办法，我又开始了我的谷歌之路。 面向谷歌编程 还是原先的那篇教程，解决方案中有一句 在配置文件中添加open_files_limit = 65535实际也没有生效 在一开始看的时候，我直接忽略的这句，毕竟人家都没生效嘛，我就不去尝试了。再次看到这里，我想，难道是这个的问题？ 抱着死马当活马医的态度，我在Mariadb的配置文件中也加入了这句配置，重启Mariadb后，奇迹发生了，最大连接数终于变成1000了 问题出现的原因MySQL官方文档里面说了 The maximum number of connections MySQL can support depends on the quality of the thread library on a given platform, the amount of RAM available, how much RAM is used for each connection, the workload from each connection, and the desired response time. Linux or Solaris should be able to support at 500 to 1000 simultaneous connections routinely and as many as 10,000 connections if you have many gigabytes of RAM available and the workload from each is low or the response time target undemanding. Windows is limited to (open tables × 2 + open connections) &lt; 2048 due to the Posix compatibility layer used on that platform. Increasing open-files-limit may be necessary. Also see Section 2.5, “Installing MySQL on Linux”, for how to raise the operating system limit on how many handles can be used by MySQL. 大概意思是 MySQL 能够支持的最大连接数量受限于操作系统,必要时可以增大 open-files-limit。换言之，连接数与文件打开数有关。 解决方案执行ulimit -n查看最大文件描述符数（我这里是65535） 在MySQL配置文件中添加open_files_limit = 65535 到这里应该就OK了，如果还是不行，可以继续尝试下面的步骤，但是，此方案可能造成MySQL因为内存不足无法正常启动，或者即使正常启动了，也可能耗尽系统的内存。 更改 MySQL 在 Linux 的最大文件描述符限制，编辑/usr/lib/systemd/system/mysqld.service文件，在文件最后添加:12LimitNOFILE=65535LimitNPROC=65535 保存后执行下面的命令12systemctl daemon-reloadsystemctl restart mysqld.service 这样最大连接数就是你配置的数量了 一把辛酸泪啊~~ MySQL 5.7 Reference Manual：Too many connections]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>MySQL配置</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二叉树三种遍历顺序互求]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F%E6%A0%91%2F%E4%BA%8C%E5%8F%89%E6%A0%91%2F%E4%BA%8C%E5%8F%89%E6%A0%91%E4%B8%89%E7%A7%8D%E9%81%8D%E5%8E%86%E9%A1%BA%E5%BA%8F%E4%BA%92%E6%B1%82%2F</url>
    <content type="text"><![CDATA[本文总结下二叉树前序、中序、后序遍历相互求法，即如果知道两个的遍历，如何求第三种遍历方法。 比较笨的方法是画出来二叉树，然后根据各种遍历不同的特性来求，也可以编程求出，下面我们分别说明。 首先，我们看看前序、中序、后序遍历的特性： 前序遍历： 访问根节点 前序遍历左子树 前序遍历右子树 中序遍历： 中序遍历左子树 访问根节点 中序遍历右子树 后序遍历： 后序遍历左子树 后序遍历右子树 访问根节点 前序+中序，求后序前序遍历: GDAFEMHZ 中序遍历: ADEFGHMZ 画树求法第一步，根据前序遍历的特点，我们知道根结点为G 第二步，观察中序遍历ADEFGHMZ。其中root节点G左侧的ADEF必然是root的左子树，G右侧的HMZ必然是root的右子树。 第三步，观察左子树ADEF，左子树的中的根节点必然是大树的root的leftchild。在前序遍历中，大树的root的leftchild位于root之后，所以左子树的根节点为D。 第四步，同样的道理，root的右子树节点HMZ中的根节点也可以通过前序遍历求得。在前序遍历中，一定是先把root和root的所有左子树节点遍历完之后才会遍历右子树，并且遍历的左子树的第一个节点就是左子树的根节点。同理，遍历的右子树的第一个节点就是右子树的根节点。 第五步，观察发现，上面的过程是递归的。先找到当前树的根节点，然后划分为左子树，右子树，然后进入左子树重复上面的过程，然后进入右子树重复上面的过程。最后就可以还原一棵树了。该步递归的过程可以简洁表达如下： 确定根,确定左子树，确定右子树。 在左子树中递归。 在右子树中递归。 打印当前根。 那么，我们可以画出这个二叉树的形状 那么，根据后序的遍历规则，我们可以知道，后序遍历顺序为：AEFDHZMG 编程求法依据上面的思路，写递归程序12345678910111213141516171819202122232425262728293031323334353637383940414243#include &lt;iostream&gt;#include &lt;fstream&gt;#include &lt;string&gt;using namespace std;struct TreeNode&#123; struct TreeNode* left; struct TreeNode* right; char elem;&#125;;void BinaryTreeFromOrderings(char* inorder, char* preorder, int length)&#123; if(length == 0)&#123; return; &#125; TreeNode* node = new TreeNode;//Noice that [new] should be written out. node-&gt;elem = *preorder; int rootIndex = 0; for(; rootIndex &lt; length; rootIndex++)&#123; if(inorder[rootIndex] == *preorder) break; &#125; //Left BinaryTreeFromOrderings(inorder, preorder +1, rootIndex); //Right BinaryTreeFromOrderings(inorder + rootIndex + 1, preorder + rootIndex + 1, length - (rootIndex + 1)); //Root cout &lt;&lt; node-&gt;elem &lt;&lt;endl; return;&#125;int main()&#123; char pr[]="GDAFEMHZ"; char in[]="ADEFGHMZ"; BinaryTreeFromOrderings(in, pr, 8); cout &lt;&lt; endl; return 0;&#125; 输出的结果为：AEFDHZMG 中序+后续，求前序依然是上面的题，这次我们只给出中序和后序遍历： 中序遍历: ADEFGHMZ 后序遍历: AEFDHZMG 画树求法第一步，根据后序遍历的特点，我们知道后序遍历最后一个结点即为根结点，即根结点为G。 第二步，观察中序遍历ADEFGHMZ。其中root节点G左侧的ADEF必然是root的左子树，G右侧的HMZ必然是root的右子树。 第三步，观察左子树ADEF，左子树的中的根节点必然是大树的root的leftchild。在前序遍历中，大树的root的leftchild位于root之后，所以左子树的根节点为D。 第四步，同样的道理，root的右子树节点HMZ中的根节点也可以通过前序遍历求得。在前后序遍历中，一定是先把root和root的所有左子树节点遍历完之后才会遍历右子树，并且遍历的左子树的第一个节点就是左子树的根节点。同理，遍历的右子树的第一个节点就是右子树的根节点。 第五步，观察发现，上面的过程是递归的。先找到当前树的根节点，然后划分为左子树，右子树，然后进入左子树重复上面的过程，然后进入右子树重复上面的过程。最后就可以还原一棵树了。该步递归的过程可以简洁表达如下： 确定根,确定左子树，确定右子树。 在左子树中递归。 在右子树中递归。 打印当前根。 这样，我们就可以画出二叉树的形状，如上图所示，这里就不再赘述。 那么，前序遍历: GDAFEMHZ 编程求法123456789101112131415161718192021222324252627282930313233343536373839#include &lt;iostream&gt;#include &lt;fstream&gt;#include &lt;string&gt;using namespace std;struct TreeNode&#123; struct TreeNode* left; struct TreeNode* right; char elem;&#125;;TreeNode* BinaryTreeFromOrderings(char* inorder, char* aftorder, int length)&#123; if(length == 0)&#123; return NULL; &#125; TreeNode* node = new TreeNode;//Noice that [new] should be written out. node-&gt;elem = *(aftorder+length-1); cout &lt;&lt; node-&gt;elem &lt;&lt; endl; int rootIndex = 0; for(; rootIndex &lt; length; rootIndex++) //a variation of the loop&#123; if(inorder[rootIndex] == *(aftorder+length-1)) break; &#125; node-&gt;left = BinaryTreeFromOrderings(inorder, aftorder , rootIndex); node-&gt;right = BinaryTreeFromOrderings(inorder + rootIndex + 1, aftorder + rootIndex , length - (rootIndex + 1)); return node;&#125;int main()&#123; char af[]="AEFDHZMG"; char in[]="ADEFGHMZ"; BinaryTreeFromOrderings(in, af, 8); cout &lt;&lt; endl ; return 0;&#125; 输出结果：GDAFEMHZ]]></content>
      <categories>
        <category>数据结构</category>
        <category>树</category>
        <category>二叉树</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>二叉树</tag>
        <tag>二叉树遍历</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java中的异常机制]]></title>
    <url>%2FJava%2FJava%E5%9F%BA%E7%A1%80%2FJava%E4%B8%AD%E7%9A%84%E5%BC%82%E5%B8%B8%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[在这个世界不可能存在完美的东西，不管完美的思维有多么缜密，细心，我们都不可能考虑所有的因素，这就是所谓的智者千虑必有一失。同样的道理，计算机的世界也是不完美的，异常情况随时都会发生，我们所需要做的就是避免那些能够避免的异常，处理那些不能避免的异常。 这里我将介绍Java中的异常机制。 为什么要使用异常首先我们可以明确一点就是异常的处理机制可以确保我们程序的健壮性，提高系统可用率。 虽然我们不是特别喜欢看到它，但是我们不能不承认它的地位和作用。有异常就说明程序存在问题，有助于我们及时改正。在我们的程序设计当中，任何时候任何地方因为任何原因都有可能会出现异常，在没有异常机制的时候我们是这样处理的：通过函数的返回值来判断是否发生了异常（这个返回值通常是已经约定好了的），调用该函数的程序负责检查并且分析返回值。虽然可以解决异常问题，但是这样做存在几个缺陷： 容易混淆。如果约定返回值为-11111时表示出现异常，那么当程序最后的计算结果真的为-1111呢？ 代码可读性差。将异常处理代码和程序代码混淆在一起将会降低代码的可读性。 由调用函数来分析异常，这要求程序员对库函数有很深的了解。 在OO中提供的异常处理机制是提供代码健壮的强有力的方式。 使用异常机制，能够降低错误处理代码的复杂度，如果不使用异常，那么就必须检查特定的错误，并在程序中的许多地方去处理它。而如果使用异常，那就不必在方法调用处进行检查，因为异常机制将保证能够捕获这个错误，并且，只需在一个地方处理错误，即所谓的异常处理程序中。这种方式不仅节约代码，而且把“概述在正常执行过程中做什么事”的代码和“出了问题怎么办”的代码相分离。总之，与以前的错误处理方法相比，异常机制使代码的阅读、编写和调试工作更加井井有条。 ——摘自《Think in Java》 在初学时，总是听老师说把有可能出错的地方记得加异常处理，刚刚开始还不明白，有时候还觉得只是多此一举，现在随着自己的不断深入，实际生产项目做的多了，渐渐明白了异常是非常重要的。 基本定义 在《Think in java》中是这样定义异常的：异常情形是指阻止当前方法或者作用域继续执行的问题。 在这里一定要明确一点：异常代表某种程度的错误，尽管Java有异常处理机制，但是我们不能以“正常”的眼光来看待异常，异常处理机制的原因就是告诉你：这里可能会或者已经产生了错误，您的程序出现了不正常的情况，可能会导致程序失败！ 异常体系简介异常是指由于各种不期而至的情况，导致程序中断运行的一种指令流,如：文件找不到、非法参数、网络超时等。 为了保证正序正常运行，在设计程序时必须考虑到各种异常情况，并正确的对异常进行处理。 异常也是一种对象，java当中定义了许多异常类，并且定义了基类java.lang.Throwable作为所有异常的超类。Java语言设计者将异常划分为两类：Error和Exception，其体系结构大致如下图所示： 从上面这幅图可以看出，Throwable是java语言中所有错误和异常的超类（万物即可抛）。它有两个子类：Error、Exception。 Error(错误)Error是程序中无法处理的错误，表示运行应用程序中出现了严重的错误。此类错误一般表示代码运行时JVM出现问题。通常有Virtual MachineError（虚拟机运行错误）、NoClassDefFoundError（类定义错误）等。比如说当jvm耗完可用内存时，将出现OutOfMemoryError。此类错误发生时，JVM将终止线程。 这些错误是不可查的，非代码性错误。因此，当此类错误发生时，应用不应该去处理此类错误。 Exception(异常)程序本身可以捕获并且可以处理的异常。 Exception这种异常又分为两类：运行时异常和编译异常。 运行时异常(不受检异常)RuntimeException类及其子类表示JVM在运行期间可能出现的错误。比如说试图使用空值对象的引用（NullPointerException）、数组下标越界（ArrayIndexOutBoundException）。此类异常属于不可查异常，一般是由程序逻辑错误引起的，在程序中可以选择捕获处理，也可以不处理。 编译异常(受检异常)Exception中除RuntimeException及其子类之外的异常。如果程序中出现此类异常，比如说IOException，必须对该异常进行处理，否则编译不通过。在程序中，通常不会自定义该类异常，而是直接使用系统提供的异常类。 可查异常与不可查异常Java的所有异常可以分为可查异常（checked exception）和不可查异常（unchecked exception）。 可查异常编译器要求必须处理的异常。正确的程序在运行过程中，经常容易出现的、符合预期的异常情况。一旦发生此类异常，就必须采用某种方式进行处理。除RuntimeException及其子类外，其他的Exception异常都属于可查异常。编译器会检查此类异常，也就是说当编译器检查到应用中的某处可能会此类异常时，将会提示你处理本异常——要么使用try-catch捕获，要么使用throws语句抛出，否则编译不通过。 不可查异常编译器不会进行检查并且不要求必须处理的异常，也就说当程序中出现此类异常时，即使我们没有try-catch捕获它，也没有使用throws抛出该异常，编译也会正常通过。 该类异常包括运行时异常（RuntimeException及其子类）和错误（Error）。 异常处理流程在java应用中，异常的处理机制分为抛出异常和捕获异常。 抛出异常当一个方法出现错误而引发异常时，该方法会将该异常类型以及异常出现时的程序状态信息封装为异常对象，并交给本应用。运行时，该应用将寻找处理异常的代码并执行。任何代码都可以通过throw关键词抛出异常，比如Java源代码抛出异常、自己编写的代码抛出异常等。 捕获异常一旦方法抛出异常，系统自动根据该异常对象寻找合适异常处理器（Exception Handler）来处理该异常。所谓合适类型的异常处理器指的是异常对象类型和异常处理器类型一致。 异常处理方式对于不同的异常，java采用不同的异常处理方式： 运行异常将由系统自动抛出，应用本身可以选择处理或者忽略该异常。 对于方法中产生的Error，该异常一旦发生JVM将自行处理该异常，因此Java允许应用不抛出此类异常。 对于所有的可查异常，必须进行捕获或者抛出该方法之外交给上层处理。也就是当一个方法存在异常时，要么使用try-catch捕获，要么使用该方法使用throws将该异常抛调用该方法的上层调用者。 捕获异常try-catch语句1234567try &#123; //可能产生的异常的代码区，也成为监控区&#125;catch (ExceptionType1 e) &#123; //捕获并处理try抛出异常类型为ExceptionType1的异常&#125;catch(ExceptionType2 e) &#123; //捕获并处理try抛出异常类型为ExceptionType2的异常&#125; 监控区一旦发生异常，则会根据当前运行时的信息创建异常对象，并将该异常对象抛出监控区，同时系统根据该异常对象依次匹配catch子句，若匹配成功（抛出的异常对象的类型和catch子句的异常类的类型或者是该异常类的子类的类型一致），则运行其中catch代码块中的异常处理代码，一旦处理结束，那就意味着整个try-catch结束。 含有多个catch子句，一旦其中一个catch子句与抛出的异常对象类型一致时，其他catch子句将不再有匹配异常对象的机会。 try-catch-finally123456789try &#123; //可能产生的异常的代码区&#125;catch (ExceptionType1 e) &#123; //捕获并处理try抛出异常类型为ExceptionType1的异常&#125;catch (ExceptionType2 e)&#123; //捕获并处理try抛出异常类型为ExceptionType2的异常&#125;finally&#123; //无论是出现异常，finally块中的代码都将被执行&#125; try-catch-finally代码块的执行顺序：1) try没有捕获异常时，try代码块中的语句依次被执行，跳过catch。如果存在finally则执行finally代码块，否则执行后续代码。2) try捕获到异常时，如果没有与之匹配的catch子句，则该异常交给JVM处理。如果存在finally，则其中的代码仍然被执行，但是finally之后的代码不会被执行。3) try捕获到异常时，如果存在与之匹配的catch，则跳到该catch代码块执行处理。如果存在finally则执行finally代码块，执行完finally代码块之后继续执行后续代码；否则直接执行后续代码。另外注意，try代码块出现异常之后的代码不会被执行。 finally语句 如果没有finally代码块，整个方法在执行完try代码块后返回相应的值来结束整个方法； 如果有finally代码块，此时程序执行到try代码块里的return语句之时并不会立即执行return，而是先去执行finally代码块里的代码， 若finally代码块里没有return或没有能够终止程序的代码，程序将在执行完finally代码块代码之后再返回try代码块执行return语句来结束整个方法； 若finally代码块里有return或含有能够终止程序的代码，方法将在执行完finally之后被结束，不再跳回try代码块执行return。 Talk is cheap, show me the code.单单通过语言描述太抽象了，我们还是通过一个具体的代码实例来真正体验一下吧 情况1：1234567891011121314151617181920public class ExceptionTest &#123; public static void main(String[] args) &#123; System.out.println(tryCatchTest()); &#125; public static String tryCatchTest() &#123; try &#123; // int i = 1 / 0; //引发异常的语句 System.out.println("try 块中语句被执行"); return "try 块中语句 return"; &#125; catch (Exception e) &#123; System.out.println("catch 块中语句被执行"); return "catch 块中语句 return"; &#125; // finally &#123; // System.out.println("finally 块中语句被执行"); // return "finally 块中语句 return"; // &#125; &#125;&#125; 输出：12try 块中语句被执行try 块中语句 return 情况2.1：12345678910111213141516171819public class ExceptionTest &#123; public static void main(String[] args) &#123; System.out.println(tryCatchTest()); &#125; public static String tryCatchTest() &#123; try &#123; // int i = 1 / 0; //引发异常的语句 System.out.println("try 块中语句被执行"); return "try 块中语句 return"; &#125; catch (Exception e) &#123; System.out.println("catch 块中语句被执行"); return "catch 块中语句 return"; &#125; finally &#123; System.out.println("finally 块中语句被执行"); // return "finally 块中语句 return"; &#125; &#125;&#125; 输出：123try 块中语句被执行finally 块中语句被执行try 块中语句 return 情况2.2：12345678910111213141516171819public class ExceptionTest &#123; public static void main(String[] args) &#123; System.out.println(tryCatchTest()); &#125; public static String tryCatchTest() &#123; try &#123; // int i = 1 / 0; //引发异常的语句 System.out.println("try 块中语句被执行"); return "try 块中语句 return"; &#125; catch (Exception e) &#123; System.out.println("catch 块中语句被执行"); return "catch 块中语句 return"; &#125; finally &#123; System.out.println("finally 块中语句被执行"); return "finally 块中语句 return"; &#125; &#125;&#125; 输出：123try 块中语句被执行finally 块中语句被执行finally 块中语句 return 如果有catch块，基本和上面情况相似，只不过是把catch块续到try块引发异常语句后面，finally的地位依旧。 总结try代码块：用于捕获异常。其后可以接零个或者多个catch块。如果没有catch块，后必须跟finally块，来完成资源释放等操作，另外建议不要在finally中使用return，不用尝试通过catch来控制代码流程。 catch代码块：用于捕获异常，并在其中处理异常。 finally代码块：无论是否捕获异常，finally代码总会被执行。如果try代码块或者catch代码块中有return语句时，finally代码块将在方法返回前被执行。注意以下几种情况，finally代码块不会被执行： 在前边的代码中使用System.exit()退出应用 程序所在的线程死亡或者cpu关闭 如果在finally代码块中的操作又产生异常，则该finally代码块不能完全执行结束，同时该异常会覆盖前边抛出的异常 抛出异常throws抛出异常如果一个方法可能抛出异常，但是没有能力处理该异常或者需要通过该异常向上层汇报处理结果，可以在方法声明时使用throws来抛出异常。这就相当于计算机硬件发生损坏，但是计算机本身无法处理，就将该异常交给维修人员来处理。123public methodName throws Exception1,Exception2….(params)&#123; &#125; 其中Exception1,Exception2…为异常列表。一旦该方法中某行代码抛出异常，则该异常将由调用该方法的上层方法处理。如果上层方法无法处理，可以继续将该异常向上层抛。 throw抛出异常在方法内，用throw来抛出一个Throwable类型的异常。一旦遇到到throw语句，后面的代码将不被执行。然后，便是进行异常处理——包含该异常的try-catch最终处理，也可以向上层抛出。注意我们只能抛出Throwable类和其子类的对象。1throw newExceptionType; 比如我们可以抛出：throw new Exception(); 也有时候我们也需要在catch中抛出异常,这也是允许的，比如说：12345Try&#123; //可能会发生异常的代码&#125;catch(Exceptione)&#123; throw newException(e);&#125; 异常关系链在实际开发过程中经常在捕获一个异常之后抛出另外一个异常，并且我们希望在新的异常对象中保存原始异常对象的信息，实际上就是异常传递，即把底层的异常对象传给上层，一级一级，逐层抛出。当程序捕获了一个底层的异常，而在catch处理异常的时候选择将该异常抛给上层…这样异常的原因就会逐层传递，形成一个由低到高的异常链。 但是异常链在实际应用中一般不建议使用，同时异常链每次都需要就将原始的异常对象封装为新的异常对象，消耗大量资源。现在（JDK 1.4之后）所有的Throwable的子类构造中都可以接受一个cause对象，这个cause也就是原始的异常对象。 异常转义异常转义就是将一种类型的异常转成另一种类型的异常，然后再抛出异常。之所以要进行转译，是为了更准确的描述异常。 就我个人而言，我更喜欢称之为异常类型转换。在实际应用中，为了构建自己的日志系统，经常需要把系统的一些异常信息描述成我们想要的异常信息，就可以使用异常转译。异常转译针对所有Throwable类的子类而言，其子类型都可以相互转换。 通常而言，更为合理的转换方式是： Error——&gt;Exception Error——&gt;RuntimeException Exception——&gt;RuntimeException, Throwable类中常用的方法像catch(Exception e)中的Exception就是异常的变量类型，e则是形参。通常在进行异常输出时有如下几个方法可用： e.getCause():返回抛出异常的原因。 e.getMessage():返回异常信息。 e.printStackTrace():发生异常时，跟踪堆栈信息并输出。]]></content>
      <categories>
        <category>Java</category>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>异常</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DNS原理及其查询过程]]></title>
    <url>%2F%E7%BD%91%E7%BB%9C%2FDNS%E5%8E%9F%E7%90%86%E5%8F%8A%E5%85%B6%E6%9F%A5%E8%AF%A2%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[网络通讯大部分是基于TCP/IP的，而TCP/IP是基于IP地址的，所以计算机在网络上进行通讯时只能识别如202.96.134.133之类的IP地址，而不能认识域名。我们无法记住10个以上网站的IP地址，所以我们访问网站时，更多的是在浏览器地址栏中输入域名，就能看到所需要的页面，这是因为有一个叫DNS服务器的计算机自动把我们的域名“翻译”成了相应的IP地址，然后调出IP地址所对应的网页。 DNS(Domain Name System)是域名系统的英文缩写，是一种组织成域层次结构的计算机和网络服务命名系统，它用于TCP/IP网络，它所提供的服务是用来将主机名和域名转换为IP地址的工作。 DNS使用的协议DNS同时占用UDP和TCP端口53是公认的，这种单个应用协议同时使用两种传输协议的情况在TCP/IP栈也算是个另类。但很少有人知道DNS分别在什么情况下使用这两种协议。 DNS在进行区域传输的时候使用TCP协议，其它时候则使用UDP协议； DNS的规范规定了2种类型的DNS服务器，一个叫主DNS服务器，一个叫辅助DNS服务器。在一个区中主DNS服务器从自己本机的数据文件中读取该区的DNS数据信息，而辅助DNS服务器则从区的主DNS服务器中读取该区的DNS数据信息。当一个辅助DNS服务器启动时，它需要与主DNS服务器通信，并加载数据信息，这就叫做区传送（zone transfer）。 为什么既使用TCP又使用UDP？ 首先了解一下TCP与UDP传送字节的长度限制： UDP报文的最大长度为512字节，而TCP则允许报文长度超过512字节。当DNS查询超过512字节时，协议的TC标志出现删除标志，这时则使用TCP发送。通常传统的UDP报文一般不会大于512字节。 区域传送时使用TCP，主要有一下两点考虑： 辅域名服务器会定时（一般时3小时）向主域名服务器进行查询以便了解数据是否有变动。如有变动，则会执行一次区域传送，进行数据同步。区域传送将使用TCP而不是UDP，因为数据同步传送的数据量比一个请求和应答的数据量要多得多。 TCP是一种可靠的连接，保证了数据的准确性。 域名解析时使用UDP协议 客户端向DNS服务器查询域名，一般返回的内容都不超过512字节，用UDP传输即可。不用经过TCP三次握手，这样DNS服务器负载更低，响应更快。虽然从理论上说，客户端也可以指定向DNS服务器查询的时候使用TCP，但事实上，很多DNS服务器进行配置的时候，仅支持UDP查询包。 DNS的查询过程 在浏览器中输入www.qq.com域名，操作系统会先检查自己本地的hosts文件是否有这个网址映射关系，如果有，就先调用这个IP地址映射，完成域名解析。 如果hosts里没有这个域名的映射，则查找本地DNS解析器缓存，是否有这个网址映射关系，如果有，直接返回，完成域名解析。 如果hosts与本地DNS解析器缓存都没有相应的网址映射关系，首先会找TCP/ip参数中设置的首选DNS服务器，在此我们叫它本地DNS服务器，此服务器收到查询时，如果要查询的域名，包含在本地配置区域资源中，则返回解析结果给客户机，完成域名解析，此解析具有权威性。 如果要查询的域名，不由本地DNS服务器区域解析，但该服务器已缓存了此网址映射关系，则调用这个IP地址映射，完成域名解析，此解析不具有权威性。 如果本地DNS服务器本地区域文件与缓存解析都失效，则根据本地DNS服务器的设置（是否设置转发器）进行查询，如果未用转发模式，本地DNS就把请求发至13台根DNS，根DNS服务器收到请求后会判断这个域名(.com)是谁来授权管理，并会返回一个负责该顶级域名服务器的一个IP。本地DNS服务器收到IP信息后，将会联系负责.com域的这台服务器。这台负责.com域的服务器收到请求后，如果自己无法解析，它就会找一个管理.com域的下一级DNS服务器地址(qq.com)给本地DNS服务器。当本地DNS服务器收到这个地址后，就会找qq.com域服务器，重复上面的动作，进行查询，直至找到www.qq.com主机。 如果用的是转发模式，此DNS服务器就会把请求转发至上一级DNS服务器，由上一级服务器进行解析，上一级服务器如果不能解析，或找根DNS或把转请求转至上上级，以此循环。不管是本地DNS服务器用是是转发，还是根提示，最后都是把结果返回给本地DNS服务器，由此DNS服务器再返回给客户机。]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring中Bean的生命周期]]></title>
    <url>%2FSpring%2FSpring%E4%B8%ADBean%E7%9A%84%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%2F</url>
    <content type="text"><![CDATA[去一些企业面试时，经常会被问到Spring的问题，有一次就被问到关于Spring中Bean的生命周期是怎样的？其实这也是在业务中经常会遇到的，但容易遗忘，所以专门总结一下以备不时之需。 PS：可以借鉴Servlet的生命周期，实例化、初始init、接收请求service、销毁destroy。 Spring上下文中的Bean也类似，【Spring上下文的生命周期】 概括版描述 实例化一个Bean，也就是我们通常说的new 按照Spring上下文对实例化的Bean进行配置，也就是IOC注入 如果这个Bean实现了BeanNameAware接口，会调用它实现的setBeanName(String beanId)方法，此处传递的是Spring配置文件中Bean的ID 如果这个Bean实现了BeanFactoryAware接口，会调用它实现的setBeanFactory()，传递的是Spring工厂本身（可以用这个方法获取到其他Bean） 如果这个Bean实现了ApplicationContextAware接口，会调用setApplicationContext(ApplicationContext)方法，传入Spring上下文，该方式同样可以实现步骤4，但比4更好，因为ApplicationContext是BeanFactory的子接口，有更多的实现方法 如果这个Bean关联了BeanPostProcessor接口，将会调用postProcessBeforeInitialization(Object obj, String s)方法，BeanPostProcessor经常被用作是Bean内容的更改，并且由于这个是在Bean初始化结束时调用After方法，也可用于内存或缓存技术 如果这个Bean在Spring配置文件中配置了init-method属性会自动调用其配置的初始化方法 如果这个Bean关联了BeanPostProcessor接口，将会调用postAfterInitialization(Object obj, String s)方法注意：以上工作完成以后就可以用这个Bean了，那这个Bean是一个single的，所以一般情况下我们调用同一个ID的Bean会是在内容地址相同的实例 当Bean不再需要时，会经过清理阶段，如果Bean实现了DisposableBean接口，会调用其实现的destroy方法 最后，如果这个Bean的Spring配置中配置了destroy-method属性，会自动调用其配置的销毁方法 以上10步骤可以作为面试或者笔试的模板，另外我们这里描述的是应用Spring上下文Bean的生命周期，如果应用Spring的工厂也就是BeanFactory的话去掉第5步就Ok了 详细描述ApplicationContext Bean生命周期Spring Bean的完整生命周期从创建Spring容器开始，直到最终Spring容器销毁Bean，这其中包含了一系列关键点。 流程图 若容器注册了以上各种接口，程序那么将会按照以上的流程进行。下面将仔细讲解各接口作用。 各种接口方法分类Bean的完整生命周期经历了各种方法调用，这些方法可以划分为以下几类： Bean自身的方法：这个包括了Bean本身调用的方法和通过配置文件中的init-method和destroy-method指定的方法 Bean级生命周期接口方法：这个包括了BeanNameAware、BeanFactoryAware、InitializingBean和DiposableBean这些接口的方法 容器级生命周期接口方法：这个包括了InstantiationAwareBeanPostProcessor和 BeanPostProcessor这两个接口实现，一般称它们的实现类为“后处理器”。容器中每个bean初始化都要经过这一步。 工厂后处理器接口BeanFactoryPostProcessor方法：这个包括了AspectJWeavingEnabler,ConfigurationClassPostProcessor, CustomAutowireConfigurer等等非常有用的工厂后处理器接口的方法。工厂后处理器也是容器级的，在应用上下文装配配置文件之后立即调用。 流程说明 首先容器启动后，会对scope为singleton且非懒加载(lazy-init=false)的bean进行实例化， 按照Bean定义信息配置信息，注入所有的属性 如果Bean实现了BeanNameAware接口，会回调该接口的setBeanName()方法，传入该Bean的id，此时该Bean就获得了自己在配置文件中的id 如果Bean实现了BeanFactoryAware接口,会回调该接口的setBeanFactory()方法，传入该Bean的BeanFactory，这样该Bean就获得了自己所在的BeanFactory 如果Bean实现了ApplicationContextAware接口,会回调该接口的setApplicationContext()方法，传入该Bean的ApplicationContext，这样该Bean就获得了自己所在的ApplicationContext 如果有Bean实现了BeanPostProcessor接口，则会回调该接口的postProcessBeforeInitialzation()方法 如果Bean实现了InitializingBean接口，则会回调该接口的afterPropertiesSet()方法 如果Bean配置了init-method方法，则会执行init-method配置的方法 如果有Bean实现了BeanPostProcessor接口，则会回调该接口的postProcessAfterInitialization()方法 经过流程9之后，就可以正式使用该Bean了,对于scope为singleton的Bean,Spring的ioc容器中会缓存一份该bean的实例，而对于scope为prototype的Bean,每次被调用都会new一个新的对象，生命周期就交给调用方管理了，不再是Spring容器进行管理了 容器关闭后，如果Bean实现了DisposableBean接口，则会回调该接口的destroy()方法 如果Bean配置了destroy-method方法，则会执行destroy-method配置的方法，至此，整个Bean的生命周期结束 BeanFactory Bean生命周期BeanFactoty容器中, Bean的生命周期如下图所示，与ApplicationContext相比，有如下几点不同: BeanFactory容器中，不会调用ApplicationContextAware接口的setApplicationContext()方法 BeanPostProcessor接口的postProcessBeforeInitialzation()方法和postProcessAfterInitialization()方法不会自动调用，必须自己通过代码手动注册 BeanFactory容器启动的时候，不会去实例化所有Bean,包括所有scope为singleton且非懒加载的Bean也是一样，而是在调用的时候去实例化。 流程图 流程说明 当调用者通过 getBean(name)向 容器寻找Bean时，如果容器注册了org.springframework.beans.factory.config.InstantiationAwareBeanPostProcessor接口，在实例bean之前，将调用该接口的 postProcessBeforeInstantiation()方法 容器寻找Bean的定义信息，并将其实例化 使用依赖注入，Spring按照Bean定义信息配置Bean的所有属性 如果Bean实现了BeanNameAware接口，工厂调用Bean的setBeanName()方法传递Bean的id 如果实现了BeanFactoryAware接口，工厂调用setBeanFactory()方法传入工厂自身 如果BeanPostProcessor和Bean关联，那么它们的postProcessBeforeInitialization()方法将被调用（需要手动进行注册！） 如果Bean实现了InitializingBean接口，则会回调该接口的afterPropertiesSet()方法 如果Bean指定了init-method方法，就会调用init-method方法 如果BeanPostProcessor和Bean关联，那么它的postProcessAfterInitialization()方法将被调用（需要手动注册！） 现在Bean已经可以使用了 scope为singleton的Bean缓存在Spring IOC容器中 scope为prototype的Bean生命周期交给客户端 销毁 如果Bean实现了DisposableBean接口，destory()方法将会被调用 如果配置了destory-method方法，就调用这个方法]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring默认单例机制的探讨]]></title>
    <url>%2FSpring%2FSpring%E9%BB%98%E8%AE%A4%E5%8D%95%E4%BE%8B%E6%9C%BA%E5%88%B6%E7%9A%84%E6%8E%A2%E8%AE%A8%2F</url>
    <content type="text"><![CDATA[使用过Spring的程序员都知道，我们的bean（controller、service和Dao，实体bean除外）都是通过Spring的IoC容器统一管理的，同时这些bean默认都是单例的，即一个bean在一个IoC容器中只有一个实例。这一点跟设计模式中的单例略有不同，设计模式中的单例是整个应用中只有一个实例。 最近看一个同学去面试，其中一个问题是关于Spring单例的，本文就整理一下我对Spring单例的理解。 我们把bean放在IOC容器中统一进行管理，只在初始化加载的时候实例化一次，一方面提高了效率,另一方面大大降低了内存开销。spring的依赖反转原则降低了程序之间的耦合性，也提高了我们的开发效率，不用每次都手动去new了。 单例模式确实有很多优点，但是说到单例我们就会想到线程安全，并发访问情况下spring中bean是线程安全的吗？ 到底是不是线程安全的，要根据实际场景判断。为什么这么说呢？首先，大多数时候客户端都在访问我们应用中的业务对象，而这些业务对象并没有做线程的并发限制，因此不会出现各个线程之间的等待问题，或是死锁问题。这一部分不在考虑。 再有就是成员变量这一重要因素了。在并发访问的时候这些成员变量将会是并发线程中的共享对象，也是影响线程安全的重要因素。成员变量又分为基本类型的成员变量和引用类型的成员变量。 其中引用类型的成员变量即我们在controller中注入的service，在service中注入的dao，这里将其定义为成员变量主要是为了实例化进而调用里面的业务方法，在这些类中一般不会有全局变量，因此只要我们的业务方法不含有独立的全局变量即使是被多线程共享，也是线程安全的。 再有就是基本类型成员变量，刚刚说了service层和dao层一般不会有全局变量，这里主要针对于controller层。基本类型成员变量的定义又分为两种情况：如果此成员变量是final类型修饰的不可被修改的，则仍是线程安全的。另外一种情况就是不安全的了，解决方法：要么把全局变量定义成局部的，要么修改controller的单例模式把它定义成prototype类型的。 从文中开始我们就提到过，实体bean不是单例的，并没有交给spring来管理，每次我们都手动去new一个实例。从客户端传递到后台的controller–&gt;service–&gt;Dao,这一个流程中，即使处理我们提交数据的业务处理类是被多线程共享的，但是他们处理的数据并不是共享的，数据是每一个线程都有自己的一份，所以在数据这个方面是不会出现线程同步方面的问题的。]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[详解CountDownLatch]]></title>
    <url>%2FJava%2FJava%E5%9F%BA%E7%A1%80%2F%E8%AF%A6%E8%A7%A3CountDownLatch%2F</url>
    <content type="text"><![CDATA[正如每个Java文档所描述的那样，CountDownLatch是一个同步工具类，它允许一个或多个线程一直等待，直到其他线程的操作执行完后再执行。在Java并发中，countdownlatch的概念是一个常见的面试题，所以一定要确保你很好的理解了它。在这篇文章中，我将会涉及到在Java并发编程中跟CountDownLatch相关的以下几点： CountDownLatch是什么？ CountDownLatch如何工作？ 在实时系统中的应用场景 应用范例 常见的面试题 CountDownLatch是什么CountDownLatch是在java1.5被引入的，跟它一起被引入的并发工具类还有CyclicBarrier、Semaphore、ConcurrentHashMap和BlockingQueue，它们都存在于java.util.concurrent包下。 CountDownLatch这个类能够使一个线程等待其他线程完成各自的工作后再执行。例如，应用程序的主线程希望在负责启动框架服务的线程已经启动所有的框架服务之后再执行。 CountDownLatch是通过一个计数器来实现的，计数器的初始值为线程的数量。每当一个线程完成了自己的任务后，计数器的值就会减1。当计数器值到达0时，它表示所有的线程已经完成了任务，然后在闭锁上等待的线程就可以恢复执行任务。 CountDownLatch如何工作CountDownLatch.java类中定义的构造函数：12//Constructs a CountDownLatch initialized with the given count.public void CountDownLatch(int count) &#123;...&#125; 构造器中的计数值（count）实际上就是闭锁需要等待的线程数量。这个值只能被设置一次，而且CountDownLatch没有提供任何机制去重新设置这个计数值。 与CountDownLatch的第一次交互是主线程等待其他线程。主线程必须在启动其他线程后立即调用CountDownLatch.await()方法。这样主线程的操作就会在这个方法上阻塞，直到其他线程完成各自的任务。 其他N个线程必须引用闭锁对象，因为他们需要通知CountDownLatch对象，他们已经完成了各自的任务。这种通知机制是通过CountDownLatch.countDown()方法来完成的；每调用一次这个方法，在构造函数中初始化的count值就减1。所以当N个线程都调用了这个方法，count的值等于0，然后主线程就能通过await()方法，恢复执行自己的任务。 在实时系统中的使用场景让我们尝试罗列出在Java实时系统中CountDownLatch都有哪些使用场景。我所罗列的都是我所能想到的。如果你有别的可能的使用方法，请在留言里列出来，这样会帮助到大家。 实现最大的并行性：有时我们想同时启动多个线程，实现最大程度的并行性。例如，我们想测试一个单例类。如果我们创建一个初始计数为1的CountDownLatch，并让所有线程都在这个锁上等待，那么我们可以很轻松地完成测试。我们只需调用一次countDown()方法就可以让所有的等待线程同时恢复执行。 开始执行前等待n个线程完成各自任务：例如应用程序启动类要确保在处理用户请求前，所有N个外部系统已经启动和运行了。 死锁检测：一个非常方便的使用场景是，你可以使用n个线程访问共享资源，在每次测试阶段的线程数目是不同的，并尝试产生死锁。 CountDownLatch使用例子在这个例子中，我模拟了一个应用程序启动类，它开始时启动了n个线程类，这些线程将检查外部系统并通知闭锁，并且启动类一直在闭锁上等待着。一旦验证和检查了所有外部服务，那么启动类恢复执行。 BaseHealthChecker.java：这个类是一个Runnable，负责所有特定的外部服务健康的检测。它删除了重复的代码和闭锁的中心控制代码。123456789101112131415161718192021222324252627282930313233343536373839public abstract class BaseHealthChecker implements Runnable &#123; private CountDownLatch latch; private String serviceName; private boolean serviceUp; //Get latch object in constructor so that after completing the task, thread can countDown() the latch public BaseHealthChecker(String serviceName, CountDownLatch latch) &#123; super(); this.latch = latch; this.serviceName = serviceName; this.serviceUp = false; &#125; @Override public void run() &#123; try &#123; verifyService(); serviceUp = true; &#125; catch (Throwable t) &#123; t.printStackTrace(System.err); serviceUp = false; &#125; finally &#123; if (latch != null) &#123; latch.countDown(); &#125; &#125; &#125; public String getServiceName() &#123; return serviceName; &#125; public boolean isServiceUp() &#123; return serviceUp; &#125; //This method needs to be implemented by all specific service checker public abstract void verifyService();&#125; NetworkHealthChecker.java：这个类继承了BaseHealthChecker，实现了verifyService()方法。DatabaseHealthChecker.java和CacheHealthChecker.java除了服务名和休眠时间外，与NetworkHealthChecker.java是一样的。12345678910111213141516public class NetworkHealthChecker extends BaseHealthChecker &#123; public NetworkHealthChecker(CountDownLatch latch) &#123; super("Network Service", latch); &#125; @Override public void verifyService() &#123; System.out.println("Checking " + this.getServiceName()); try &#123; Thread.sleep(7000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(this.getServiceName() + " is UP"); &#125;&#125; ApplicationStartupUtil.java：这个类是一个主启动类，它负责初始化闭锁，然后等待，直到所有服务都被检测完。123456789101112131415161718192021222324252627282930313233343536373839404142434445public class ApplicationStartupUtil &#123; //List of service checkers private static List&lt;BaseHealthChecker&gt; services; //This latch will be used to wait on private static CountDownLatch latch; private ApplicationStartupUtil() &#123; &#125; private final static ApplicationStartupUtil INSTANCE = new ApplicationStartupUtil(); public static ApplicationStartupUtil getInstance() &#123; return INSTANCE; &#125; public static boolean checkExternalServices() throws Exception &#123; //Initialize the latch with number of service checkers latch = new CountDownLatch(3); //All add checker in lists services = new ArrayList&lt;BaseHealthChecker&gt;(); services.add(new NetworkHealthChecker(latch)); services.add(new CacheHealthChecker(latch)); services.add(new DatabaseHealthChecker(latch)); //Start service checkers using executor framework Executor executor = Executors.newFixedThreadPool(services.size()); for (final BaseHealthChecker v : services) &#123; executor.execute(v); &#125; //Now wait till all services are checked latch.await(); //Services are file and now proceed startup for (final BaseHealthChecker v : services) &#123; if (!v.isServiceUp()) &#123; return false; &#125; &#125; return true; &#125;&#125; 现在你可以写测试代码去检测一下闭锁的功能了。1234567891011public class Main &#123; public static void main(String args[]) &#123; boolean result = false; try &#123; result = ApplicationStartupUtil.checkExternalServices(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; System.out.println("External services validation completed !! Result was :: " + result); &#125;&#125; 123456789Output in console: Checking Network ServiceChecking Cache ServiceChecking Database ServiceDatabase Service is UPCache Service is UPNetwork Service is UPExternal services validation completed !! Result was :: true 常见面试题解释一下CountDownLatch概念?CountDownLatch这个类能够使一个线程等待其他线程完成各自的工作后再执行。 CountDownLatch是通过一个计数器来实现的，计数器的初始值为线程的数量。每当一个线程完成了自己的任务后，计数器的值就会减1。当计数器值到达0时，它表示所有的线程已经完成了任务，然后在闭锁上等待的线程就可以恢复执行任务。 CountDownLatch和CyclicBarrier的不同之处?CountDownLatch:一个线程(或者多个)， 等待另外N个线程完成某个事情之后才能执行。 CyclicBarrier:N个线程相互等待，任何一个线程完成之前，所有的线程都必须等待。 这样应该就清楚一点了，对于CountDownLatch来说，重点是那个“一个线程”, 是它在等待，而另外那N的线程在把“某个事情”做完之后可以继续等待，可以终止。而对于CyclicBarrier来说，重点是那N个线程，他们之间任何一个没有完成，所有的线程都必须等待。 CountDownLatch 是计数器，线程完成一个就记一个，就像报数一样，只不过是递减的。 而CyclicBarrier更像一个水闸，线程执行就想水流，在水闸处都会堵住，等到水满(线程到齐)了，才开始泄流。 给出一些CountDownLatch使用的例子? 实现最大的并行性 开始执行前等待n个线程完成各自任务 死锁检测 CountDownLatch类中主要的方法? await方法：让当前线程等到其他所有线程都执行完毕后再开始执行 countDown方法：每有一个线程执行完毕后，调用这个方法使计数器减一 源码解析 &gt; 【JUC】JDK1.8源码分析之CountDownLatch（五）]]></content>
      <categories>
        <category>Java</category>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Java基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring容器中Bean的作用域]]></title>
    <url>%2FSpring%2FSpring%E5%AE%B9%E5%99%A8%E4%B8%ADBean%E7%9A%84%E4%BD%9C%E7%94%A8%E5%9F%9F%2F</url>
    <content type="text"><![CDATA[当通过Spring容器创建一个Bean实例时，不仅可以完成Bean实例的实例化，还可以为Bean指定特定的作用域。 本文介绍了Spring中Bean的作用域的用法，作用域包括singleton、prototype、request、session和globalsession等5种。 Spring支持如下5种作用域： singleton：单例模式，在整个Spring IoC容器中，使用singleton定义的Bean将只有一个实例 prototype：原型模式，每次通过容器的getBean方法获取prototype定义的Bean时，都将产生一个新的Bean实例 request：对于每次HTTP请求，使用request定义的Bean都将产生一个新实例，即每次HTTP请求将会产生不同的Bean实例。只有在Web应用中使用Spring时，该作用域才有效 session：对于每次HTTP Session，使用session定义的Bean豆浆产生一个新实例。同样只有在Web应用中使用Spring时，该作用域才有效 globalsession：每个全局的HTTP Session，使用session定义的Bean都将产生一个新实例。典型情况下，仅在使用portlet context的时候有效。同样只有在Web应用中使用Spring时，该作用域才有效 其中比较常用的是singleton和prototype两种作用域。 对于singleton作用域的Bean，每次请求该Bean都将获得相同的实例。容器负责跟踪Bean实例的状态，负责维护Bean实例的生命周期行为；如果一个Bean被设置成prototype作用域，程序每次请求该id的Bean，Spring都会新建一个Bean实例，然后返回给程序。在这种情况下，Spring容器仅仅使用new关键字创建Bean实例，一旦创建成功，容器不在跟踪实例，也不会维护Bean实例的状态。 如果不指定Bean的作用域，Spring默认使用singleton作用域。 Java在创建Java实例时，需要进行内存申请；销毁实例时，需要完成垃圾回收，这些工作都会导致系统开销的增加。因此，prototype作用域Bean的创建、销毁代价比较大。而singleton作用域的Bean实例一旦创建成功，可以重复使用。因此，除非必要，否则尽量避免将Bean被设置成prototype作用域。 设置Bean的基本行为，通过scope属性指定，该属性可以接受singleton、prototype、request、session、globlesession5个值，分别代表以上5种作用域。下面的配置片段中，singleton和prototype各有一个：1234&lt;!-- 默认的作用域：singleton --&gt;&lt;bean id="p1" class="com.abc.Person" /&gt; &lt;!-- 指定的作用域：prototype --&gt;&lt;bean id="p2" class="com.abc.Person" scope="prototype" /&gt; 下面是一个测试类123456789public class BeanTest &#123; public static void main(String args[]) &#123; //加载类路径下的beans.xml文件以初始化Spring容器 ApplicationContext context = new ClassPathXmlApplicationContext(); //分两次分别取同一个Bean，比较二者是否是同一个对象 System.out.println(context.getBean("p1") == context.getBean("p1")); System.out.println(context.getBean("p2") == context.getBean("p2")); &#125;&#125; 执行结果分别是：true和false 从结果可以看出，正如上文所述：对于singleton作用域的Bean，每次请求该id的Bean，都将返回同一个实例，而prototype作用域的Bean，每次请求都将产生全新的实例。 注意：早期指定Bean的作用域也可通过singleton属性指定，该属性只接受两个属性值：true和false，分别代表singleton和prototype的作用域。使用singleton属性则无法指定其他三个作用域。实际上Spring2.X不推荐使用singleton属性指定Bean的作用域，singleton属性是Spring 1.2.X的使用方式。 对于request作用域，查看如下Bean定义：1&lt;bean id="loginAction" class="com.abc.LoginAction" scope="request" /&gt; 针对每次HTTP请求，Spring容器会根据loginActionBean定义创建一个全新的LoginAction实例，且该loginAction实例尽在当前HTTPRequest内有效。因此，如果程序需要，完全可以自由更改Bean实例的内部状态；其他请求所获得的loginAction实例无法感觉到这种内部状态的改变。当处理请求结束时，request作用域的Bean将会被销毁。 注意：request、session作用域的Bean只对Web应用才真正有效。实际上通常只会将Web应用的控制器Bean才指定成request作用域 session作用域与request作用域完全类似，区别在于：request作用域的Bean对于每次HTTP请求有效，而session作用域的Bean对于每次Session有效。在Web应用中，为了让request和session作用域生效，必须将HTTP请求对象绑定到为该请求提供服务的线程上，这使得具有request和session作用域的Bean实例能够在后面的调用链中被访问到。 为此我们有两种配置方式：采用Listener配置或者采用Filter配置。当使用Servlet 2.4及以上规范的Web容器时，我们可以在Web应用的web.xml文件中增加Listener配置，该Listener负责为request作用域生效：12345&lt;listener&gt; &lt;listener-class&gt; org.springframework.web.context.request.RequestContextListener &lt;/listener-class&gt;&lt;/listener&gt; 如果使用了只支持Servlet 2.4以前规范的Web容器，则该容器不支持Listener规范，故无法使用这种配置方式，只能改为使用Filter配置方式，配置片段如下：12345678910&lt;filter&gt; &lt;filter-name&gt;requestContextFilter&lt;/filter-name&gt; &lt;filter-class&gt; org.springframework.web.filter.RequestContextFilter &lt;/filter-class&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;requestContextFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 一旦在web.xml中增加了如上任意一种配置，程序就可以在Spring配置文件中使用request或者session作用域了。下面是Spring配置文件的片段：1&lt;bean id=&quot;p3&quot; class=&quot;com.abc.Person&quot; scope=&quot;request&quot; /&gt; 这样，Spring容器会每次HTTP请求都生成一个Person实例，当该请求响应结束时，该实例也随之消失。 如果Web应用直接使用Spring MVC作为MVC框架，即使用SpringDispatcherServlet或DispatcherPortlet来连接所有用户请求，则无需这些额外的配置，因为他们已经处理了所有和请求有关的状态处理。 注意：Spring 3.0 不仅可以为Bean指定已经存在的5个作用域，还支持自定义作用域，关于自定义作用域的内容，请参看Spring官方文档等资料。]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[日志模块]]></title>
    <url>%2FJava%2FJava%E5%9F%BA%E7%A1%80%2F%E6%97%A5%E5%BF%97%E6%A8%A1%E5%9D%97%2F</url>
    <content type="text"><![CDATA[在实际的生产应用中，养成良好的埋点打日志的习惯，是一个优秀软件开发工程师必不可缺的技能。丰富的日志有助于我们排查线上出现的问题。 本文介绍了Java中的日志，并给出了在实际应用中配置的日志模块的实例。 日志的级别 日志信息的优先级从高到底有ERROR、WARN、INFO、DEBUG，分别用来指定这条日志信息的重要程序 日志信息的优先级 org.apache.log4j.Level类提供以下级别，但也可以通过Level类的子类自定义级别 Level 描述 ALL 各级包括自定义级别 DEBUG 指定细粒度信息事件是最有用的应用程序调试 ERROR 错误事件可能仍然允许应用程序继续运行 FATAL 非常严重的错误事件，这可能导致应用程序的终止 INFO 指定能够突出在粗粒度级别的应用程序运行情况的信息 OFF 这是最高等级，是为了关闭日志记录 TRACE 指定细粒度比DEBUG更低的信息事件 WARN 指定具有潜在危害的情况 日志级别是如何工作的 级别p的级别使用q，在记录日志请求时，如果p&gt;=q启用 这条规则是log4j的核心，它假设级别是有序的 标准级别的关系如下： ALL &lt; DEBUG &lt; INFO &lt; WARN &lt; ERROR &lt; FATAL &lt; OFF 在实际开发中使用日志功能slf4j+logback项目依赖123456789101112131415&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;1.7.12&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-core&lt;/artifactId&gt; &lt;version&gt;1.1.3&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-classic&lt;/artifactId&gt; &lt;version&gt;1.1.3&lt;/version&gt;&lt;/dependency&gt; 配置文件1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;!--scan: 当此属性设置为true时，配置文件如果发生改变，将会被重新加载，默认值为true。scanPeriod: 设置监测配置文件是否有修改的时间间隔，如果没有给出时间单位，默认单位是毫秒。当scan为true时，此属性生效。默认的时间间隔为1分钟。debug: 当此属性设置为true时，将打印出logback内部日志信息，实时查看logback运行状态。默认值为false。--&gt;&lt;configuration scan=&quot;true&quot; scanPeriod=&quot;60 seconds&quot; debug=&quot;false&quot;&gt; &lt;jmxConfigurator/&gt; &lt;!--定义变量--&gt; &lt;property name=&quot;log_dir&quot; value=&quot;/opt/logs&quot;/&gt; &lt;!-- 将日志添加到控制台 --&gt; &lt;appender name=&quot;STDOUT&quot; class=&quot;ch.qos.logback.core.ConsoleAppender&quot;&gt; &lt;layout class=&quot;ch.qos.logback.classic.PatternLayout&quot;&gt; &lt;pattern&gt; [%-5level] [%thread] [%logger&#123;36&#125;] [%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125;] %msg%n &lt;/pattern&gt; &lt;/layout&gt; &lt;/appender&gt; &lt;!--将日志添加到文件--&gt; &lt;!-- &lt;file&gt;：被写入的文件名，可以是相对目录，也可以是绝对目录，如果上级目录不存在会自动创建，没有默认值。 &lt;append&gt;：如果是 true，日志被追加到文件结尾，如果是 false，清空现存文件，默认是true。 &lt;encoder&gt;：对记录事件进行格式化。（具体参数稍后讲解 ） &lt;prudent&gt;：如果是 true，日志会被安全的写入文件，即使其他的FileAppender也在向此文件做写入操作，效率低，默认是 false。 &lt;rollingPolicy&gt;:当发生滚动时，决定 RollingFileAppender 的行为，涉及文件移动和重命名。 &lt;triggeringPolicy &gt;: 告知 RollingFileAppender 合适激活滚动。 --&gt; &lt;appender name=&quot;logger&quot; class=&quot;ch.qos.logback.core.rolling.RollingFileAppender&quot;&gt; &lt;File&gt;$&#123;log_dir&#125;/demo.log&lt;/File&gt; &lt;append&gt;true&lt;/append&gt; &lt;rollingPolicy class=&quot;ch.qos.logback.core.rolling.TimeBasedRollingPolicy&quot;&gt; &lt;fileNamePattern&gt;$&#123;log_dir&#125;/demo.log.%d&#123;yyyy-MM-dd&#125;&lt;/fileNamePattern&gt; &lt;/rollingPolicy&gt; &lt;encoder class=&quot;ch.qos.logback.classic.encoder.PatternLayoutEncoder&quot;&gt; &lt;pattern&gt;[%-5level] [%thread] [%logger&#123;36&#125;] [%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125;] %msg%n&lt;/pattern&gt; &lt;charset class=&quot;java.nio.charset.Charset&quot;&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;!--日志级别--&gt; &lt;!-- ERROR &gt; WARN &gt; INFO &gt; DEBUG 只有大于等于当前级别的日志消息才会被打印 --&gt; &lt;!--这种logger向上级root传递日志--&gt; &lt;logger name=&quot;com.winsky.logs&quot;/&gt; &lt;!--没有设置addtivity，默认为true，将此logger的打印信息向上级传递；--&gt; &lt;logger name=&quot;logger_demo&quot; additivity=&quot;false&quot;&gt; &lt;level value=&quot;INFO&quot;/&gt; &lt;appender-ref ref=&quot;logger&quot;/&gt; &lt;appender-ref ref=&quot;STDOUT&quot;/&gt; &lt;/logger&gt; &lt;root level=&quot;ERROR&quot;&gt; &lt;appender-ref ref=&quot;STDOUT&quot;/&gt; &lt;/root&gt;&lt;/configuration&gt; java使用demo12345678910111213141516171819public class LogFactory &#123; private static final Logger log = LoggerFactory.getLogger("logger_demo"); public static Logger getLog() &#123; return log; &#125;&#125;public class LogDemo &#123; private static Logger log = LogFactory.getLog(); public static void main(String[] args) &#123; log.trace("======trace"); log.debug("======debug"); log.info("======info"); log.warn("======warn"); log.error("======error"); &#125;&#125; log4j项目依赖 slf4j-api.jar slf4j-log4j.jar 配置文件12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455# 优先级从高到低分别是 ERROR、WARN、INFO、DEBUG# 设置日志目录log=/data/logslog4j.rootLogger=ERROR,rootDailyFile#设置日志模块是否向上冒泡到根模块log4j.additivity.sysLogger=truelog4j.additivity.synLogger=truelog4j.additivity.httpLogger=true###输出到控制台log4j.appender.systemOut=org.apache.log4j.ConsoleAppenderlog4j.appender.systemOut.layout=org.apache.log4j.PatternLayoutlog4j.appender.systemOut.layout.ConversionPattern=[%-5p][%-22d&#123;yyyy/MM/dd HH:mm:ssS&#125;][%l] %m%nlog4j.appender.systemOut.Threshold=DEBUGlog4j.appender.systemOut.ImmediateFlush=TRUElog4j.appender.systemOut.Target=System.out##输出到文件# 系统日志数据库存储模块log4j.appender.rootDailyFile=org.apache.log4j.DailyRollingFileAppenderlog4j.appender.rootDailyFile.layout=org.apache.log4j.PatternLayoutlog4j.appender.rootDailyFile.layout.ConversionPattern=[%-5p][%-22d&#123;yyyy/MM/dd HH:mm:ssS&#125;][%l] %m%nlog4j.appender.rootDailyFile.Threshold=DEBUGlog4j.appender.rootDailyFile.ImmediateFlush=TRUElog4j.appender.rootDailyFile.Append=TRUElog4j.appender.rootDailyFile.File=$&#123;log&#125;/root.loglog4j.appender.rootDailyFile.DatePattern=&apos;.&apos;yyyy-MM-ddlog4j.appender.rootDailyFile.Encoding=UTF-8### 系统日志数据库存储模块log4j.logger.sysLogger=ERROR,sysDailyFilelog4j.appender.sysDailyFile=org.apache.log4j.DailyRollingFileAppenderlog4j.appender.sysDailyFile.layout=org.apache.log4j.PatternLayoutlog4j.appender.sysDailyFile.layout.ConversionPattern=[%-5p][%-22d&#123;yyyy/MM/dd HH:mm:ssS&#125;][%l] %m%nlog4j.appender.sysDailyFile.Threshold=DEBUGlog4j.appender.sysDailyFile.ImmediateFlush=TRUElog4j.appender.sysDailyFile.Append=TRUElog4j.appender.sysDailyFile.File=$&#123;log&#125;/sysLog.loglog4j.appender.sysDailyFile.DatePattern=&apos;.&apos;yyyy-MM-ddlog4j.appender.sysDailyFile.Encoding=UTF-8### http模块log4j.logger.httpLogger=INFO,httpDailyFilelog4j.appender.httpDailyFile=org.apache.log4j.DailyRollingFileAppenderlog4j.appender.httpDailyFile.layout=org.apache.log4j.PatternLayoutlog4j.appender.httpDailyFile.layout.ConversionPattern=[%-5p][%-22d&#123;yyyy/MM/dd HH:mm:ssS&#125;][%l] %m%nlog4j.appender.httpDailyFile.Threshold=DEBUGlog4j.appender.httpDailyFile.ImmediateFlush=TRUElog4j.appender.httpDailyFile.Append=TRUElog4j.appender.httpDailyFile.File=$&#123;log&#125;/http.loglog4j.appender.httpDailyFile.DatePattern=&apos;.&apos;yyyy-MM-ddlog4j.appender.httpDailyFile.Encoding=UTF-8 Java使用demo12345678910111213141516171819202122232425public class LogFactory &#123; private static final Log sysLogger; private static final Log httpLogger; static &#123; sysLogger = LogFactory.getLog("sysLogger"); httpLogger = LogFactory.getLog("httpLogger"); &#125; public static Log getSysLogger() &#123; return sysLogger; &#125; public static Log getHttpLogger() &#123; return httpLogger; &#125;&#125;public class LoggerDemo &#123; private static final Log sysLogger = LogFactory.getSysLogger(); public static void main(String[] args) &#123; sysLogger.info("log4j日志模块测试"); &#125;&#125;]]></content>
      <categories>
        <category>Java</category>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自定义注解]]></title>
    <url>%2FJava%2FJava%E5%9F%BA%E7%A1%80%2F%E8%87%AA%E5%AE%9A%E4%B9%89%E6%B3%A8%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[在一些现代化的Java web开发框架中，我们能看到各种各样的注解，这些注解极大简便了我们的开发配置，降低了文件间的耦合。 其实，我们自己也可以自定义项目中应用到的注解。本文介绍了Java中的注解，并给出了自定义注解的实例。 元注解 元注解的作用就是负责注解其他的注解。Java5.0定义的元注解： @Target @Retention @Documented @Inherited @Target @Target说明了Annotation所修饰的对象的范围：Annotation可以被用于packages、types（类、接口、枚举、Annotation类型）、类型成员 （方法、构造方法、成员变量、枚举值）、方法参数和本地变量 在Annotation类型的申明中使用了target可以更加明晰其修饰的目标 作用：用于描述注解的使用范围 取值（ElementType）有： 1.CONSTRUCTOR:用于描述构造 2.FIELD:用于描述域 3.LOCAL_VARIABLE:用于描述局部变量 4.METHOD:用于描述方法 5.PACKAGE:用于描述包 6.PARAMETER:用于描述参数 7.TYPE:用于描述类、接口(包括注解类型) 或enum声明 使用实例：123456789101112131415// 可以用于注解类、接口或enumerate声明@Target(ElementType.TYPE)public @interface Table &#123; /** * 数据表名称注解，默认值为类名称 * @return */ public String tableName() default "className";&#125;// 可以注解类的成员变量@Target(ElementType.FIELD)public @interface NoDBColumn &#123;&#125; @Retention 定义了该Annotation被保留的时间长短 有些Annotation仅出现在源代码中，而被编译器丢弃；而另一些却被编译在class文件中；编译在class文件中的Annotation可能会被虚拟机忽略，而另一些class被装载时将被读取 注意，注解并不影响class 的执行，因为Annotation与class在使用上是分离的 作用：表示需要在什么级别保留该注解信息，用于描述注解的生命周期 @Retention取值有 1.SOURCE:在源文件中有效（即源文件保留） 2.CLASS:在class文件中有效（即class保留） 3.RUNTIME:在运行时有效（即运行时保留），这样注解处理器可以通过反射，获取到该注解的属性值，从而去做一些运行时的处理逻辑 @Documented 用于描述其它类型的annotation应该被作为被标注的程序成员的公共API，因此可以被例如javadoc此类的工具文档化。Documented是一个标记注解，没有成员 @Inherited 一个标记注解，阐述了某个被标注的类型是可以被继承的 如果使用了@Inherited修饰的Annotation类型被用于一个class，则这个Annotation将被用于该class的子类 注意：@Inherited annotation类型是被标注过的class的子类所继承。类并不从它所实现的接口继承annotation，方法并不从它所重载的方法继承annotation。 当@Inherited annotation类型标注的annotation的Retention是RetentionPolicy.RUNTIME，则反射API增强了这种继承性。如果我们使用java.lang.reflect去查询一个@Inherited annotation类型的annotation时，反射代码检查将展开工作：检查class和其父类，直到发现指定的annotation类型被发现，或者到达类继承结构的顶层。 自定义注解 使用@interface自定义注解时，自动继承了java.lang.annotation.Annotation接口，，由编译程序自动完成其他细节 在定义注解时，不能继承其他的注解或接口 @interface用来声明一个注解，其中的每一个方法实际上是声明了一个配置参数。方法的名称就是参数的名称，返回值类型就是参数的类型（返回值类型只能是基本类型、Class、String、enum）。可以通过default来声明参数的默认值 定义注解的格式 1public @interface 注解名 &#123;定义体&#125; 注解参数可支持的数据类型 1.所有基本数据类型（int,float,boolean,byte,double,char,long,short) 2.String类型 3.Class类型 4.enum类型 5.Annotation类型 6.以上所有类型的数组 Annotation类型里面的参数该怎么设定: 只能用public 或默认(default)这两个访问权修饰 参数成员只能用上述几个类型 如果只有一个参数成员，最好把参数名称设为value，后加小括号 注解元素的默认值： 注解元素必须有确定的值，要么在定义注解的默认值中指定，要么在使用注解时指定，非基本类型的注解元素的值不可为null 使用空字符串或0作为默认值是一种常用的做法 这个约束使得处理器很难表现一个元素的存在或缺失的状态，因为每个注解的声明中，所有元素都存在，并且都具有相应的值 为了绕开这个约束，我们只能定义一些特殊的值，例如空字符串或者负数，一次表示某个元素不存在，在定义注解时，这已经成为一个习惯用法 注解处理器 注解处理器类库(java.lang.reflect.AnnotatedElement) Java使用Annotation接口来代表程序元素前面的注解，该接口是所有Annotation类型的父接口。除此之外，Java在java.lang.reflect 包下新增了AnnotatedElement接口，该接口代表程序中可以接受注解的程序元素，该接口主要有如下几个实现类： Class：类定义 Constructor：构造器定义 Field：累的成员变量定义 Method：类的方法定义 Package：类的包定义 java.lang.reflect 包下主要包含一些实现反射功能的工具类。实际上，java.lang.reflect 包所有提供的反射API扩充了读取运行时Annotation信息的能力。当一个Annotation类型被定义为运行时的Annotation后，该注解才能是运行时可见，当class文件被装载时被保存在class文件中的Annotation才会被虚拟机读取 AnnotatedElement 接口是所有程序元素（Class、Method和Constructor）的父接口，所以程序通过反射获取了某个类的AnnotatedElement对象之后，程序就可以调用该对象的如下四个个方法来访问Annotation信息： 方法1： T getAnnotation(Class annotationClass): 返回改程序元素上存在的、指定类型的注解，如果该类型注解不存在，则返回null。 方法2：Annotation[] getAnnotations():返回该程序元素上存在的所有注解。 方法3：boolean is AnnotationPresent(Class&lt;?extends Annotation&gt; annotationClass):判断该程序元素上是否包含指定类型的注解，存在则返回true，否则返回false. 方法4：Annotation[] getDeclaredAnnotations()：返回直接存在于此元素上的所有注释。与此接口中的其他方法不同，该方法将忽略继承的注释。（如果没有注释直接存在于此元素上，则返回长度为零的一个数组。）该方法的调用者可以随意修改返回的数组；这不会对其他调用者返回的数组产生任何影响。 使用demo https://gitlab.com/winsky/java-utils/tree/master/src/main/java/com/annotation]]></content>
      <categories>
        <category>Java</category>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>注解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java中左移、右移、无符号右移]]></title>
    <url>%2FJava%2FJava%E5%9F%BA%E7%A1%80%2FJava%E4%B8%AD%E5%B7%A6%E7%A7%BB%E3%80%81%E5%8F%B3%E7%A7%BB%E3%80%81%E6%97%A0%E7%AC%A6%E5%8F%B7%E5%8F%B3%E7%A7%BB%2F</url>
    <content type="text"><![CDATA[计算机对有符号数（包括浮点数）的表示有三种方法：原码、反码和补码，补码=反码+1。在 二进制里，是用 0 和 1 来表示正负的，最高位为符号位，最高位为 1 代表负数，最高位为 0 代表正数。 在计算机中对数进行操作，移位操作可能比乘除操作更效率更高。比如，在ArrayList的扩容实现机制中，Java8中就将0.5倍由原来的除以2改为右移1位，以提高效率。 Java中负数的表示方法在讲解移位操作之前，我们有必要了解一下Java中的负数表示方法。 绝对值取反码再加一 以Java中8位的byte为例，最大值为：01111111，最小值为1000 0000。 那么根据十进制的数字，我们如何转换为二进制呢？对于正数我们直接转换即可，对于负数则有一个过程。 以负数-5为例： 先将-5的绝对值转换成二进制，即为00000101； 然后求该二进制的反码，即为1111 1010； 最后将反码加1，即为：1111 1011 所以Java中Integer.toBinaryString(-5)结果为11111111 11111111 1111111111111011(Integer是32位(bit)的) 左移正负数皆右补0 System.out.println(-3&lt;&lt;1);左移相对来说比较简单. 1111 1111 1111 1111 1111 1111 1111 1101左移一位为 1111 1111 1111 1111 1111 1111 1111 1010,其为-6的补码. System.out.println(Integer.MAX_VALUE&lt;&lt; 1);输出为-2 右移正数左补0、负数左补1 System.out.println(-3&gt;&gt;1);结果是-2,为什么会是-2呢?下面我们来看一下. System.out.println(Integer.toHexString(-3));得到-3的16进制为fffffffd(此为-3的补码,计算机中负数用补码表示). 转换成2进制为1111 1111 1111 1111 1111 1111 1111 1101 右移一位为1111 1111 1111 1111 1111 1111 1111 1110,显而易见此为-2补码. 无符号右移全部左补0 System.out.println(-3&gt;&gt;&gt;1); 1111 1111 1111 1111 1111 1111 1111 1101无符号右移,高位补0, 01111 1111 1111 1111 1111 1111 1111 1110,其为2147483646的原码. 为什么没有无符号左移？因为左移是在后面补0，而右移是在前面边补1或0 有无符号是取决于数的前面的第一位是0还是1，所以右移是会产生到底补1还是0的问题。 而左移始终是在右边补，不会产生符号问题。所以没有必要无符号左移&lt;&lt;&lt;。 无符号左移&lt;&lt;&lt;和左移&lt;&lt;是一样的概念 Java中表示二进制、八进制、十进制、十六进制Java里不能这样表示二进制，只能是 8，10，16进制 8进制：前置0 10进制：不需要前置 16进制：前置0x或者0X 为什么8位的二进制补码范围是[-128,127]计算机对带符号数的表示有三种方法：原码、反码和补码 8位原码和反码能够表示数的范围是-127~127 8位补码能够表示数的范围是 -128~127 所以既然范围是-128~127，那肯定是用补码表示的。 10000000-11111111表示-128到-1, 00000000-01111111表示0-127 补码的1111 1111转换成原码就是1000 0001，也就是-1。 补码就是二进制表示负数的一种方法： 负数的补码就是对反码加一 正数不变,正数的原码反码补码是一样的 在补码中用(-128)代替了(-0)，所以补码的表示范围为：(-128 ~ 0 ~ 127)共256个。 注意:(-128)没有相对应的原码和反码, (-128) = (10000000) 为了充分利用资源，就将原来本应该表示“-0”的补码规定为代表-128 所谓原码就是二进制定点表示法，即最高位为符号位，“0”表示正，“1”表示负，其余位表示数值的大小。 反码表示法规定：正数的反码与其原码相同；负数的反码是对其原码逐位取反，但符号位除外。 补码表示法规定：正数的补码与其原码相同；负数的补码是在其反码的末位加1。]]></content>
      <categories>
        <category>Java</category>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[root用户无法删除文件]]></title>
    <url>%2FLinux%2Froot%E7%94%A8%E6%88%B7%E6%97%A0%E6%B3%95%E5%88%A0%E9%99%A4%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[通常来说，root用户拥有了系统的最高控制权，按理应该不会出现permission denied的问题。但是，今天我在删除服务器上一个文件时，提示permission denied，奇怪了，我明明就是root用户啊，怎么还会权限不足呢？ 最后经过一番排查，原来是文件有 隐藏的 -i属性。解决方案： lsattr 文件名 #找到隐藏文件 chattr -i 文件名 #取消-i 参数 rm -rf 文件名 #删除文件 背景有时候你发现用root权限都不能修改某个文件，大部分原因是曾经用chattr命令锁定该文件了。chattr命令的作用很大，其中一些功能是由Linux内核版本来支持的，不过现在生产绝大部分跑的linux系统都是2.6以上内核了。通过chattr命令修改属性能够提高系统的安全性，但是它并不适合所有的目录。chattr命令不能保护/、/dev、/tmp、/var目录。lsattr命令是显示chattr命令设置的文件属性。 这两个命令是用来查看和改变文件、目录属性的，与chmod这个命令相比，chmod只是改变文件的读写、执行权限，更底层的属性控制是由chattr来改变的。 命令介绍chattr命令的用法：chattr [ -RVf ] [ -v version ] [ mode ] files… 最关键的是在[mode]部分，[mode]部分是由+-=和[ASacDdIijsTtu]这些字符组合的，这部分是用来控制文件的属性。 ：在原有参数设定基础上，追加参数。 ：在原有参数设定基础上，移除参数。= ：更新为指定参数设定。A：文件或目录的 atime (access time)不可被修改(modified), 可以有效预防例如手提电脑磁盘I/O错误的发生。S：硬盘I/O同步选项，功能类似sync。a：即append，设定该参数后，只能向文件中添加数据，而不能删除，多用于服务器日志文件安全，只有root才能设定这个属性。c：即compresse，设定文件是否经压缩后再存储。读取时需要经过自动解压操作。d：即no dump，设定文件不能成为dump程序的备份目标。i：设定文件不能被删除、改名、设定链接关系，同时不能写入或新增内容。i参数对于文件 系统的安全设置有很大帮助。j：即journal，设定此参数使得当通过mount参数：data=ordered 或者 data=writeback 挂 载的文件系统，文件在写入时会先被记录(在journal中)。如果filesystem被设定参数为 data=journal，则该参数自动失效。s：保密性地删除文件或目录，即硬盘空间被全部收回。u：与s相反，当设定为u时，数据内容其实还存在磁盘中，可以用于undeletion。各参数选项中常用到的是a和i。a选项强制只可添加不可删除，多用于日志系统的安全设定。而i是更为严格的安全设定，只有superuser (root) 或具有CAP_LINUX_IMMUTABLE处理能力（标识）的进程能够施加该选项。 应用举例 用chattr命令防止系统中某个关键文件被修改： # chattr +i /etc/resolv.conf 然后用mv /etc/resolv.conf等命令操作于该文件，都是得到Operation not permitted的结果。vim编辑该文件时会提示W10: Warning: Changing a readonly file错误。要想修改此文件就要把i属性去掉： chattr -i /etc/resolv.conf # lsattr /etc/resolv.conf会显示如下属性 ----i-------- /etc/resolv.conf 让某个文件只能往里面追加数据，但不能删除，适用于各种日志文件： # chattr +a /var/log/messages]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[root账户无法登录解决办法]]></title>
    <url>%2FLinux%2Froot%E5%B8%90%E6%88%B7%E6%97%A0%E6%B3%95%E7%99%BB%E9%99%86%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95%2F</url>
    <content type="text"><![CDATA[今天遇到的一个看上去很奇怪的问题（其实是自己蠢，具体经过感兴趣的可以参照一次特殊的root密码错误经历）。本文主要记录Linux系统下，root账户无法登录的解决办法。 /etc/securetty中规定了root可以从哪个tty设备登录，如果root登录不了，可以检查/etc/securetty文件，看看是否禁用了什么设备。如果发现被修改，可以将文件改回原来的样子。并且注意，如果修改了该文件，要保证该文件的权限模式为600，才能正常生效。 正常的/etc/securetty文件内容： 1234567891011121314151617181920212223consolevc/1vc/2vc/3vc/4vc/5vc/6vc/7vc/8vc/9vc/10vc/11tty1tty2tty3tty4tty5tty6tty7tty8tty9tty10tty11 /etc/ssh/sshd_config文件中禁用root登录。如果sshd_config文件中有PermitRootLogin no这行，root就无法通过ssh登录。请改成PermitRootLogin yes，然后重启ssd：/etc/init.d/sshd restart 使用了pam认证，pam配置中限制了root账号的登录。这种情况的可能性比较多，需要仔细检查/etc/pam.d/下以及/etc/security/下的配置文件是否有禁止root的设置。 /etc/passwd文件被修改。检查passwd文件中，root的uid是否为0，root的shell路径是否真实存在，总之root这行的每个设置要完全正常才行。 我就遇到过一种特殊情况，passwd文件的换行符变成了DOS格式，结果linux系统认为shell路径是/bin/bash^M，返回路径不存在错误，导致了root无法登录。所以还要保证passwd文件的换行符是unix格式。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一次特殊的root密码错误经历]]></title>
    <url>%2FLinux%2F%E4%B8%80%E6%AC%A1%E7%89%B9%E6%AE%8A%E7%9A%84root%E5%AF%86%E7%A0%81%E9%94%99%E8%AF%AF%E7%BB%8F%E5%8E%86%2F</url>
    <content type="text"><![CDATA[自从去年寒假剁手买了搬瓦工VPS之后，寒假中就在上面搭建了自己的博客。今天晚上吃饭回来准备更新一把博客，结果登录的时候提示Permission denied, please try again.。我的密码是搬瓦工后台管理面板自己生成的，而且我最近也没有改啊，怎么会突然就不能登录呢？ 经过一晚上的排查，最后终于知道自己犯了一个很蠢的错误。欲知详情，请继续阅读。 一发现不能登录系统，首先想到的是会不会机器被黑了然后密码被改了，或者本地存储密码的文件被我不小心改了？于是我先上搬瓦工控制面板后台尝试重置密码。结果提示：12ErrorMAINTENANCE: KiwiVM control panel is temporarily unavailable. Please try again in a few minutes. (3009005) 这是啥情况？晕死。难道是搬瓦工的管理后台罢工了？论坛求助各位大佬，貌似也没人遇到过这种情况。没办法只能自己捣鼓了。 这期间我甚至尝试了调用搬瓦工提供的API接口重置密码，结果发现可以重置成功，但是还是不能登录上，初步明确了是我的机器的问题，搬瓦工管理面板应该没有罢工。 因为机器上已经装了很多环境了，重新格盘再装的话会很麻烦，所以我暂时还不想重装。试了一下，搬瓦工管理面板上的basic shell仍然能进去，但是进行passwd的时候，提示我用interactive shell来执行这个命令。然鹅，interactive shell首先就需要我登录。我不知道root的密码啊亲，这就很尴尬了。 谷歌一下，有网友说是不是root用户被禁用了，或者其他原因导致了root账户不可登录。具体可以参照root帐户无法登陆解决办法。一一尝试过后仍然不行。这时看到一篇帖子说可以尝试去掉root的密码，将root密码置为空，就可以不输入密码登录。抱着死马当活马医的心态，尝试了一下。结果发现，我的/etc/shadow里面全是空的，一行数据都没有，这是咋了？？ 冷静下来仔细回想了一下，原来中午的时候，清理机器上的文件，因为这个跟某个知名梯子的名称比较像，当时没反应过来，把/etc/shadow当做垃圾文件给删了(哭。。。)，甚至，我还把它的备份文件/etc/shadow-也给删了，被自己蠢哭了。 还好是自己的机器，不是公司的生产机器，不然该出事故了。所以特地写作此文以示警戒。 挖个坑，后面有空来补一下Linux系统中存储用户名和密码的/etc/passwd和/etc/shadow两个文件中内容的具体含义 [X] /etc/passwd文件详解]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux根文件系统shadow文件详解]]></title>
    <url>%2FLinux%2Flinux%E6%A0%B9%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9Fshadow%E6%96%87%E4%BB%B6%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[在Linux系统中，每个用户都具有一个唯一的身份标识，称作用户ID（简称UID）。/etc/passwd是存放用户的地方， 与Linux /etc/passwd文件不同，Linux /etc/shadow文件是只有系统管理员才有权利进行查看和修改的文件，系统管理员应该弄明白Linux中/etc/shadow文件中每个字符段的相应的意义，清楚管理时的具体意义。 Linux中/etc/shadow文件中的记录行与/etc/passwd中的一一对应，它由pwconv命令根据/etc/passwd中的数据自动产生。 /etc/shadow文件格式与/etc/passwd类似，由若干个字段组成，字段之间用“:”隔开。这些字段是：登录名:加密口令:最后一次修改时间:最小时间间隔:最大时间间隔:警告时间:不活动时间:失效时间:标志1）“登录名”是与/etc/passwd文件中的登录名相一致的用户账号2）“口令”字段存放的是加密后的用户口令字，长度为13个字符。如果为空，则对应用户没有口令，登录时不需要口令；如果含有不属于集合{./0-9A-Za-z}中的字符，则对应的用户不能登录。3）“最后一次修改时间”表示的是从某个时刻起，到用户最后一次修改口令时的天数。时间起点对不同的系统可能不一样。例如在SCOLinux中，这个时间起点是1970年1月1日。4）“最小时间间隔”指的是两次修改口令之间所需的最小天数。5）“最大时间间隔”指的是口令保持有效的最大天数。6）“警告时间”字段表示的是从系统开始警告用户到用户密码正式失效之间的天数。7）“不活动时间”表示的是用户没有登录活动但账号仍能保持有效的最大天数。8）“失效时间”字段给出的是一个绝对的天数，如果使用了这个字段，那么就给出相应账号的生存期。期满后，该账号就不再是一个合法的账号，也就不能再用来登录了。 下面是/etc/shadow的一个例子：12345678910111213＃cat/etc/shadowroot:Dnakfw28zf38w:8764:0:168:7:::daemon:*::0:0::::bin:*::0:0::::sys:*::0:0::::adm:*::0:0::::uucp:*::0:0::::nuucp:*::0:0::::auth:*::0:0::::cron:*::0:0::::listen:*::0:0::::lp:*::0:0::::sam:EkdiSECLWPdSa:9740:0:0::]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>/etc/shadow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[中断、异常和系统调用比较]]></title>
    <url>%2F%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%2F%E4%B8%AD%E6%96%AD%E3%80%81%E5%BC%82%E5%B8%B8%E5%92%8C%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8%E6%AF%94%E8%BE%83%2F</url>
    <content type="text"><![CDATA[为什么操作系统需要中断、异常和系统调用： 在计算机运行时，内核是被信任的第三方 只有内核才可以执行特权指令 方便应用程序 概念接下来我们先简单了解一下三者的概念 系统调用（system call）应用程序主动向操作系统发出的服务请求 异常(exception)非法指令或者其他原因导致当前指令执行失败(如：内存出错)后的处理请求 中断(hardware interrupt)来自硬件设备的处理请求 比较源头 中断：外设引起 异常：应用程序意想不到的部分 系统调用：应用程序请求操作系统提供服务 响应方式 中断：异步 异常：同步 系统调用：异步或同步 处理机制 中断：持续，对应用程序是透明的 异常：杀死或重新执行意想不到的程序执行 系统调用：等待和持续 中断处理机制软件处理首先进行现场保存（由编译器完成），然后进行中断服务处理（中断服务例程完成），接着清除中断标记（中断服务例程），最后进行现场恢复（编译器）。 硬件处理依据内部或者外部事件设置中断标志，然后依据中断向量调用相应的中断服务例程。]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>操作系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[八大排序算法复杂度]]></title>
    <url>%2F%E7%AE%97%E6%B3%95%2F%E5%85%AB%E5%A4%A7%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E5%A4%8D%E6%9D%82%E5%BA%A6%2F</url>
    <content type="text"><![CDATA[排序算法是笔面试中经常被问到的问题，本文主要总结各排序算法的时空复杂度，以做记录。 排序算法时间空间复杂度表 排序方法 平均时间 最坏时间 辅助空间 稳定性 冒泡排序 O(n^2) O(n^2) O(1) 稳定 简单选择排序 O(n^2) O(n^2) O(1) 稳定 直接插入排序 O(n^2) O(n^2) O(1) 稳定 希尔排序 O(nlog n) O(n^2) O(1) 不稳定 堆排序 O(nlog n) O(nlog n) O(1) 不稳定 并归排序 O(nlog n) O(nlog n) O(n) 稳定 快速排序 O(nlog n) O(n^2) O(nlog n) 不稳定 基数排序 O(d(n+r)) O(d(n+r)) O(n) 稳定 注：基数排序中，d 为位数，r 为基数，n 为原数组个数。 参考资料 Sorting Algorithms Animations 冒泡排序 | Wikipedia 选择排序 | Wikipedia 快速排序 | Wikipedia 堆排序| Wikipedia 希尔排序 | Wikipedia 归并排序 | Wikipedia 维基百科上的算法和数据结构链接很强大 | 21aspnet]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[操作系统面试问题集锦]]></title>
    <url>%2F%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%2F%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98%E9%9B%86%E9%94%A6%2F</url>
    <content type="text"><![CDATA[本文对面试/笔试过程中经常会被问到的一些关于操作系统的问题进行了梳理和总结，一方面方便自己温故知新，另一方面也希望为找工作的同学们提供一个复习参考。 进程与线程及他们的区别 进程是对运行时程序的封装，是系统进行资源调度和分配的的基本单位，实现了操作系统的并发； 线程是进程的子任务，是CPU调度和分派的基本单位，用于保证程序的实时性，实现进程内部的并发； 一个程序至少有一个进程，一个进程至少有一个线程，线程依赖于进程而存在； 进程在执行过程中拥有独立的内存单元，而多个线程共享进程的内存 进程间通信的几种方式 管道（pipe）及命名管道（named pipe）：管道可用于具有亲缘关系的父子进程间的通信，有名管道除了具有管道所具有的功能外，它还允许无亲缘关系进程间的通信； 信号（signal）：信号是一种比较复杂的通信方式，用于通知接收进程某个事件已经发生； 消息队列：消息队列是消息的链接表，它克服了上两种通信方式中信号量有限的缺点，具有写权限得进程可以按照一定得规则向消息队列中添加新信息；对消息队列有读权限得进程则可以从消息队列中读取信息； 共享内存：可以说这是最有用的进程间通信方式。它使得多个进程可以访问同一块内存空间，不同进程可以及时看到对方进程中对共享内存中数据得更新。这种方式需要依靠某种同步操作，如互斥锁和信号量等； 信号量：主要作为进程之间及同一种进程的不同线程之间得同步和互斥手段； 套接字：这是一种更为一般得进程间通信机制，它可用于网络中不同机器之间的进程间通信，应用非常广泛。 线程同步的方式 互斥量 Synchronized/Lock：采用互斥对象机制，只有拥有互斥对象的线程才有访问公共资源的权限。因为互斥对象只有一个，所以可以保证公共资源不会被多个线程同时访问 信号量 Semphare：它允许同一时刻多个线程访问同一资源，但是需要控制同一时刻访问此资源的最大线程数量 事件(信号)，Wait/Notify：通过通知操作的方式来保持多线程同步，还可以方便的实现多线程优先级的比较操作 进程同步有哪几种机制 原子操作 信号量机制 自旋锁管程 会合 分布式系统 死锁死锁的概念在两个或者多个并发进程中，如果每个进程持有某种资源而又等待其它进程释放它或它们现在保持着的资源，在未改变这种状态之前都不能向前推进，称这一组进程产生了死锁。通俗的讲，就是两个或多个进程无限期的阻塞、相互等待的一种状态。 死锁产生的四个必要条件 互斥：至少有一个资源必须属于非共享模式，即一次只能被一个进程使用；若其他申请使用该资源，那么申请进程必须等到该资源被释放为止； 占有并等待：一个进程必须占有至少一个资源，并等待另一个资源，而该资源为其他进程所占有； 非抢占：进程不能被抢占，即资源只能被进程在完成任务后自愿释放 循环等待：若干进程之间形成一种头尾相接的环形等待资源关系 死锁的处理基本策略和常用方法解决死锁的基本方法主要有 预防死锁、避免死锁、检测死锁、解除死锁 、鸵鸟策略 等 死锁预防死锁预防的基本思想是只要确保死锁发生的四个必要条件中至少有一个不成立，就能预防死锁的发生，具体方法包括： 打破互斥条件：允许进程同时访问某些资源。但是，有些资源是不能被多个进程所共享的，这是由资源本身属性所决定的，因此，这种办法通常并无实用价值。 打破占有并等待条件：可以实行资源预先分配策略(进程在运行前一次性向系统申请它所需要的全部资源，若所需全部资源得不到满足，则不分配任何资源，此进程暂不运行；只有当系统能满足当前进程所需的全部资源时，才一次性将所申请资源全部分配给该线程)或者只允许进程在没有占用资源时才可以申请资源（一个进程可申请一些资源并使用它们，但是在当前进程申请更多资源之前，它必须全部释放当前所占有的资源）。但是这种策略也存在一些缺点：在很多情况下，无法预知一个进程执行前所需的全部资源，因为进程是动态执行的，不可预知的；同时，会降低资源利用率，导致降低了进程的并发性。 打破非抢占条件：允许进程强行从占有者那里夺取某些资源。也就是说，当一个进程占有了一部分资源，在其申请新的资源且得不到满足时，它必须释放所有占有的资源以便让其它线程使用。这种预防死锁的方式实现起来困难，会降低系统性能。 打破循环等待条件：实行资源有序分配策略。对所有资源排序编号，所有进程对资源的请求必须严格按资源序号递增的顺序提出，即只有占用了小号资源才能申请大号资源，这样就不回产生环路，预防死锁的发生 死锁避免的基本思想死锁避免的基本思想是动态地检测资源分配状态，以确保循环等待条件不成立，从而确保系统处于安全状态。 所谓安全状态是指：如果系统能按某个顺序为每个进程分配资源（不超过其最大值），那么系统状态是安全的，换句话说就是，如果存在一个安全序列，那么系统处于安全状态。资源分配图算法和银行家算法是两种经典的死锁避免的算法，其可以确保系统始终处于安全状态。其中，资源分配图算法应用场景为每种资源类型只有一个实例(申请边，分配边，需求边，不形成环才允许分配)，而银行家算法应用于每种资源类型可以有多个实例的场景。 死锁解除死锁解除的常用两种方法为进程终止和资源抢占。所谓进程终止是指简单地终止一个或多个进程以打破循环等待，包括两种方式：终止所有死锁进程和一次只终止一个进程直到取消死锁循环为止；所谓资源抢占是指从一个或多个死锁进程那里抢占一个或多个资源，此时必须考虑三个问题： 选择一个牺牲品 回滚：回滚到安全状态 饥饿（在代价因素中加上回滚次数，回滚的越多则越不可能继续被作为牺牲品，避免一个进程总是被回滚） 操作系统内存管理，分页和分段有什么区别段式存储管理段式存储管理是一种符合用户视角的内存分配管理方案。在段式存储管理中，将程序的地址空间划分为若干段（segment），如代码段，数据段，堆栈段；这样每个进程有一个二维地址空间，相互独立，互不干扰。段式管理的优点是：没有内碎片（因为段大小可变，改变段大小来消除内碎片）。但段换入换出时，会产生外碎片（比如4k的段换5k的段，会产生1k的外碎片） 页式存储管理页式存储管理是一种用户视角内存与物理内存相分离的内存分配管理方案。在页式存储管理中，将程序的逻辑地址划分为固定大小的页（page），而物理内存划分为同样大小的帧，程序加载时，可以将任意一页放入内存中任意一个帧，这些帧不必连续，从而实现了离散分离。页式存储管理的优点是：没有外碎片（因为页的大小固定），但会产生内碎片（一个页可能填充不满）。 两者的不同点： 目的不同：分页是由于系统管理的需要而不是用户的需要，它是信息的物理单位；分段的目的是为了能更好地满足用户的需要，它是信息的逻辑单位，它含有一组其意义相对完整的信息； 大小不同：页的大小固定且由系统决定，而段的长度却不固定，由其所完成的功能决定； 地址空间不同： 段向用户提供二维地址空间；页向用户提供的是一维地址空间； 信息共享：段是信息的逻辑单位，便于存储保护和信息的共享，页的保护和共享受到限制； 内存碎片：页式存储管理的优点是没有外碎片（因为页的大小固定），但会产生内碎片（一个页可能填充不满）；而段式管理的优点是没有内碎片（因为段大小可变，改变段大小来消除内碎片）。但段换入换出时，会产生外碎片（比如4k的段换5k的段，会产生1k的外碎片）。 操作系统中进程调度策略 FCFS(先来先服务，队列实现，非抢占的)：先请求CPU的进程先分配到CPU SJF(最短作业优先调度算法)：平均等待时间最短，但难以知道下一个CPU区间长度 优先级调度算法(可以是抢占的，也可以是非抢占的)：优先级越高越先分配到CPU，相同优先级先到先服务，存在的主要问题是：低优先级进程无穷等待CPU，会导致无穷阻塞或饥饿；解决方案：老化 时间片轮转调度算法(可抢占的)：队列中没有进程被分配超过一个时间片的CPU时间，除非它是唯一可运行的进程。如果进程的CPU区间超过了一个时间片，那么该进程就被抢占并放回就绪队列。 多级队列调度算法：将就绪队列分成多个独立的队列，每个队列都有自己的调度算法，队列之间采用固定优先级抢占调度。其中，一个进程根据自身属性被永久地分配到一个队列中。 多级反馈队列调度算法：与多级队列调度算法相比，其允许进程在队列之间移动：若进程使用过多CPU时间，那么它会被转移到更低的优先级队列；在较低优先级队列等待时间过长的进程会被转移到更高优先级队列，以防止饥饿发生。 虚拟内存虚拟内存允许执行进程不必完全在内存中。 虚拟内存的基本思想是：每个进程拥有独立的地址空间，这个空间被分为大小相等的多个块，称为页(Page)，每个页都是一段连续的地址。这些页被映射到物理内存，但并不是所有的页都必须在内存中才能运行程序。当程序引用到一部分在物理内存中的地址空间时，由硬件立刻进行必要的映射；当程序引用到一部分不在物理内存中的地址空间时，由操作系统负责将缺失的部分装入物理内存并重新执行失败的命令。这样，对于进程而言，逻辑上似乎有很大的内存空间，实际上其中一部分对应物理内存上的一块(称为帧，通常页和帧大小相等)，还有一些没加载在内存中的对应在硬盘上，如下图所示。 由上图可以看出，虚拟内存实际上可以比物理内存大。当访问虚拟内存时，会访问MMU（内存管理单元）去匹配对应的物理地址（比如图5的0，1，2）。如果虚拟内存的页并不存在于物理内存中（如图5的3,4），会产生缺页中断，从磁盘中取得缺的页放入内存，如果内存已满，还会根据某种算法将磁盘中的页换出。 注意，请求分页系统、请求分段系统和请求段页式系统都是针对虚拟内存的，通过请求实现内存与外存的信息置换。 页面置换算法 FIFO先进先出算法：在操作系统中经常被用到，比如作业调度（主要实现简单，很容易想到）； LRU（Least recently use）最近最少使用算法：根据使用时间到现在的长短来判断； LFU（Least frequently use）最少使用次数算法：根据使用次数来判断； OPT（Optimal replacement）最优置换算法：理论的最优，理论；就是要保证置换出去的是不再被使用的页，或者是在实际内存中最晚使用的算法。 虚拟内存的应用与优点虚拟内存很适合在多道程序设计系统中使用，许多程序的片段同时保存在内存中。当一个程序等待它的一部分读入内存时，可以把CPU交给另一个进程使用。虚拟内存的使用可以带来以下好处： 在内存中可以保留多个进程，系统并发度提高 解除了用户与内存之间的紧密约束，进程可以比内存的全部空间还大 颠簸颠簸本质上是指频繁的页调度行为，具体来讲，进程发生缺页中断，这时，必须置换某一页。然而，其他所有的页都在使用，它置换一个页，但又立刻再次需要这个页。因此，会不断产生缺页中断，导致整个系统的效率急剧下降，这种现象称为颠簸（抖动）。 内存颠簸的解决策略包括： 如果是因为页面替换策略失误，可以修改替换算法来解决这个问题； 如果是因为运行的程序太多，造成程序无法同时将所有频繁访问的页面调入内存，则要降低多道程序的数量； 否则，还剩下两个办法：终止该进程或增加物理内存容量。 局部性原理时间上的局部性：最近被访问的页在不久的将来还会被访问； 空间上的局部性：内存中被访问的页周围的页也很可能被访问。 系统中断中断是指CPU对系统发生的某个事件做出的一种反应，CPU暂停正在执行的程序，保留现场后自动地转去执行相应的处理程序，处理完该事件后再返回断点继续执行被“打断”的程序。 中断可分为三类 第一类是由CPU外部引起的，称作中断，如I/O中断、时钟中断、控制台中断等。 第二类是来自CPU的内部事件或程序执行中的事件引起的过程，称作异常，如由于CPU本身故障（电源电压低于105V或频率在47～63Hz之外）、程序故障（非法操作码、地址越界、浮点溢出等）等引起的过程。 第三类由于在程序中使用了请求系统服务的系统调用而引发的过程，称作“陷入”(trap,或者陷阱)。 前两类通常都称作中断，它们的产生往往是无意、被动的，而陷入是有意和主动的。 中断处理中断处理一般分为中断响应和中断处理两个步骤。中断响应由硬件实施，中断处理主要由软件实施。 中断响应对中断请求的整个处理过程是由硬件和软件结合起来而形成的一套中断机构实施的。发生中断时，CPU暂停执行当前的程序，而转去处理中断。这个由硬件对中断请求作出反应的过程，称为中断响应。一般说来，中断响应顺序执行下述三步动作： 中止当前程序的执行； 保存原程序的断点信息（主要是程序计数器PC和程序状态寄存器PS的内容）； 从中断控制器取出中断向量，转到相应的处理程序。 通常CPU在执行完一条指令后，立即检查有无中断请求，如果有，则立即做出响应。 当发生中断时，系统作出响应，不管它们是来自硬件（如来自时钟或者外部设备）、程序性中断（执行指令导致“软件中断”—Software Interrupts），或者来自意外事件（如访问页面不在内存）。 如果当前CPU的执行优先级低于中断的优先级，那么它就中止对当前程序下条指令的执行，接受该中断，并提升处理机的执行级别（一般与中断优先级相同），以便在CPU处理当前中断时，能屏蔽其它同级的或低级的中断，然后保存断点现场信息，通过取得的中断向量转到相应的中断处理程序的入口。 中断处理CPU从中断控制器取得中断向量，然后根据具体的中断向量从中断向量表IDT中找到相应的表项，该表项应是一个中断门。于是，CPU就根据中断门的设置而到达了该通道的总服务程序的入口。 核心对中断处理的顺序主要由以下动作完成： 保存正在运行进程的各寄存器的内容，把它们放入核心栈的新帧面中 确定“中断源”或核查中断发生，识别中断的类型（如时钟中断或盘中断）和中断的设备号（如哪个磁盘引起的中断）。系统接到中断后，就从机器那里得到一个中断号，它是检索中断向量表的位移。中断向量因机器而异，但通常都包括相应中断处理程序入口地址和中断处理时处理机的状态字 核心调用中断处理程序，对中断进行处理 中断处理完成并返回。中断处理程序执行完以后，核心便执行与机器相关的特定指令序列，恢复中断时寄存器内容和执行核心栈退栈，进程回到用户态。如果设置了重调度标志，则在本进程返回到用户态时做进程调度。 管程机制管程的概念管程可以看做一个软件模块，它是将共享的变量和对于这些共享变量的操作封装起来，形成一个具有一定接口的功能模块，进程可以调用管程来实现进程级别的并发控制。 进程只能互斥地使用管程，即当一个进程使用管程时，另一个进程必须等待。当一个进程使用完管程后，它必须释放管程并唤醒等待管程的某一个进程。 在管程入口处的等待队列称为入口等待队列，由于进程会执行唤醒操作，因此可能有多个等待使用管程的队列，这样的队列称为紧急队列，它的优先级高于等待队列。 管程的特征 模块化 管程是一个基本的软件模块，可以被单独编译。 抽象数据类型 管程中封装了数据及对于数据的操作，这点有点像面向对象编程语言中的类。 信息隐藏 管程外的进程或其他软件模块只能通过管程对外的接口来访问管程提供的操作，管程内部的实现细节对外界是透明的。 使用的互斥 任何一个时刻，管程只能由一个进程使用。进入管程时的互斥由编译器负责完成。 目态与管态大多数计算机系统将CPU执行状态分为目态与管态。CPU的状态属于程序状态字PSW的一位。CPU交替执行操作系统程序和用户程序。 管态又叫特权态，系统态或核心态。CPU在管态下可以执行指令系统的全集。通常，操作系统在管态下运行。 目态又叫常态或用户态。机器处于目态时，程序只能执行非特权指令。用户程序只能在目态下运行，如果用户程序在目态下执行特权指令，硬件将发生中断，由操作系统获得控制，特权指令执行被禁止，这样可以防止用户程序有意或无意的破坏系统。 从目态转换为管态的唯一途径是中断。 从管态到目态可以通过修改程序状态字来实现，这将伴随这由操作系统程序到用户程序的转换。 面试/笔试第二弹——操作系统面试问题集锦]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>操作系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java反射机制]]></title>
    <url>%2FJava%2FJava%E5%9F%BA%E7%A1%80%2FJava%E5%8F%8D%E5%B0%84%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[反射机制可以说是Java语言比较一个突出的特点。反射机制允许JVM在运行时知道一个对象的所有信息。本文介绍了Java中的反射机制及其在开发中的应用。 反射的定义 反射机制是在运行时， 对于任意一个类， 都能够知道这个类的所有属性和方法； 对于任意一个对象， 都能够调用它的任意一个方法。 在Java中，只要给定类的名字， 那么就可以通过反射机制来获得类的所有信息。 反射机制主要提供了以下功能： 在运行时判定任意一个对象所属的类； 在运行时创建对象； 在运行时判定任意一个类所具有的成员变量和方法； 在运行时调用任意一个对象的方法； 生成动态代理。 哪里用到反射机制？jdbc 中有一行代码：Class.forName(‘com.mysql.jdbc.Driver.class’);//加载MySql的驱动类。这就是反射。 现在很多框架都用到反射机制， hibernate， struts 都是用反射机制实现的。 反射的实现方式在 Java 中实现反射最重要的一步， 也是第一步就是获取Class对象，得到Class对象后可以通过该对象调用相应的方法来获取该类中的属性、方法以及调用该类中的方法。 有 4 种方法可以得到 Class 对象： Class.forName(“类的路径” ); 类名.class 对象名.getClass() 如果是基本类型的包装类， 则可以通过调用包装类的 Type 属性来获得该包装类的 Class 对象, Class&lt;?&gt; clazz = Integer.TYPE;实现Java反射的类 Class：它表示正在运行的 Java 应用程序中的类和接口。 Field：提供有关类或接口的属性信息， 以及对它的动态访问权限。 Constructor：提供关于类的单个构造方法的信息以及对它的访问权限 Method：提供关于类或接口中某个方法信息。 注意：Class类是Java反射中最重要的一个功能类，所有获取对象的信息(包括： 方法/属性/构造方法/访问权限)都需要它来实现。 反射机制的优缺点优点 能够运行时动态获取类的实例， 大大提高程序的灵活性。 与 Java 动态编译相结合， 可以实现无比强大的功能。 缺点 使用反射的性能较低。Java反射是要解析字节码，将内存中的对象进行解析。 解决方案： 由于JDK的安全检查耗时较多， 所以通过setAccessible(true)的方式关闭安全检查来（取消对访问控制修饰符的检查） 提升反射速度。 需要多次动态创建一个类的实例的时候，有缓存的写法会比没有缓存要快很多: ReflectASM 工具类 ， 通过字节码生成的方式加快反射速度。 使用反射相对来说不安全，破坏了类的封装性，可以通过反射获取这个类的私有方法和属性。]]></content>
      <categories>
        <category>Java</category>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>反射</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[进程和线程]]></title>
    <url>%2F%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%2F%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[进程与线程的区别？？？ 进程和线程的主要差别在于它们是不同的操作系统资源管理方式。 进程有独立的地址空间，一个进程崩溃后，在保护模式下不会对其它进程产生影响，而线程只是一个进程中的不同执行路径。线程有自己的堆栈和局部变量，但线程之间没有单独的地址空间，一个线程死掉就等于整个进程死掉，所以多进程的程序要比多线程的程序健壮，但在进程切换时，耗费资源较大，效率要差一些。但对于一些要求同时进行并且又要共享某些变量的并发操作，只能用线程，不能用进程。 1) 简而言之,一个程序至少有一个进程,一个进程至少有一个线程. 2) 线程的划分尺度小于进程，使得多线程程序的并发性高。 3) 另外，进程在执行过程中拥有独立的内存单元，而多个线程共享内存，从而极大地提高了程序的运行效率。 4) 线程在执行过程中与进程还是有区别的。每个独立的线程有一个程序运行的入口、顺序执行序列和程序的出口。但是线程不能够独立执行，必须依存在应用程序中，由应用程序提供多个线程执行控制。 5) 从逻辑角度来看，多线程的意义在于一个应用程序中，有多个执行部分可以同时执行。但操作系统并没有将多个线程看做多个独立的应用，来实现进程的调度和管理以及资源分配。这就是进程和线程的重要区别。 进程与线程概述进程是具有一定独立功能的程序关于某个数据集合上的一次运行活动,进程是系统进行资源分配和调度的一个独立单位. 线程是进程的一个实体,是CPU调度和分派的基本单位,它是比进程更小的能独立运行的基本单位.线程自己基本上不拥有系统资源,只拥有一点在运行中必不可少的资源(如程序计数器,一组寄存器和栈),但是它可与同属一个进程的其他的线程共享进程所拥有的全部资源. 一个线程可以创建和撤销另一个线程;同一个进程中的多个线程之间可以并发执行. 相对进程而言，线程是一个更加接近于执行体的概念，它可以与同进程中的其他线程共享数据，但拥有自己的栈空间，拥有独立的执行序列。 在串行程序基础上引入线程和进程是为了提高程序的并发度，从而提高程序运行效率和响应时间。 区别进程和线程的主要差别在于它们是不同的操作系统资源管理方式。 进程有独立的地址空间，一个进程崩溃后，在保护模式下不会对其它进程产生影响，而线程只是一个进程中的不同执行路径。 线程有自己的堆栈和局部变量，但线程之间没有单独的地址空间，一个线程死掉就等于整个进程死掉，所以多进程的程序要比多线程的程序健壮，但在进程切换时，耗费资源较大，效率要差一些。 但对于一些要求同时进行并且又要共享某些变量的并发操作，只能用线程，不能用进程。 1) 简而言之,一个程序至少有一个进程,一个进程至少有一个线程. 2) 线程的划分尺度小于进程，使得多线程程序的并发性高。 3) 另外，进程在执行过程中拥有独立的内存单元，而多个线程共享内存，从而极大地提高了程序的运行效率。 4) 线程在执行过程中与进程还是有区别的。每个独立的线程有一个程序运行的入口、顺序执行序列和程序的出口。但是线程不能够独立执行，必须依存在应用程序中，由应用程序提供多个线程执行控制。 5) 从逻辑角度来看，多线程的意义在于一个应用程序中，有多个执行部分可以同时执行。但操作系统并没有将多个线程看做多个独立的应用，来实现进程的调度和管理以及资源分配。这就是进程和线程的重要区别 优缺点线程和进程在使用上各有优缺点：线程执行开销小，但不利于资源的管理和保护；而进程正相反。同时，线程适合于在SMP机器上运行，而进程则可以跨机器迁移。 多进程和多线程概述进程就是一个程序运行的时候被CPU抽象出来的，一个程序运行后被抽象为一个进程，但是线程是从一个进程里面分割出来的，由于CPU处理进程的时候是采用时间片轮转的方式，所以要把一个大个进程给分割成多个线程，例如：网际快车中文件分成100部分，10个线程，文件就被分成了10份来同时下载。1-10占一个线程，11-20占一个线程,依次类推,线程越多,文件就被分的越多,同时下载 当然速度也就越快 进程是程序在计算机上的一次执行活动。当你运行一个程序，你就启动了一个进程。显然，程序只是一组指令的有序集合，它本身没有任何运行的含义，只是一个静态实体。而进程则不同，它是程序在某个数据集上的执行，是一个动态实体。它因创建而产生，因调度而运行，因等待资源或事件而被处于等待状态，因完成任务而被撤消，反映了一个程序在一定的数据集上运行的全部动态过程。进程是操作系统分配资源的单位。在Windows下，进程又被细化为线程，也就是一个进程下有多个能独立运行的更小的单位。线程(Thread)是进程的一个实体，是CPU调度和分派的基本单位。线程不能够独立执行，必须依存在应用程序中，由应用程序提供多个线程执行控制。 线程和进程的关系是： 线程是属于进程的，线程运行在进程空间内，同一进程所产生的线程共享同一内存空间，当进程退出时该进程所产生的线程都会被强制退出并清除。线程可与属于同一进程的其它线程共享进程所拥有的全部资源，但是其本身基本上不拥有系统资源，只拥有一点在运行中必不可少的信息(如程序计数器、一组寄存器和栈)。 在同一个时间里，同一个计算机系统中如果允许两个或两个以上的进程处于运行状态，这便是多任务。现代的操作系统几乎都是多任务操作系统，能够同时管理多个进程的运行。 多任务带来的好处是明显的，比如你可以边听mp3边上网，与此同时甚至可以将下载的文档打印出来，而这些任务之间丝毫不会相互干扰。那么这里就涉及到并行的问题，俗话说，一心不能二用，这对计算机也一样，原则上一个CPU只能分配给一个进程，以便运行这个进程。我们通常使用的计算机中只有一个CPU，也就是说只有一颗心，要让它一心多用，同时运行多个进程，就必须使用并发技术。实现并发技术相当复杂，最容易理解的是“时间片轮转进程调度算法”，它的思想简单介绍如下：在操作系统的管理下，所有正在运行的进程轮流使用CPU，每个进程允许占用CPU的时间非常短(比如10毫秒)，这样用户根本感觉不出来CPU是在轮流为多个进程服务，就好象所有的进程都在不间断地运行一样。但实际上在任何一个时间内有且仅有一个进程占有CPU。 如果一台计算机有多个CPU，情况就不同了，如果进程数小于CPU数，则不同的进程可以分配给不同的CPU来运行，这样，多个进程就是真正同时运行的，这便是并行。但如果进程数大于CPU数，则仍然需要使用并发技术。 在Windows中，进行CPU分配是以线程为单位的，一个进程可能由多个线程组成，这时情况更加复杂，但简单地说，有如下关系：12总线程数 &lt;= CPU数量：并行运行总线程数 &gt; CPU数量：并发运行 并行运行的效率显然高于并发运行，所以在多CPU的计算机中，多任务的效率比较高。但是，如果在多CPU计算机中只运行一个进程(线程)，就不能发挥多CPU的优势。 多任务操作系统(如Windows)的基本原理是:操作系统将CPU的时间片分配给多个线程,每个线程在操作系统指定的时间片内完成(注意,这里的多个线程是分属于不同进程的).操作系统不断的从一个线程的执行切换到另一个线程的执行,如此往复,宏观上看来,就好像是多个线程在一起执行.由于这多个线程分属于不同的进程,因此在我们看来,就好像是多个进程在同时执行,这样就实现了多任务. 分类根据进程与线程的设置，操作系统大致分为如下类型： 单进程、单线程，MS-DOS大致是这种操作系统； 多进程、单线程，多数UNIX(及类UNIX的LINUX)是这种操作系统； 多进程、多线程，Win32(Windows NT/2000/XP等)、Solaris 2.x和OS/2都是这种操作系统； 单进程、多线程，VxWorks是这种操作系统。 引入线程带来的主要好处 在进程内创建、终止线程比创建、终止进程要快； 同一进程内的线程间切换比进程间的切换要快,尤其是用户级线程间的切换。]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>操作系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring AOP概念]]></title>
    <url>%2FSpring%2FSpring%20AOP%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[面向切面编程（AOP）通过提供另外一种思考程序结构的途经来弥补面向对象编程（OOP）的不足。在OOP中模块化的关键单元是类（classes），而在AOP中模块化的单元则是切面。切面能对关注点进行模块化，例如横切多个类型和对象的事务管理。（在AOP术语中通常称作横切（crosscutting）关注点。） AOP框架是Spring的一个重要组成部分。但是Spring IoC容器并不依赖于AOP，这意味着你有权利选择是否使用AOP，AOP做为Spring IoC容器的一个补充,使它成为一个强大的中间件解决方案。 AOP概念首先让我们从一些重要的AOP概念和术语开始。这些术语不是Spring特有的。不过AOP术语并不是特别的直观，如果Spring使用自己的术语，将会变得更加令人困惑。 切面（Aspect）：一个关注点的模块化，这个关注点可能会横切多个对象。事务管理是J2EE应用中一个关于横切关注点的很好的例子。在Spring AOP中，切面可以使用基于模式或者基于@Aspect注解的方式来实现。 连接点（Joinpoint）：在程序执行过程中某个特定的点，比如某方法调用的时候或者处理异常的时候。在Spring AOP中，一个连接点总是表示一个方法的执行。 通知（Advice）：在切面的某个特定的连接点上执行的动作。其中包括了“around”、“before”和“after”等不同类型的通知（通知的类型将在后面部分进行讨论）。许多AOP框架（包括Spring）都是以拦截器做通知模型，并维护一个以连接点为中心的拦截器链。 前置通知（Before advice）：在某连接点之前执行的通知，但这个通知不能阻止连接点之前的执行流程（除非它抛出一个异常）。 后置通知（After returning advice）：在某连接点正常完成后执行的通知：例如，一个方法没有抛出任何异常，正常返回。 异常通知（After throwing advice）：在方法抛出异常退出时执行的通知。 最终通知（After (finally) advice）：当某连接点退出的时候执行的通知（不论是正常返回还是异常退出）。 环绕通知（Around Advice）：包围一个连接点的通知，如方法调用。这是最强大的一种通知类型。环绕通知可以在方法调用前后完成自定义的行为。它也会选择是否继续执行连接点或直接返回它自己的返回值或抛出异常来结束执行。 切入点（Pointcut）：匹配连接点的断言。通知和一个切入点表达式关联，并在满足这个切入点的连接点上运行（例如，当执行某个特定名称的方法时）。切入点表达式如何和连接点匹配是AOP的核心：Spring缺省使用AspectJ切入点语法。 引入（Introduction）：用来给一个类型声明额外的方法或属性（也被称为连接类型声明（inter-type declaration））。Spring允许引入新的接口（以及一个对应的实现）到任何被代理的对象。例如，你可以使用引入来使一个bean实现IsModified接口，以便简化缓存机制。 目标对象（Target Object）： 被一个或者多个切面所通知的对象。也被称做被通知（advised）对象。 既然Spring AOP是通过运行时代理实现的，这个对象永远是一个被代理（proxied）对象。 AOP代理（AOP Proxy）：AOP框架创建的对象，用来实现切面契约（例如通知方法执行等等）。在Spring中，AOP代理可以是JDK动态代理或者CGLIB代理。 织入（Weaving）：把切面连接到其它的应用程序类型或者对象上，并创建一个被通知的对象。这些可以在编译时（例如使用AspectJ编译器），类加载时和运行时完成。Spring和其他纯Java AOP框架一样，在运行时完成织入。 Spring AOP的功能和目标Spring AOP使用纯Java实现。它不需要专门的编译过程。Spring AOP不需要控制类装载器层次，因此它适用于J2EE web容器或应用服务器。 Spring目前仅支持使用方法调用作为连接点（join point）（在Spring bean上通知方法的执行）。虽然可以在不影响到Spring AOP核心API的情况下加入对成员变量拦截器支持，但Spring并没有实现成员变量拦截器。如果你需要把对成员变量的访问和更新也作为通知的连接点，可以考虑其它的语言，如AspectJ。 Spring实现AOP的方法跟其他的框架不同。Spring并不是要提供最完整的AOP实现（尽管Spring AOP有这个能力），相反的，它其实侧重于提供一种AOP实现和Spring IoC容器之间的整合，用于帮助解决在企业级开发中的常见问题。 因此，Spring的AOP功能通常都和Spring IoC容器一起使用。切面使用普通的bean定义语法来配置（尽管Spring提供了强大的”自动代理（autoproxying）”功能）：与其他AOP实现相比这是一个显著的区别。有些事使用Spring AOP是无法轻松或者高效完成的，比如说通知一个细粒度的对象（例如典型的域对象）：这种时候，使用AspectJ是最好的选择。不过经验告诉我们，对于大多数在J2EE应用中适合用AOP来解决的问题，Spring AOP都提供了一个非常好的解决方案。 Spring AOP从来没有打算通过提供一种全面的AOP解决方案来与AspectJ竞争。我们相信无论是基于代理（proxy-based）的框架如Spring AOP或者是成熟的框架如AspectJ都是很有价值的，他们之间应该是互补而不是竞争的关系。Spring 2.0可以无缝的整合Spring AOP，IoC和AspectJ，使得所有的AOP应用完全融入基于Spring的应用体系。这样的集成不会影响Spring AOP API或者AOP Alliance API；Spring AOP保持了向下兼容性。下一章会详细讨论Spring AOP的API。 AOP代理Spring缺省使用J2SE 动态代理（dynamic proxies）来作为AOP的代理。 这样任何接口（或者接口集）都可以被代理。 Spring也可以使用CGLIB代理. 对于需要代理类而不是代理接口的时候CGLIB代理是很有必要的。如果一个业务对象并没有实现一个接口，默认就会使用CGLIB。作为面向接口编程的最佳实践，业务对象通常都会实现一个或多个接口。但也有可能会强制使用CGLIB，在这种情况（希望不常有）下，你可能需要通知一个没有在接口中声明的方法，或者需要传入一个代理对象给方法作为具体类型]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring</tag>
        <tag>AOP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[http重定向301、302、303、307]]></title>
    <url>%2F%E7%BD%91%E7%BB%9C%2Fhttp%E9%87%8D%E5%AE%9A%E5%90%91301%E3%80%81302%E3%80%81303%E3%80%81307%2F</url>
    <content type="text"><![CDATA[重定向常常和请求转发放在一起讨论（前者是两次不相关的请求，后者是一次请求服务器端转发），然而本文并不讨论两者的区别，而是HTTP 1.0规范和HTTP 1.1规范中关于重定向的区别，以及实际使用中的情况。 重定向实际使用是一个响应码（301或302或303或307）和一个响应头location，当浏览器收到响应的时候check响应码是3xx，则会取出响应头中location对应的url，然后将该url替换浏览器地址栏并发起另一次HTTP事务。 HTTP 1.0301301状态码在HTTP 1.0和HTTP 1.1规范中均代表永久重定向. 对于资源请求，原来的url和响应头中location的url而言，资源应该对应location中的url。对于post请求的重定向，还是需要用户确认之后才能重定向，并且应该以post方法发出重定向请求。 关于post请求重定向用户确认的问题，实际上浏览器都没有实现；而且post请求的重定向应该发起post请求，这里浏览器也并不一定遵守，所以说HTTP规范的实现并未严格按照HTTP规范的语义。 在301中资源对应的路径修改为location的url，在SEO中并未出现问题，但是在302中就出现了302劫持问题，请往下看。 302在http 1.0规范中，302表示临时重定向，location中的地址不应该被认为是资源路径，在后续的请求中应该继续使用原地址。 规范：原请求是post，则不能自动进行重定向；原请求是get，可以自动重定向； 实现：浏览器和服务器的实现并没有严格遵守HTTP中302的规范，服务器不加遵守的返回302，浏览器即便原请求是post也会自动重定向，导致规范和实现出现了二义性，由此衍生了一些问题，譬如302劫持，因此在HTTP 1.1中将302的规范细化成了303和307，希望以此来消除二义性。 302劫持A站通过重定向到B站的资源xxoo，A站实际上什么都没做但是有一个比较友好的域名，web资源xxoo存在B站并由B站提供，但是B站的域名不那么友好，因此对搜索引擎而言，可能会保存A站的地址对应xxoo资源而不是B站，这就意味着B站出了资源版权、带宽、服务器的钱，但是用户通过搜索引擎搜索xxoo资源的时候出来的是A站，A站什么都没做却被索搜引擎广而告之用户，B站做了一切却不被用户知道，价值被A站窃取了 HTTP 1.1301和http 1.0规范中保持一致，注意资源对应的路径应该是location中返回的url，而不再是原请求地址。 302在HTTP 1.1中，实际上302是不再推荐使用的，只是为了兼容而作保留。规范中再次重申只有当原请求是GET or HEAD方式的时候才能自动的重定向，为了消除HTTP 1.0中302的二义性，在HTTP 1.1中引入了303和307来细化HTTP 1.0中302的语义。 303在HTTP 1.0的时候，302的规范是原请求是post不可以自动重定向，但是服务器和浏览器的实现是运行重定向。 把HTTP 1.0规范中302的规范和实现拆分开，分别赋予HTTP 1.1中303和307，因此在HTTP 1.1中，303继承了HTTP 1.0中302的实现（即原请求是post，也允许自动进行重定向，结果是无论原请求是get还是post，都可以自动进行重定向），而307则继承了HTTP 1.0中302的规范（即如果原请求是post，则不允许进行自动重定向，结果是post不重定向，get可以自动重定向）。 307在http 1.1规范中，307为临时重定向，如果重定向307的原请求不是get或者head方法，那么浏览器一定不能自动的进行重定向，即便location有url，也应该忽略。 也就是307继承了302在HTTP 1.0中的规范（303继承了302在HTTP 1.0中的实现）。 【HTTP】http重定向301/302/303/307]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP报文]]></title>
    <url>%2F%E7%BD%91%E7%BB%9C%2FHTTP%E6%8A%A5%E6%96%87%2F</url>
    <content type="text"><![CDATA[HTTP报文是面向文本的，报文中的每一个字段都是一些ASCII码串，各个字段的长度是不确定的。HTTP有两类报文：请求报文和响应报文。 本文主要介绍HTTP报文的格式和具体内容。 HTTP请求报文一个HTTP请求报文由请求行（request line）、请求头部（header）、空行和请求数据4个部分组成，下图给出了请求报文的一般格式。 请求行请求行由请求方法字段、URL字段和HTTP协议版本字段3个字段组成，它们用空格分隔。例如，GET /index.html HTTP/1.1。 HTTP协议的请求方法有GET、POST、HEAD、PUT、DELETE、OPTIONS、TRACE、CONNECT。 请求头部请求头部由关键字/值对组成，每行一对，关键字和值用英文冒号“:”分隔。请求头部通知服务器有关于客户端请求的信息，典型的请求头有：12345User-Agent：产生请求的浏览器类型。Accept：客户端可识别的内容类型列表。Host：请求的主机名，允许多个域名同处一个IP地址，即虚拟主机。 空行最后一个请求头之后是一个空行，发送回车符和换行符，通知服务器以下不再有请求头。 请求数据请求数据不在GET方法中使用，而是在POST方法中使用。POST方法适用于需要客户填写表单的场合。与请求数据相关的最常使用的请求头是Content-Type和Content-Length。 HTTP响应报文HTTP响应也由三个部分组成，分别是：状态行、消息报头、响应正文。1234567＜status-line＞＜headers＞＜blank line＞[＜response-body＞] 正如你所见，在响应中唯一真正的区别在于第一行中用状态信息代替了请求信息。状态行（status line）通过提供一个状态码来说明所请求的资源情况。 状态行格式如下： HTTP-Version Status-Code Reason-Phrase CRLF 其中，HTTP-Version表示服务器HTTP协议的版本；Status-Code表示服务器发回的响应状态代码；Reason-Phrase表示状态代码的文本描述。状态代码由三位数字组成，第一个数字定义了响应的类别，且有五种可能取值。 1xx：指示信息–表示请求已接收，继续处理。 2xx：成功–表示请求已被成功接收、理解、接受。 3xx：重定向–要完成请求必须进行更进一步的操作。 4xx：客户端错误–请求有语法错误或请求无法实现。 5xx：服务器端错误–服务器未能实现合法的请求。 常见状态代码、状态描述的说明如下。 200 OK：客户端请求成功。 400 Bad Request：客户端请求有语法错误，不能被服务器所理解。 401 Unauthorized：请求未经授权，这个状态代码必须和WWW-Authenticate报头域一起使用。 403 Forbidden：服务器收到请求，但是拒绝提供服务。 404 Not Found：请求资源不存在，举个例子：输入了错误的URL。 500 Internal Server Error：服务器发生不可预期的错误。 503 Server Unavailable：服务器当前不能处理客户端的请求，一段时间后可能恢复正常。 举个例子：HTTP/1.1 200 OK（CRLF）。]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[类加载机制（类加载过程和类加载器）]]></title>
    <url>%2FJava%2FJVM%2F%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6(%E7%B1%BB%E5%8A%A0%E8%BD%BD%E8%BF%87%E7%A8%8B%E5%92%8C%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%99%A8)%2F</url>
    <content type="text"><![CDATA[作为Java开发者，一般不需要专门编写关注一个Java类是如何被加载执行的，只需要关注好类中具体方法的逻辑实现就行了。 但，想进一步提高自己的Java水平，就需要对JVM以及其中的类加载机制有一定的了解了。本文简单介绍了Java中的类加载机制。 为什么需要使用类加载器Java语言里，类加载都是在程序运行期间完成的，这种策略虽然会令类加载时稍微增加一些性能开销，但是会给java应用程序提供高度的灵活性。 编写一个面向接口的应用程序，可能等到运行时再指定其实现的子类； 用户可以自定义一个类加载器，让程序在运行时从网络或其他地方加载一个二进制流作为程序代码的一部分；(这个是Android插件化，动态安装更新apk的基础) 类加载过程使用Java编译器可以把Java代码编译为存储字节码的Class文件，使用其他语言的编译器一样可以把程序代码翻译成Class文件，Java虚拟机不关心Class的来源是何种语言。如图所示： 在Class文件中描述的各种信息，最终都需要加载到虚拟机中才能运行和使用。那么虚拟机是如何加载这些Class文件的呢？ JVM把描述类数据的字节码.Class文件加载到内存，并对数据进行校验、转换解析和初始化，最终形成可以被虚拟机直接使用的Java类型，这就是虚拟机的类加载机制。 类从被加载到虚拟机内存中开始，到卸载出内存为止，它的生命周期包括了：加载(Loading)、验证(Verification)、准备(Preparation)、解析(Resolution)、初始化(Initialization)、使用(Using)、卸载(Unloading)七个阶段，其中验证、准备、解析三个部分统称链接。 加载(装载)、验证、准备、初始化和卸载这五个阶段顺序是固定的，类的加载过程必须按照这种顺序开始，而解析阶段不一定；它在某些情况下可以在初始化之后再开始，这是为了运行时动态绑定特性（JIT例如接口只在调用的时候才知道具体实现的是哪个子类）。值得注意的是：这些阶段通常都是互相交叉的混合式进行的，通常会在一个阶段执行的过程中调用或激活另外一个阶段。 过程描述包含了加载、验证、准备、解析和初始化这 5 个阶段。 加载加载是类加载的一个阶段，注意不要混淆。 加载过程完成以下三件事： 通过一个类的全限定名来获取定义此类的二进制字节流 将这个字节流所代表的静态存储结构转化为方法区的运行时存储结构 在内存中生成一个代表这个类的Class对象，作为方法区这个类的各种数据的访问入口 其中二进制字节流可以从以下方式中获取： 从 ZIP 包读取，这很常见，最终成为日后 JAR、EAR、WAR 格式的基础。 从网络中获取，这种场景最典型的应用是 Applet 运行时计算生成，这种场景使用得最多得就是动态代理技术，在 java.lang.reflect.Proxy 中，就是用了 ProxyGenerator.generateProxyClass 的代理类的二进制字节流 由其他文件生成，典型场景是 JSP 应用，即由 JSP 文件生成对应的 Class 类。 从数据库读取，这种场景相对少见，例如有些中间件服务器（如 SAP Netweaver）可以选择把程序安装到数据库中来完成程序代码在集群间的分发 验证确保Class文件的字节流中包含的信息符合当前虚拟机的要求，并且不会危害虚拟机自身的安全。 主要有以下 4 个阶段： 文件格式验证：验证字节流是否符合 Class 文件格式的规范，并且能被当前版本的虚拟机处理。 元数据验证：对字节码描述的信息进行语义分析，以保证其描述的信息符合 Java 语言规范的要求。 字节码验证：通过数据流和控制流分析，确保程序语义是合法、符合逻辑的。 符号引用验证：发生在虚拟机将符号引用转换为直接引用的时候，对类自身以外（常量池中的各种符号引用）的信息进行匹配性校验。 准备类变量是被static修饰的变量，准备阶段为类变量分配内存并设置初始值，使用的是方法区的内存。 实例变量不会在这阶段分配内存，它将会在对象实例化时随着对象一起分配在 Java 堆中。 初始值一般为0值，例如下面的类变量value被初始化为 0 而不是 123。1public static int value = 123; 如果类变量是常量，那么会按照表达式来进行初始化，而不是赋值为 0。1public static final int value = 123; 解析将常量池的符号引用替换为直接引用的过程。 初始化初始化阶段才真正开始执行类中的定义的 Java 程序代码。初始化阶段即虚拟机执行类构造器()方法的过程。 在准备阶段，类变量已经赋过一次系统要求的初始值，而在初始化阶段，根据程序员通过程序制定的主观计划去初始化类变量和其它资源。 () 方法具有以下特点： 是由编译器自动收集类中所有类变量的赋值动作和静态语句块（static{}块）中的语句合并产生的，编译器收集的顺序由语句在源文件中出现的顺序决定。特别注意的是，静态语句块只能访问到定义在它之前的类变量，定义在它之后的类变量只能赋值，不能访问。例如以下代码： 1234567public class Test &#123; static &#123; i = 0; // 给变量赋值可以正常编译通过 System.out.print(i); // 这句编译器会提示“非法向前引用” &#125; static int i = 1;&#125; 与类的构造函数（或者说实例构造器 ()）不同，不需要显式的调用父类的构造器。虚拟机会自动保证在子类的()方法运行之前，父类的()方法已经执行结束。因此虚拟机中第一个执行 () 方法的类肯定为 java.lang.Object。 由于父类的 () 方法先执行，也就意味着父类中定义的静态语句块要优于子类的变量赋值操作。例如以下代码： 1234567891011121314static class Parent &#123; public static int A = 1; static &#123; A = 2; &#125;&#125;static class Sub extends Parent &#123; public static int B = A;&#125;public static void main(String[] args) &#123; System.out.println(Sub.B); // 输出结果是父类中的静态变量 A 的值 ，也就是 2。&#125; ()方法对于类或接口不是必须的，如果一个类中不包含静态语句块，也没有对类变量的赋值操作，编译器可以不为该类生成 () 方法。 接口中不可以使用静态语句块，但仍然有类变量初始化的赋值操作，因此接口与类一样都会生成()方法。但接口与类不同的是，执行接口的 ()方法不需要先执行父接口的()方法。只有当父接口中定义的变量使用时，父接口才会初始化。另外，接口的实现类在初始化时也一样不会执行接口的 () 方法。 虚拟机会保证一个类的()方法在多线程环境下被正确的加锁和同步，如果多个线程同时初始化一个类，只会有一个线程执行这个类的()方法，其它线程都会阻塞等待，直到活动线程执行()方法完毕。如果在一个类的()方法中有耗时的操作，就可能造成多个进程阻塞，在实际过程中此种阻塞很隐蔽。 类加载器JVM设计者把类加载阶段中的“通过’类全名’来获取定义此类的二进制字节流”这个动作放到Java虚拟机外部去实现，以便让应用程序自己决定如何去获取所需要的类。实现这个动作的代码模块称为“类加载器”。 类与类加载器对于任何一个类，都需要由加载它的类加载器和这个类来确立其在JVM中的唯一性。也就是说，两个类来源于同一个Class文件，并且被同一个类加载器加载，这两个类才相等。 双亲委派模型从虚拟机的角度来说，只存在两种不同的类加载器 一种是启动类加载器（Bootstrap ClassLoader），该类加载器使用C++语言实现，属于虚拟机自身的一部分。 另外一种就是所有其它的类加载器，这些类加载器是由Java语言实现，独立于JVM外部，并且全部继承自抽象类java.lang.ClassLoader。 从Java开发人员的角度来看，大部分Java程序一般会使用到以下三种系统提供的类加载器： 启动类加载器（Bootstrap ClassLoader）：负责加载JAVA_HOME\lib目录中并且能被虚拟机识别的类库到JVM内存中，如果名称不符合的类库即使放在lib目录中也不会被加载。该类加载器无法被Java程序直接引用。 扩展类加载器（Extension ClassLoader）：该加载器主要是负责加载JAVA_HOME\lib\ext\，该加载器可以被开发者直接使用。 应用程序类加载器（Application ClassLoader）：该类加载器也称为系统类加载器，它负责加载用户类路径（Classpath）上所指定的类库，开发者可以直接使用该类加载器，如果应用程序中没有自定义过自己的类加载器，一般情况下这个就是程序中默认的类加载器。 我们的应用程序都是由这三类加载器互相配合进行加载的，我们也可以加入自己定义的类加载器。这些类加载器之间的关系如下图所示： 如上图所示的类加载器之间的这种层次关系，就称为类加载器的双亲委派模型（Parent Delegation Model）。该模型要求除了顶层的启动类加载器外，其余的类加载器都应当有自己的父类加载器。子类加载器和父类加载器不是以继承（Inheritance）的关系来实现，而是通过组合（Composition）关系来复用父加载器的代码。 双亲委派模型的工作过程为： 如果一个类加载器收到了类加载的请求，它首先不会自己去尝试加载这个类，而是把这个请求委派给父类加载器去完成，每一个层次的加载器都是如此。因此所有的类加载请求都会传给顶层的启动类加载器，只有当父加载器反馈自己无法完成该加载请求（该加载器的搜索范围中没有找到对应的类）时，子加载器才会尝试自己去加载。 使用这种模型来组织类加载器之间的关系的好处是Java类随着它的类加载器一起具备了一种带有优先级的层次关系。 例如java.lang.Object类，无论哪个类加载器去加载该类，最终都是由启动类加载器进行加载，因此Object类在程序的各种类加载器环境中都是同一个类。否则的话，如果不使用该模型的话，如果用户自定义一个java.lang.Object类且存放在classpath中，那么系统中将会出现多个Object类，应用程序也会变得很混乱。如果我们自定义一个rt.jar中已有类的同名Java类，会发现JVM可以正常编译，但该类永远无法被加载运行。 在rt.jar包中的java.lang.ClassLoader类中，我们可以查看类加载实现过程的代码，具体源码如下：1234567891011121314151617181920212223protected synchronized Class loadClass(String name, boolean resolve) throws ClassNotFoundException &#123; // 首先检查该name指定的class是否有被加载 Class c = findLoadedClass(name); if (c == null) &#123; try &#123; if (parent != null) &#123; // 如果parent不为null，则调用parent的loadClass进行加载 c = parent.loadClass(name, false); &#125; else &#123; // parent为null，则调用BootstrapClassLoader进行加载 c = findBootstrapClass0(name); &#125; &#125; catch (ClassNotFoundException e)&#123; // 如果仍然无法加载成功，则调用自身的findClass进行加载 c = findClass(name); &#125; &#125; if (resolve) &#123; resolveClass(c); &#125; return c;&#125; 双亲委派模型是通过loadClass()方法来实现的，根据代码以及代码中的注释可以很清楚地了解整个过程其实非常简单： 先检查是否已经被加载过，如果没有则调用父加载器的loadClass()方法，如果父加载器为空则默认使用启动类加载器作为父加载器。如果父类加载器加载失败，则先抛出ClassNotFoundException，然后再调用自己的findClass()方法进行加载。 一道面试题能不能自己写个类叫java.lang.System？ 【答案】： 通常不可以，但可以采取另类方法达到这个需求。 为了不让我们写System类，类加载采用委托机制，这样可以保证爸爸们优先，爸爸们能找到的类，儿子就没有机会加载。而System类是Bootstrap加载器加载的，就算自己重写，也总是使用Java系统提供的System，自己写的System类根本没有机会得到加载。 但是，我们可以自己定义一个类加载器来达到这个目的，为了避免双亲委托机制，这个类加载器也必须是特殊的。由于系统自带的三个类加载器都加载特定目录下的类，如果我们自己的类加载器放在一个特殊的目录，那么系统的加载器就无法加载，也就是最终还是由我们自己的加载器加载。 反驳上述话的理论 https://blog.csdn.net/tang9140/article/details/42738433]]></content>
      <categories>
        <category>Java</category>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>JVM</tag>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[区块链架构模式分析]]></title>
    <url>%2F%E5%8C%BA%E5%9D%97%E9%93%BE%2F%E5%8C%BA%E5%9D%97%E9%93%BE%E6%9E%B6%E6%9E%84%E6%A8%A1%E5%BC%8F%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[本文介绍了当下比较流行的区块链技术，在查阅了大量资料的基础上，分析了区块链架构的模式，属于一篇综述性文章。 区块链的介绍在区块链技术应用之前，主要有两种交易体系，一种是以银行为信任中心的货币体系，另一种是以第三方机构为信任中心的网络交易。但不管是以银行为信任中心的货币体系，还是以第三方机构为信任中心的网络交易体系，都或多或少的存在着各种问题，不能很好地满足各经济体之间交易的需要。 区块链背景介绍以银行为信任中心的货币体系陌生人之间完全缺少必要的互信，双方之间都对对方不信任。那么扩展到整个社会的交易关系，每个人形成了一个信任孤岛，社会经济也必然会衰败。这也是所谓的拜占庭将军问题，军队虽大，但是将军之间没有互信，最终也无法攻占一个小城池。 当银行的出现后，经济交易关系就不同了。银行作为一个充分稳定、可信的第三方权威机构坐落于两两交易双方，此时银行就充当了陌生人交易之间的信任中介。交易双方都是认同银行不可动摇的信用度，并将自己的资金流和银行交互，而银行作为一个中间人沟通陌生人之间的资金流，从而完成陌生人之间的可靠交易。此时银行成了整个社会交易关系的中心点，每个人都和银行产生联系，进而间接的和其他人发生可靠的交易。而至于银行发行的货币其实就是银行可靠信任度的代表，是银行信任的凭证。 以第三方机构为信任中心的网络交易当前互联网高速发展下，网络交易频繁、交易额巨大，这已经成为国家GDP重要组成部分。保证网络交易的可靠就显得格外的重要。当前的网络交易都需要依赖于第三方可信机构。网络环境和现实环境下的交易关系是很相似的，卖家和买家之间缺少必要的可靠信任，那么设计到各自利益的交易就不是那么容易完成。 买卖双方的互信依赖于第三方机构的保证和维持。买家将所需资金存储到第三方可信机构，卖家在这个事件驱动下为买家提供商品、服务。当双方都确定得到了双方开始的承诺，第三方机构完成资金流的转移，交易完成。这些第三方机构有：微信、支付宝、网银等提供各种特定交易服务的平台。 基于区块链技术的比特币网络当前的共享经济严重依赖于第三方信任中心，这种高度集中的交易关系网络必然有自己的劣势： 过于集中。集中式机构的安全性是比较弱的。当外界对机构产生严重冲击， 就容易崩溃，无法对外提供稳定的可用服务，容灾能力较差。 信任中心不是总是可靠的。信任中心是有团体维护的，必然会存在内部人员出于自身利益而偷偷在机构内部发生攻击，损害用户利益。同时信任中心可能会考虑自身整体利益采取极端措施。 增加交易成本。基于第三方信任中心的交易为了提高信任度，必然要付出额外的信任代价，从而增加交易成本。 为了克服信任中心的劣势，区块链技术应运而生。区块链引人关注之处在于能够在网络中建立点对点之间可靠的信任，使得价值传递过程去除了中介的干扰，既公开信息又保护隐私，既共同决策又保护个体权益，这种机制提高了价值交互的效率并降低了成本。 区块链是一种按照时间顺序将数据区块以顺序相连的方式组合成的一种链式结构，并以密码学方式保证的不可篡改的分布式账本。通过“去中心化”和“去信任化”的方式进行直接的点对点的交易，运用“分布式结构”的数据存储、传递和验证，用数据区块取代了对中心化的依赖。这里的分布式不仅体现为数据的分布式存储，也体现为数据的分布式记录。每条记录从后向前有序链接起来，从而使保存在节点上的交易信息可以快速得到确认，并且由参与的成员集体维护一个可靠数据库的技术，具备公开透明、无法篡改、方便追溯的特点。 目前当我们单独说到区块链的时候，就是指区块链技术，是实现了数据公开、透明、可追溯的产品的架构设计方法，算作广义的区块链。而当在具体产品中谈到区块链的时候，可以指类似比特币的数据存储方式，或许是数据库设计，或许是文件形式的设计，这算作狭义的区块链。广义的区块链技术，必须包含点对点网络设计、加密技术应用、分布式算法的实现、数据存储技术的使用等4个方面，其他的可能涉及到分布式存储、机器学习、VR、物联网、大数据等。狭义的区块链仅仅涉及到数据存储技术，数据库或文件操作等。本文的区块链，指的是广义的区块链。 区块链解决的问题在中心化集中式交易模式中，交易除了卖家和买家，还牵扯到了买家和卖家都信赖的第三方信任机构。比如传统交易中金融机构作为可资信赖的第三方为交易双方提供了电子支付服务。因为交易都围绕一个权威的中心化代理展开，一旦它出了问题便会造成交易的失败，所以该种模式内生性受制于“基于信用的模式”。该种“基于信用的模式”存在这样几大弊端： 交易双方无法实现不可逆交易，逆交易的出现恰是对“信用”最直接的嘲讽，因为它制造了刷信用的潜在可能。一旦出现逆交易就需要中介机构出面协调，这就引致出第二个弊端。 交易成本问题。通过第三方中介机构会产生额外的费用，这些成本最终将以各种形式由每个用户进行承担，增加了交易成本。因此，中介机构的存在限制了实际可行的最小交易规模；此外，第三方中介赚取利润的核心就在于通过转移资金获取各项服务费用，这些费用就是交易成本，而它不就不该存在； 时间成本问题。在跨境交易时，交易的清算和结算都需要经过第三方，而不是直接由交易双方完成，时间成本较高。 借助第三方机构来处理信息的模式拥有点与点之间缺乏信任的弱点，潜在的退款行为促使商家在销售货物之前会索要购买方个人信息(但仍然不能避免一定的欺诈行为)，而这是完全没有必要存在的。 数字签名本身能够解决电子货币身份问题，如果还需要第三方支持才能防止双重消费，则系统将失去价值。 安全性问题。第三方公信系统的记录一旦被篡改，将无法被纠正，即使有备份记录，交易双方也会由于无法完全相信其中的一方而无法达成一致。 因此，针对这种随时都需要第三方金融机构的“基于信用的模式”，区块链提出了一种基于密码学而不是基于信用的交易，使得任何达成一致的双方可以直接支付，即直接排除了第三方中介参与的可能性。杜绝回滚支付交易的存在就可以保护卖家免于欺诈，而亦能使买家在担保机制下免于欺诈。此外，去中心化的处理方式会更便捷，同时也无须担心自己的与交易无关的信息泄漏。设想如果有成千上万笔交易在进行，去中心化的处理方式会节约很多资源，使得整个交易自主化、简单化，并且排除了被中心化代理控制的风险。 区块链在不引入第三方中介机构的前提下，可以提供去中心化、不可篡改、安全可靠等特性保证。因此，所有直接或间接依赖于第三方担保信任机构的活动，均可能从区块链技术中获益。未来几年内，可能深入应用区块链的场景将包括： 金融服务：主要是降低交易成本，减少跨组织交易风险等。该领域的区块链应用将最快成熟起来，银行和金融交易机构将是主力推动者。 征信和权属管理：这是大型社交平台和保险公司都梦寐以求的，目前还缺乏足够的数据来源、可靠的平台支持和有效的数据分析和管理。该领域创业的门槛极高，需要自上而下的推动。 资源共享：Airbnb为代表的公司将欢迎这类应用，极大降低管理成本。这个领域创业门槛低，主题集中，会受到投资热捧。 投资管理：无论公募还是私募基金，都可以应用区块链技术降低管理成本和管控风险。 物联网与供应链：物联网是很适合的一个领域，短期内会有大量应用出现，特别是租赁、物流等特定场景。但物联网自身的发展局限将导致短期内较难出现规模应用。 基于以上的应用场景，区块链针对交易中存在的典型问题，提供了如下的解决方案： 安全性问题：分布式账本存储。每个用户的计算机都被视作一个账本记录区块，如果某些区块的某条记录被篡改，但是与其他区块的记录相矛盾，该条记录会被更正。进一步，如果需要篡改某条信息，则至少需要篡改50%以上区块的该条信息，显然这样的代价几乎是不可能实现的。 费用成本问题：共识算法共同管理。将每个存储账本的用户的计算机当作存储设备，此设备由用户自身进行维护，这样相比于中心化信任过程，机房设备和维护的费用都能几乎降为0。 时间成本问题：互联网直接对接交易双方。所有交易直接通过互联网进行，无需经过清算、结算中心，既避免了错误产生后人工更正的时间，又将交易双方直接相连接，节省了交易时间。 区块链模式分析区块链模式概述区块链是用分布式数据库识别、传播和记载信息的智能化对等网络, 也称为价值互联网。区块链模式将系统功能分为区块、交易、结点、智能合约和共享数据账本这几个模块。实现了在分布式环境里多方参与的双边交易中的去中心化，信息是分布式和分散式的，需要在节点之间进行逻辑验证交易。值得注意的是，虽然区块链也可以用来存储数据，但它要解决的问题是多方的互信问题。 元素 交易：可识别的数据包，记录一笔资产转移的过程。包含加密的货币值、代码、函数调用的参数/结果和公钥/签名等。 区块：存放交易的容器，记录一段时间内全局最新交易的数据块。 节点：拥有一系列已达成一致的交易区块记录，是对当前账本状态的一次共识。 智能合约：数字化合约，在交易完成后自动执行。共享数据账本(元数据，小数据)：记录一个业务活动的系统，记录了参与者之间的资产转移，在参与者间共享且每个人都有自己的副本 关系 每个节点都将新的交易记录存放到一个区块中。 区块中保存数据，每个区块包含一个时间戳和链接到前一个块的信息。 全局认可的区块按时序串接在一起，形成全局共享账本。 节点中包含区块，区块中包含交易。 约束 每个节点都拥有一份完整的数据备份。 区块链每个节点都按照块链式结构存储完整的数据，每个节点存储都是独立的、地位等同的，依靠共识机制保证存储的一致性 保证大多数节点都对交易结果达成一致。 在区块链模式上会强制执行全局规则。 新区块只能添加，不能被修改和删除。采用密码学的方法保证已有数据不能被篡改。 至少有一个区块，并且从第二个区块开始，每个区块中都包含上一个区块的哈希值。 区块链中的节点采用共识算法，对新增数据达成一致。 区块中的每一笔交易都要遵守和执行智能合约，都有发起人的数字签名来保证真实性和合法性 区块链系统架构图从架构设计上来说，区块链可以简单的分为三个层次，协议层、扩展层和应用层。其中，协议层又可以分为存储层和网络层，它们相互独立但又不可分割。 协议层协议层，指代最底层的技术。这个层次通常是一个完整的区块链产品，维护着网络节点，仅提供API供调用。通常官方会提供简单的客户端，这个客户端功能也很简单，只能建立地址、验证签名、转账支付、查看余额等。这个层次是一切的基础，构建了网络环境，搭建了交易通道，制定了节点奖励规则，至于你要交易什么，想干什么，它一概不过问，也过问不了。 从用到的技术来说，协议层主要包括网络编程、分布式算法、加密签名、数据存储技术等4个方面。在架构设计图里，我们把这个层面进一步分成了存储层和网络层。数据存储可以相对独立，选择自由度大一些，可以单独来讨论。选择的原则无非是性能和易用性。我们知道，系统的整体性能主要取决于网络或数据存储的I/O性能，网络I/O优化空间不大，但是本地数据存储的I/O是可以优化的。目前，困扰业界的一个重大问题是加密货币交易处理量远不如现在中心化的支付系统（银行等），除了I/O，需要全方位的突破。 分布式算法、加密签名等都要在实现点对点网络的过程中加以使用，所以是网络层的事情，也是编码的重点难点。当然，也有把点对点网络的实现单独分开的，把节点查找、数据传输和验证等逻辑独立出来，而把共识算法、加密签名、数据存储等操作放在一起组成核心层。无论怎么组合，这两个部分都是最核心最底层的部分，都是协议层的内容。 扩展层这个层面类似于电脑的驱动程序，是为了让区块链产品更加实用。目前有两类，一是各类交易市场，是法币兑换加密货币的重要渠道，实现简单。二是针对某个方向的扩展实现。值得一提的就是我们平时听得最多的“智能合约”的概念，这是典型的扩展层面的应用开发。所谓“智能合约”就是“可编程合约”，或者叫做“合约智能化”，其中的“智能”是执行上的智能，也就是说达到某个条件，合约自动执行，比如自动转移证券、自动付款等，目前还没有比较成型的产品，但不可否认，这将是区块链技术重要的发展方向。 扩展层使用的技术没有什么限制，可以包括很多，上面提到的分布式存储、机器学习、VR、物联网、大数据等等，都可以使用。这个层面与应用层更加接近，也可以理解为B/S架构的产品中的服务端。这样不仅在架构设计上更加科学，让区块链数据更小，网络更独立，同时也可以保证扩展层开发不受约束。 从这个层面来看，区块链可以架构开发任何类型的产品，不仅仅是用在金融行业。在未来，随着底层协议的更加完善，任何需要第三方支付的产品都可以方便的使用区块链技术；任何需要确权、征信和追溯的信息，都可以借助区块链来实现。 应用层这个层面类似于电脑中的各种软件程序，是普通人可以真正直接使用的产品，也可以理解为B/S架构的产品中的浏览器端。这个层面的应用，目前几乎是空白。市场亟待出现这样的应用，引爆市场，形成真正的扩张之势，让区块链技术快速走进寻常百姓，服务于大众。目前使用的各类轻钱包。 区块链对软件质量属性的影响区块链对软件质量属性的影响作为一种软件架构，区块链模式对软件的质量属性有着或好或坏的影响。下表列举了区块链架构对软件系统的质量属性的影响。 质量属性 影响 理由 可用性 提高 由于网络通信中存在各种不稳定因素，节点间通信可能会暂时中断，而在区块链中每个节点都有一份数据备份，所以一旦一个节点暂时无响应也没有影响，可以去另一个节点获得数据，区块链可以容错1/3左右的节点的异常状态。 读性能 提高 可以快速地读，因为每个节点都有一份数据备份，可以直接从本地的数据备份中高效地读取数据。 写性能 降低 因为每次交易为了验证你确实拥有足够的钱而需要追溯历史每一笔记录来计算余额，而且需要将数据传送给所有节点。当交易数据较大的时候，就会有性能问题。而且，新增区块时，因为目前还没有一个有效的启发式hash算法，所以需要消费非常多的计算资源来进行hash计算，这严重降低了性能。 吞吐量 降低 所有交易相关的数据和文件需要即时写入区块链，而区块链的各个参与节点的数据又需要保持同步，所以吞吐量会降低。 延迟性 提高 因为交易需要网络上大多数节点达成共识，而网络传输存在延迟，所以区块链的交易是存在延迟性的。还受一个小概率事件影响，就是当网络上同时有2个或以上节点竞争到记账权力，那么在网络中就会产生2个或以上的区块链分支，这时候到底哪个分支记录的数据是有效的，则要再等下一个记账周期，最终由最长的区块链分支来决定。因此区块链的交易数据是有延迟性的。 可修改性 降低 不可更改性是区块链关键的特征。每个节点都有一个账本，系统会自动比较，认为相同数量最多的账本是真的账本，少部分和别人数量不一样的账本是虚假的账本。因此，任何人修改自己的账本是没有意义的，除非能够篡改整个系统里的大部分节点。而且，所有的操作都需要得到系统内绝大部分节点的认同，也降低了可修改性。 数据完整性 提高 参与的节点各自都有独立的、完整的数据存储，数据的完整性是区块链的内在特性。 机密性 降低 在公有链上，等于每个人手上都有一份完整账本，并且由于区块链计算余额、验证交易有效性等等都需要追溯每一笔账，因此交易数据都是公开透明的，如果我知道某个人的账户，我就能知道他的所有财富和每一笔交易，没有隐私可言。所以机密性降低，但是可以通过加密等方式来增强隐私性。 安全性 提高 采取了加密的hash技术和数字签名，能够全网记录，可追溯，防篡改，具有最终性。记录通过哈希、加密和个人签名得到保护，ID和交易不能直接联系，此外，对于交易有认证机制。密码学的应用保证了未经授权者能访问到数据，但无法解析。 可扩展性 提高 在区块链里添加一个节点，不需要向任何人申请，只要按照约定的算法流程自己生成一个私钥公钥对，再用公钥生成地址，就可以使用该地址。当需要与区块链上其他节点发生交易或者有要提交到区块链上记录的交互内容时，可以全网广播，经所有参与生成区块和记录区块内容的节点确认后，地址便被区块链记住了。 可伸缩性 提高 去除了中央控制服务器的瓶颈，拥有很好的读可伸缩性，但是写可伸缩性会被公有链限制，但总体来说可伸缩性是提升的。 互操作性 降低 数据格式需要事先达成一致，新加入的参与者需要与整个区块链的参与者整合以降低已有参与者的负担，降低了系统的互操作性。 可移植性 降低 可移植性需要考量智能合约的可移植性和数据的可移植性。智能合约和数据在部署到新版区块链时，都需要考虑到向前兼容性、平台兼容性等问题。 可测试性 降低 区块链的测试很复杂，区块链提供了天然可信的分布式账本平台，但是分布式的应用本身的测试面临着很大的困难。另外，由于节点众多，不仅需要测试连接、交易、结算等常见基本功能，而且还要测含任意节点的加入、离开、跨多个节点的穿透交易等。目前缺乏有效的启发式算法来计算新区块的链接过程，所以导致测试中的消耗的时间和计算资源会非常大，降低了可测试性。 可靠性 提高 区块链每个节点都按照块链式结构存储完整一致的数据，即使部分客户端被毁也不影响数据的可靠性。使用“少数服从多数”的共识机制，保证数据不容易丢失，一旦出错可以很快恢复。 稳定性 提高 一旦信息经过验证并添加至区块链，就会永久的存储起来，除非能够同时控制住系统中超过51%的节点，否则单个节点上对数据库的修改是无效的，因此区块链的数据稳定性极高。 易用性 降低 区块链中，账户安全由私钥保护，如果不添加别的手段，如在别处备份等方式，一旦用户丢失了密钥，账户便无法找回。 自治性 提高 区块链是一个高度自治的系统，采用基于协商一致的规范和协议，使得整个系统中的所有节点能够在去信任的环境自由安全地交换数据，使得对“人”的信任改成了对机器的信任，任何人为的干预不起作用。 可管理性 降低 区块链是一个高度自治的系统，没有管理员，是彻底的无中心的。果有人想对区块链添加审核，也实现不了，因为它的设计目标就是防止出现居于中心地位的管理当局 ，所以区块链不存在可管理性。 典型质量属性的刺激响应序列接下来选取区块链架构中几个典型的质量属性，描述它们的场景刺激响应序列。 可用性 性能 安全性 可修改性 机密性 可扩展性 可测试性 可靠性 鸣谢感谢Lucy同学。考虑到单个人查阅的资料存在不全面性的情况，因此，我们俩分别对文章的每部分都做出自己的解答。然后聚在一起讨论自己找到的资料和文章中每部分的内容，整合了两个人的认知，最终形成本文。]]></content>
      <categories>
        <category>区块链</category>
      </categories>
      <tags>
        <tag>区块链</tag>
        <tag>架构模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多应用共享session]]></title>
    <url>%2Fweb%E5%BC%80%E5%8F%91%2F%E5%A4%9A%E5%BA%94%E7%94%A8%E5%85%B1%E4%BA%ABsession%2F</url>
    <content type="text"><![CDATA[在Java web开发中，我们经常使用Tomcat来作为Java web应用的容器。关于web开发中web容器的作用，可以参考这篇文章 特别情况下，我们可能需要部署在Tomcat中的多个应用共享session，以方便在不同的应用中共享存储在session中的内容。本文介绍了如何基于Tomcat实现多应用共享session。 tomcat 配置修改Tomcat/conf/server.xml文件把12345678&lt;Host appBase="webapps" autoDeploy="true" name="localhost" unpackWARs="true" xmlNamespaceAware="false" x mlValidation="false"&gt;&lt;/Host&gt;``` 修改为```XML&lt;Host appBase="webapps" autoDeploy="true" name="localhost" unpackWARs="true" xmlNamespaceAware="false" x mlValidation="false"&gt;&lt;Context path="/project_a" reloadable="false" crossContext="true"&gt;&lt;/Context&gt;&lt;Context path="/project_b" reloadable="false" crossContext="true"&gt;&lt;/Context&gt;&lt;/Host&gt; 注意 crossContext 属性：设置为true，说明你可以调用另外一个WEB应用程序，通过ServletContext.getContext() 获得ServletContext 然后再调用其getAttribute() 得到你要的对象。 编写项目代码project_a项目1234567891011PrintWriter out = response.getWriter();HttpSession session = request.getSession();session.setAttribute("name", "user");session.setMaxInactiveInterval(1800);ServletContext ContextA = request.getSession().getServletContext();ContextA.setAttribute("session", session);//测试out.println("in session put name : " + session.getAttribute("name")); project_b项目1234567891011PrintWriter out = response.getWriter();HttpSession session = request.getSession();ServletContext contextB = session.getServletContext();// 这里面传递的是 Project_A 的虚拟路径ServletContext context = contextB.getContext("/project_a");HttpSession sessionA= (HttpSession) context.getAttribute("session");//测试out.println("in session put name : " + sessionA.getAttribute("name")); 进阶：共享socket套接字project_a项目1234567891011121314151617PrintWriter out = response.getWriter();HttpSession session = request.getSession();session.setAttribute("name", "user");session.setMaxInactiveInterval(1800);ServletContext ContextA = request.getSession().getServletContext();ContextA.setAttribute("session", session);Socket socket = new Socket("127.0.0.1", 8889);ContextA.setAttribute("socket", socket);PrintWriter printWriter = new PrintWriter(socket.getOutputStream(), true);ContextA.setAttribute("printWriter", printWriter);//测试out.println("in session put name : " + session.getAttribute("name")); project_b项目12345678910111213141516PrintWriter out = response.getWriter();HttpSession session = request.getSession();ServletContext contextB = session.getServletContext();// 这里面传递的是 Project_A 的虚拟路径ServletContext context = contextB.getContext("/project_a");HttpSession sessionA= (HttpSession) context.getAttribute("session");PrintWriter printWriter = (PrintWriter) context.getAttribute("printWriter");String ipRequest = "B:" + System.currentTimeMillis();printWriter.println(ipRequest);printWriter.flush();//测试out.println("in session put name : " + sessionA.getAttribute("name")); socket服务端123456789101112131415161718192021222324252627282930public static void main(String[] args) &#123; try &#123; ServerSocket ss = new ServerSocket(8889); while (true) &#123; Socket socket = ss.accept(); receive(socket); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;&#125;public static void receive(final Socket socket)&#123; new Thread()&#123; public void run()&#123; while(true)&#123; try &#123; BufferedReader in = new BufferedReader(new InputStreamReader(socket.getInputStream())); String line = in.readLine(); System.out.println("收到消息:" + line); //返回处理信息 PrintWriter out = new PrintWriter(socket.getOutputStream(), true); out.println("发送信息:" + System.currentTimeMillis()); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;.start();&#125; 同一Tomcat下不同Web应用之间共享Session会话]]></content>
      <categories>
        <category>web开发</category>
      </categories>
      <tags>
        <tag>Tomcat</tag>
        <tag>共享Session</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[volatile关键字]]></title>
    <url>%2FJava%2FJava%E5%9F%BA%E7%A1%80%2Fvolatile%E5%85%B3%E9%94%AE%E5%AD%97%2F</url>
    <content type="text"><![CDATA[Java语言支持多线程，为了解决线程并发的问题，在语言内部引入了同步块synchronized和volatile关键字机制。在java线程并发处理中，关键字volatile比较少用，原因是：一、JDK1.5之前该关键字在不同的操作系统上有不同的表现，所带来是问题就是移植性差，二、是设计困难，而且误用较多。 synchronized同步块，通过 synchronized 关键字来实现，所有加上synchronized 和块语句，在多线程访问的时候，同一时刻只能有一个线程能够用synchronized修饰的方法 或者 代码块。 volatile用volatile修饰的变量，线程在每次使用变量的时候，都会读取变量修改后的最新的值。volatile很容易被误用，用来进行原子性操作，它不能保证多个线程修改的安全性。 这个关键字，还保证Java指令代码不被虚拟机重排。 示例下面使用一个例子来说明这个特性。1234567891011121314151617181920212223242526272829303132333435363738394041public class NovolatileCounter &#123; public static int count = 0; /** * 自增运算，每次自增1 */ public static void increase() &#123; //这里延迟1毫秒，使得结果明显 try &#123; Thread.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; count++; &#125; public static void main(String[] args) &#123; // 启动1000个线程，去进行自增运算 for (int i = 0; i &lt; 1000; i++) &#123; new Thread(new Runnable() &#123; @Override public void run() &#123; NovolatileCounter.increase(); &#125; &#125;).start(); &#125; //这里延迟10毫秒，使得所有线程都执行完毕 try &#123; Thread.sleep(10); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; //这里每次运行的值都有可能不同，可能为1000 System.out.println("Result: NovolatileCounter.count=" + NovolatileCounter.count); &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041public class VolatileCounter &#123; public static volatile int count = 0; /** * 自增运算，每次自增1 */ public static void increase() &#123; //这里延迟1毫秒，使得结果明显 try &#123; Thread.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; count++; &#125; public static void main(String[] args) &#123; // 启动1000个线程，去进行自增运算 for (int i = 0; i &lt; 1000; i++) &#123; new Thread(new Runnable() &#123; @Override public void run() &#123; VolatileCounter.increase(); &#125; &#125;).start(); &#125; //这里延迟10毫秒，使得所有线程都执行完毕 try &#123; Thread.sleep(10); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; //这里每次运行的值都有可能不同，可能为1000 System.out.println("Result: VolatileCounter.count=" + VolatileCounter.count); &#125;&#125; 运行结果依然不是期望的1000，下面分析一下原因 从上图可以看到，对于一般变量的访问，线程在初始化时从主内存中加载所需要的变量值到工作内存中，然后在线程运行时，如果读取，则直接从工作内存中读取，如果需要写入则先写入到工作内存中，之后在刷新到主内存中，但是这样的结构在多线程的情况下可能会出现问题。 如果A线程修改了变量的值，也刷新到主内存中去，但是B，C线程在此时间内读取的还是本线程的工作内存，也就是说读取的不是最新鲜的值，此时就出现了不同线程持有公共资源不同步的情况，可以使用synchronized同步代码块，也可以使用Lock锁来解决。 Java可以使用volatile关键字，确保每个线程对本地变量的访问和修改都直接与主内存交互，而不是与本地线程的工作内存交互的，保证每个线程都能获得最新的值。volatile变量的只剩如下图所示。 由上图可以看出，volatile变量的读写是分开进行的，如一个线程A读取了一个volatile变量，并且修改了这个变量，在修改的值写回主内存前，另一个线程B也读取了volatile变量，则B线程读取到的是原来的值，会造成数据的不一致。由此可以说明，volatile变量关键字并不能保证线程安全，它只能保证当线程需要该变量的值时，能够获得最近被修改的值，而不能保证多个线程的安全性。]]></content>
      <categories>
        <category>Java</category>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java中有了基本类型为什么还需要有包装类型]]></title>
    <url>%2FJava%2FJava%E5%9F%BA%E7%A1%80%2FJava%E4%B8%AD%E6%9C%89%E4%BA%86%E5%9F%BA%E6%9C%AC%E7%B1%BB%E5%9E%8B%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%98%E9%9C%80%E8%A6%81%E6%9C%89%E5%8C%85%E8%A3%85%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[Java中同时存在着基本数据类型和对这些基本数据类型的包装类型。 为什么Java中有了基本类型为什么还需要有包装类型，Java包装类型有什么特别的作用？ Java中基本数据类型与包装类型 基本类型 包装器类型 boolean Boolean char Character int Integer byte Byte short Short long Long float Float double Double 为什么要存在两种类型我们都知道在Java语言中，new一个对象存储在堆里，我们通过栈中的引用来使用这些对象； 但是对于经常用到的一系列类型如int，如果我们用new将其存储在堆里就不是很有效——特别是简单的小的变量。所以就出现了基本类型。 同C++一样，Java采用了相似的做法，对于这些类型不是用new关键字来创建，而是直接将变量的值存储在栈中，因此更加高效。 有了基本类型为什么还要有包装类型我们知道Java是一个面相对象的编程语言，基本类型并不具有对象的性质，为了让基本类型也具有对象的特征，就出现了包装类型（如我们在使用集合类型Collection时就一定要使用包装类型而非基本类型），它相当于将基本类型“包装起来”，使得它具有了对象的性质，并且为其添加了属性和方法，丰富了基本类型的操作。 另外，当需要往ArrayList，HashMap中放东西时，像int，double这种基本类型是放不进去的，因为容器都是装object的，这是就需要这些基本类型的包装器类了。 二者相互转换12int i = 0; Integer ii = new Integer(i); 12Integer ii = new Integer(0); int i = ii.intValue(); 二者的区别 声明方式不同 基本类型不使用new关键字 包装类型需要使用new关键字来在堆中分配存储空间 存储方式及位置不同 基本类型是直接将变量值存储在栈中 包装类型是将对象放在堆中，然后通过引用来使用 初始值不同 基本类型的初始值如int为0，boolean为false 包装类型的初始值为null 使用方式不同 基本类型直接赋值直接使用就好 包装类型在集合如Collection、Map时会使用到。]]></content>
      <categories>
        <category>Java</category>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>数据类型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java多线程中测试某个条件的变化]]></title>
    <url>%2FJava%2FJava%E5%9F%BA%E7%A1%80%2FJava%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%AD%E6%B5%8B%E8%AF%95%E6%9F%90%E4%B8%AA%E6%9D%A1%E4%BB%B6%E7%9A%84%E5%8F%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[wait和notify方法，有个地方要注意，就是经典的生产者和消费模式，使用wait和notify实现，判断条件为什么要用while而不能使用if呢？ 其实是因为当线程wait之后，又被唤醒的时候，是从wait后面开始执行，而不是又从头开始执行的。 所以如果用if的话，被唤醒之后就不会在判断if中的条件，而是继续往下执行了，如果list只是添加了一个数据，而存在两个消费者被唤醒的话，就会出现溢出的问题了，因为不会在判断size是否==0就直接执行remove了。但是如果使用while的话，从wait下面继续执行，还会返回执行while的条件判断，size&gt;0了才会执行remove操作，所以这个必须使用while，而不能使用if来作为判断。 基于以上认知，下面这个是使用wait和notify函数的规范代码模板：12345678// The standard idiom for calling the wait method in Java synchronized (sharedObject) &#123; while (condition) &#123; sharedObject.wait(); // (Releases lock, and reacquires on wakeup) &#125; // do action based upon condition e.g. take or put into queue &#125;]]></content>
      <categories>
        <category>Java</category>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ThreadLocal源码阅读(待补充)]]></title>
    <url>%2FJava%2F%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%2FThreadLocal%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB(%E5%BE%85%E8%A1%A5%E5%85%85)%2F</url>
    <content type="text"><![CDATA[在多并发的环境下，如果不注意考虑线程安全的问题，很容易使应用程序出现各种意料之外的结果。 为了解决线程安全问题，我们主要有三种方式：加锁、使用synchronized关键字和使用ThreadLocal。 平时我使用锁和synchronized关键字比较多，对ThreadLocal是一知半解。本文就来重点介绍一下ThreadLocal的使用及源码实现。 使用ThreadLocal的原因在多线程访问的时候，为了解决线程安全问题，使用synchronized关键字来实现线程同步的可以解决多线程并发访。但是在这种解决方案存在有性能问题，多个线程访问到的都是同一份变量的内容，在多线程同时访问的时候每次只允许一个线程读取变量内容，对变量值进行访问或者修改，其他线程只能处于排队等候状态，顺序执行，谁先抢占到系统资源谁先执行，导致系统效率低下。这是一种以延长访问时间来换取线程安全性的策略。简单来说就是以时间长度换取线程安全，在多用户并发访问的时候，由于等待时间太长，这对用户来说是不能接受的。 而使用ThreadLocal类，该类在每次实例化创建线程的时候都为每一个线程在本地变量中创建了自己独有的变量副本。每个线程都拥有了自己独立的一个变量，竞争条件被彻底消除了，那就没有必要使用synchronized关键字对这些线程进行同步，它们也能最大限度的使用系统资源，由CPU调度并发执行。并且由于每个线程在访问该变量时，读取和修改的，都是自己独有的那一份变量拷贝副本，不会对其他的任何副本产生影响，并发错误出现的可能也完全消除了。对比前一种方案，这是一种以空间来换取线程安全性的策略。在效率上来说比同步高了很多，可以应对多线程并发访问。 源码阅读通过查看ThreadLocal类源码，该类中提供了两个主要的方法get()和set()，还有一个用于回收本地变量中的方法remove()。 set方法源码1234567891011121314151617/** * Sets the current thread's copy of this thread-local variable * to the specified value. Most subclasses will have no need to * override this method, relying solely on the &#123;@link #initialValue&#125; * method to set the values of thread-locals. * * @param value the value to be stored in the current thread's copy of * this thread-local. */public void set(T value) &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value);&#125; 在set()中通过getMap(Thread t)方法获取一个和当前线程相关的 ThreadLocalMap，然后将变量的值设置到这个ThreadLocalMap对象中，如果获取到的ThreadLocalMap对象为空，就通过createMap()方法创建。 线程隔离的秘密，就在于ThreadLocalMap这个类。ThreadLocalMap是ThreadLocal类的一个静态内部类，它实现了键值对的设置和获取（类似于Map&lt;K,V&gt;存储的key-value），每个线程中都有一个独立的ThreadLocalMap副本，它所存储的值，只能被当前线程读取和修改。ThreadLocal类通过操作每一个线程特有的ThreadLocalMap副本，从而实现了变量访问在不同线程中实现隔离。因为每个线程的变量都是自己特有的，完全不会有并发错误。还有一点就是，ThreadLocalMap存储的键值对中的键是this对象指向的ThreadLocal对象，而值就是你所设置的对象了。 来分析源码中出现的getMap和createMap方法的实现：123456789101112131415161718192021/** * Get the map associated with a ThreadLocal. Overridden in * InheritableThreadLocal. * * @param t the current thread * @return the map */ThreadLocalMap getMap(Thread t) &#123; return t.threadLocals;&#125;/** * Create the map associated with a ThreadLocal. Overridden in * InheritableThreadLocal. * * @param t the current thread * @param firstValue value for the initial entry of the map */void createMap(Thread t, T firstValue) &#123; t.threadLocals = new ThreadLocalMap(this, firstValue);&#125; 通过源码分析可以看出，通过获取和设置Thread内的threadLocals变量，而这个变量的类型就是ThreadLocalMap，这样进一步验证了上文中的观点：每个线程都有自己独立的ThreadLocalMap对象。打开java.lang.Thread类的源代码，我们能得到更直观的证明：123/* ThreadLocal values pertaining to this thread. This map is maintained * by the ThreadLocal class. */ThreadLocal.ThreadLocalMap threadLocals = null; get方法源码1234567891011121314151617181920212223242526272829303132333435363738/** * Returns the value in the current thread's copy of this * thread-local variable. If the variable has no value for the * current thread, it is first initialized to the value returned * by an invocation of the &#123;@link #initialValue&#125; method. * * @return the current thread's value of this thread-local */public T get() &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) &#123; ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) &#123; @SuppressWarnings("unchecked") T result = (T)e.value; return result; &#125; &#125; return setInitialValue();&#125;/** * Variant of set() to establish initialValue. Used instead * of set() in case user has overridden the set() method. * * @return the initial value */private T setInitialValue() &#123; T value = initialValue(); Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); return value;&#125; 通过以上源码的分析，在获取和当前线程绑定的值时，ThreadLocalMap对象是以this指向的ThreadLocal对象为键进行查找的，set()方法是设置变量的拷贝副本，get()方法通过键值对的方式获取到这个本地变量的副本的value。 remove方法源码12345678910111213141516/** * Removes the current thread's value for this thread-local * variable. If this thread-local variable is subsequently * &#123;@linkplain #get read&#125; by the current thread, its value will be * reinitialized by invoking its &#123;@link #initialValue&#125; method, * unless its value is &#123;@linkplain #set set&#125; by the current thread * in the interim. This may result in multiple invocations of the * &#123;@code initialValue&#125; method in the current thread. * * @since 1.5 */public void remove() &#123; ThreadLocalMap m = getMap(Thread.currentThread()); if (m != null) m.remove(this);&#125; 通过源码可以知道，该方法就是通过this找到ThreadLocalMap中保存的变量副本做回收处理。 后续补充 并发编程 | ThreadLocal源码深入分析]]></content>
      <categories>
        <category>Java</category>
        <category>源码阅读</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>源码阅读</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring AOP @Before、@Around、@After等执行顺序]]></title>
    <url>%2FSpring%2FSpring%20AOP%20%40Before%E3%80%81%40Around%E3%80%81%40After%E7%AD%89%E6%89%A7%E8%A1%8C%E9%A1%BA%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[我们都知道，Spring AOP中常用的拦截注解有@Before，@Around，@After。 那么问题来了，你知道他们的执行顺序是怎样的吗？恐怕这个问题还是有很多同学回答不上来，没关系，阅读完本文你就知道啦。 先上结论： 一个方法只被一个Aspect类拦截在一个方法只被一个aspect类拦截时，aspect类内部的advice将按照以下的顺序进行执行： 正常流程 异常流程注意，这里的图有误，执行完method触发异常之后，是转到After去执行 同一个方法被多个Aspect类拦截这种情况下，aspect1和aspect2的执行顺序是未知的。 为了指定每个aspect的执行顺序，可以使用两种方法： 实现org.springframework.core.Ordered接口，实现它的getOrder()方法 给aspect添加@Order注解，该注解全称为：org.springframework.core.annotation.Order 不管采用上面的哪种方法，都是值越小的aspect越先执行。 【注意】 如果在同一个aspect类中，针对同一个pointcut，定义了两个相同的advice(比如，定义了两个@Before)，那么这两个advice的执行顺序是无法确定的，哪怕你给这两个advice添加了@Order这个注解，也不行。这点切记。 Spring AOP @Before @Around @After 等 advice 的执行顺序]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring</tag>
        <tag>AOP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[String中的intern方法]]></title>
    <url>%2FJava%2F%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%2FString%E4%B8%AD%E7%9A%84intern%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[在JAVA语言中有8种基本类型和一种比较特殊的类型String。这些类型为了使他们在运行过程中速度更快，更节省内存，都提供了一种常量池的概念。常量池就类似一个JAVA系统级别提供的缓存。 8种基本类型的常量池都是系统协调的，String类型的常量池比较特殊。它的主要使用方法有两种： 直接使用双引号声明出来的String对象会直接存储在常量池中。 如果不是用双引号声明的String对象，可以使用String提供的intern方法。intern方法会从字符串常量池中查询当前字符串是否存在，若不存在就会将当前字符串放入常量池中 今天我们主要来学习一下String中的intern方法 Java实现String#intern方法是一个native的方法，注释写的非常明了。 “如果常量池中存在当前字符串，就会直接返回当前字符串. 如果常量池中没有此字符串，会将此字符串放入常量池中后，再返回”。 String#intern方法的具体实现是c++的代码，这里我们不过多的关注，有兴趣的同学可以去深入解析String#intern中第一节自行查看相关源码。这里我们介绍一下它的大概实现过程： JAVA使用jni调用c++实现的StringTable的intern方法，StringTable的intern方法跟Java中的HashMap的实现是差不多的，只是不能自动扩容。默认大小是1009。 要注意的是，String的StringPool是一个固定大小的Hashtable，默认值大小长度是1009，如果放进StringPool的String非常多，就会造成Hash冲突严重，从而导致链表会很长，而链表长了后直接会造成的影响就是当调用String.intern时性能会大幅下降（因为要一个一个找）。 在JDK6中StringTable是固定的，就是1009的长度，所以如果常量池中的字符串过多就会导致效率下降很快。在JDK7中，StringTable的长度可以通过一个参数指定： -XX:StringTableSize=99991 JDK 6和JDK 7下intern的区别相信很多JAVA程序员都做做类似String s = new String(&quot;abc&quot;)这个语句创建了几个对象的题目。 这种题目主要就是为了考察程序员对字符串对象的常量池掌握与否。上述的语句中是创建了2个对象，第一个对象是abc字符串存储在常量池中，第二个对象在JAVA Heap中的String对象。 比如：1234567891011public static void main(String[] args) &#123; String s = new String("1"); s.intern(); String s2 = "1"; System.out.println(s == s2); String s3 = new String("1") + new String("1"); s3.intern(); String s4 = "11"; System.out.println(s3 == s4);&#125; 打印结果是 JDK 6下：false false JDK 7下：false true 具体为什么稍后再解释，然后将s3.intern();语句下调一行，放到String s4 = &quot;11&quot;;后面。将s.intern(); 放到String s2 = &quot;1&quot;;后面。是什么结果呢？1234567891011public static void main(String[] args) &#123; String s = new String("1"); String s2 = "1"; s.intern(); System.out.println(s == s2); String s3 = new String("1") + new String("1"); String s4 = "11"; s3.intern(); System.out.println(s3 == s4);&#125; 打印结果为： JDK 6下：false false JDK 7下：false false JDK 6的解释 注：图中绿色线条代表string对象的内容指向。黑色线条代表地址指向。 如上图所示。首先说一下JDK6中的情况，在JDK6中上述的所有打印都是false的，因为JDK6中的常量池是放在Perm区中的，Perm区和正常的JAVA Heap区域是完全分开的。 上面说过如果是使用引号声明的字符串都是会直接在字符串常量池中生成，而new出来的String对象是放在JAVA Heap区域。 所以拿一个JAVA Heap区域的对象地址和字符串常量池的对象地址进行比较肯定是不相同的，即使调用String.intern方法也是没有任何关系的。 JDK 7的解释再说说JDK7中的情况。这里要明确一点的是，在JDK6以及以前的版本中，字符串的常量池是放在堆的Perm区的，Perm区是一个类静态的区域，主要存储一些加载类的信息、常量池、方法片段等内容，默认大小只有4M，一旦常量池中大量使用intern是会直接产生java.lang.OutOfMemoryError: PermGen space错误的。 所以在JDK7的版本中，字符串常量池已经从Perm区移到正常的Java Heap区域了。为什么要移动，Perm区域太小是一个主要原因，现在JDK8已经直接取消了Perm区域，而新建立了一个元区域。应该是JDK开发者认为Perm区域已经不适合现在JAVA的发展了。 正式因为字符串常量池移动到JAVA Heap区域后，再来解释为什么会有上述的打印结果。 在第一段代码中，先看s3和s4字符串。String s3 = new String(&quot;1&quot;) + new String(&quot;1&quot;);，这句代码中现在生成了2个对象，是字符串常量池中的“1” 和JAVA Heap中的s3引用指向的对象。中间还有2个匿名的new String(&quot;1&quot;)我们不去讨论它们。此时s3引用对象内容是”11”，但此时常量池中是没有 “11”对象的。 接下来s3.intern();这一句代码，是将s3中的“11”字符串放入String常量池中，因为此时常量池中不存在“11”字符串，因此常规做法是跟JDK6图中表示的那样，在常量池中生成一个 “11” 的对象，关键点是JDK7中常量池不在Perm区域了，这块做了调整。常量池中不需要再存储一份对象了，可以直接存储堆中的引用。这份引用指向s3引用的对象。也就是说引用地址是相同的。 最后String s4 = &quot;11&quot;;这句代码中”11”是显示声明的，因此会直接去常量池中创建，创建的时候发现已经有这个对象了，此时也就是指向s3引用对象的一个引用。所以s4引用就指向和s3一样了。因此最后的比较s3 == s4是true。 再看s和s2对象。String s = new String(&quot;1&quot;);第一句代码，生成了2个对象。常量池中的“1” 和JAVA Heap中的字符串对象。s.intern();这一句是s对象去常量池中寻找后发现 “1” 已经在常量池里了。 接下来String s2 = &quot;1&quot;;这句代码是生成一个s2的引用指向常量池中的“1”对象。 结果就是s和s2的引用地址明显不同。图中画的很清晰。 来看第二段代码，从上边第二幅图中观察。第一段代码和第二段代码的改变就是s3.intern();的顺序是放在String s4 = &quot;11&quot;;后了。这样，首先执行String s4 = &quot;11&quot;;声明s4的时候常量池中是不存在“11”对象的，执行完毕后，“11“对象是s4声明产生的新对象。然后再执行s3.intern();时，常量池中“11”对象已经存在了，因此s3和s4的引用是不同的。 第二段代码中的s和s2代码中，s.intern();，这一句往后放也不会有什么影响了，因为对象池中在执行第一句代码String s = new String(&quot;1&quot;);的时候已经生成“1”对象了。下边的s2声明都是直接从常量池中取地址引用的。s和s2的引用地址是不会相等的。 总结从上述的例子代码可以看出JDK7版本对intern操作和常量池都做了一定的修改。主要包括2点： 将String常量池从Perm区移动到了Java Heap区 String#intern方法时，如果存在堆中的对象，会直接保存对象的引用，而不会重新创建对象。]]></content>
      <categories>
        <category>Java</category>
        <category>源码阅读</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>源码阅读</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OSI七层模型]]></title>
    <url>%2F%E7%BD%91%E7%BB%9C%2FOSI%E4%B8%83%E5%B1%82%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[OSI模型(Open System Interconnection model)是一个由国际标准化组织􏰁提出的概念模型,试图􏰁供一个使各种不同的计算机和网络在世界范围内实现互联的标准框架。 它将计算机网络体系结构划分为七层,每层都可以􏰁供抽象良好的接口。了解OSI模型有助于理解实际上互联网络的工业标准——TCP/IP协议。 七层模型介绍 物理层 物理层负责最后将信息编码成电流脉冲或其它信号用于网上传输； 数据链路层 数据链路层通过物理网络链路􏰁供数据传输。不同的数据链路层定义了不同的网络和协议特征,其中包括物理编址、网络拓扑结构、错误校验、数据帧序列以及流控; 网络层 网络层负责在源和终点之间建立连接; 传输层 传输层向高层􏰁提供可靠的端到端的网络数据流服务。 会话层 会话层建立、管理和终止表示层与实体之间的通信会话； 表示层 表示层􏰁供多种功能用于应用层据编码和转化,以确保以一个系统应用层发送的信息可以被另一个系统应用层识别; 应用层 OSI的应用层协议包括文件的传输、访问及管理协议(FTAM),以及文件虚拟终端协议(VIP)和公用管理系统信息(CMIP)等; 常见的应用层协议： TCP/IP协议TCP/IP的设计,是吸取了分层模型的精华思想——封装。每层对上一层􏰁供服务的时候,上一层的数据结构是黑盒,直接作为本层的数据,而不需要关心上一层协议的任何细节。 TCP/IP 分层模型的分层以以太网上传输 UDP 数据包如图所示; 数据包宽泛意义的数据包:每一个数据包都包含”标头”和”数据”两个部分.”标头”包含本数据包的一些说明.”数据”则是本数据包的内容. 细分数据包： 应用程序数据包 标头部分规定应用程序的数据格式 数据部分传输具体的数据内容 对应上图中的数据 TCP/UDP数据包:标头部分包含双方的发出端口和接收端口. UDP数据包 ‘标头’长度:8个字节 “数据包”总长度最大为65535字节,正好放进一个IP数据包. TCP数据包 理论上没有长度限制,但是,为了保证网络传输效率,通常不会超过IP数据长度,确保单个包不会被分割 对应上图中的UDP数据 IP数据包 标头部分包含通信双方的IP地址,协议版本,长度等信息 ‘标头’长度:20~60字节 “数据包”总长度最大为65535字 对应上图中的IP数据 以太网数据包 最基础的数据包 标头部分包含了通信双方的MAC地址,数据类型等 ‘标头’长度:18字节 ‘数据’部分长度:46~1500字节 对应上图中的以太网数据 四层模型 网络接口层 网间层 传输层 应用层]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java自定义排序[升序降序的辨识]]]></title>
    <url>%2FJava%2FJava%E5%9F%BA%E7%A1%80%2FJava%E8%87%AA%E5%AE%9A%E4%B9%89%E6%8E%92%E5%BA%8F%5B%E5%8D%87%E5%BA%8F%E9%99%8D%E5%BA%8F%E7%9A%84%E8%BE%A8%E8%AF%86%5D%2F</url>
    <content type="text"><![CDATA[一直以来，对Java中自定义排序中是如何判断升序还是降序没有任何概念，每次遇到都是线程调试一把看看结果。 今天来介绍一种如何辨识自定义排序中是升序还是降序的方法 我的辨识方法： 顺序主要看返回-1的情况，-1决定了是否需要调整顺序。 123if(o1.compareTo(o2) &lt; 0 )&#123; return ?;&#125; 这里o1表示位于前面的字符，o2表示后面的字符 上面的条件是，o1比o2小，这个时候，我们需要需要调整它们的顺序 如果你想升序，那么o1比o2小就是我想要的；所以返回-1，类比成false；表示我不想调整顺序 如果你想降序，那么o1比o2小不是我想要的；所以返回1，类比成true；表示我想调整顺序 如下例子就是对数组进行升序排列。因为在o1 &lt; o2的时候，返回了-1，表名不需要调整顺序 123456789Character[] s = &#123;'c', 'b', 'a'&#125;;Arrays.sort(s, new Comparator&lt;Character&gt;() &#123; @Override public int compare(Character o1, Character o2) &#123; if (o1 &gt; o2) return 1; if (o1 &lt; o2) return -1; else return 0; &#125;&#125;);]]></content>
      <categories>
        <category>Java</category>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring 注释 @Autowired 和@Resource 的区别]]></title>
    <url>%2FSpring%2FSpring%20%E6%B3%A8%E9%87%8A%20%40Autowired%20%E5%92%8C%40Resource%20%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[@Autowired和@Resource是Spring中进行依赖注入时常用的两个注解。 它们都可以进行依赖注入，那它们有什么区别呢？ 在实际生产应用中，我们应该偏向于使用哪种呢？ @Autowired和@Resource都可以用来装配bean，都可以写在字段上，或者方法上。 @Autowired属于Spring的；@Resource为JSR-250标准的注释，属于J2EE的。 @Autowired默认按类型装配，默认情况下必须要求依赖对象必须存在，如果要允许null值，可以设置它的required属性为false，例如：@Autowired(required=false) ，如果我们想使用名称装配可以结合@Qualifier注解进行使用，如下：123@Autowired() @Qualifier("baseDao")private BaseDao baseDao; @Resource，默认安装名称进行装配，名称可以通过name属性进行指定，如果没有指定name属性，当注解写在字段上时，默认取字段名进行安装名称查找，如果注解写在setter方法上默认取属性名进行装配。当找不到与名称匹配的bean时才按照类型进行装配。但是需要注意的是，如果name属性一旦指定，就只会按照名称进行装配。12@Resource(name="baseDao")private BaseDao baseDao; 推荐使用：@Resource注解在字段上，这样就不用写setter方法了，并且这个注解是属于J2EE的，减少了与Spring的耦合。这样代码看起就比较优雅。]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[synchronized关键字的使用]]></title>
    <url>%2FJava%2FJava%E5%9F%BA%E7%A1%80%2Fsynchronized%E5%85%B3%E9%94%AE%E5%AD%97%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Java语言的关键字，当它用来修饰一个方法或者一个代码块的时候，能够保证在同一时刻最多只有一个线程执行该段代码。 当多条线程同时访问共享数据时，如果不进行同步，就会发生错误。Java提供的解决方案是：只要将操作共享数据的语句在某一时间段让一个线程执行完，在执行过程中其他线程不能进来执行。 使用synchronized时会有两种方式，一种是同步方法，一种是同步代码块 synchronized(this)是锁定当前对象实例。在函数m1()里面写synchronized(this )，这个和public synchronized void m1() 等价 对静态方法和静态变量使用synchronized方法，锁定的都是类，该类所有的实例都得排队等待已经取得锁的对象实例释放锁 类锁和对象锁是两个不一样的锁，控制着不同的区域，它们是互不干扰的。同样，线程获得对象锁的同时，也可以获得该类锁，即同时获得两个锁，这是允许的。 synchronized(Object)锁定的是类中的成员变量，所有进行了synchronized(Object)的代码块都是互斥的 下面来看具体例子: 同一对象中，synchronized代码块和synchronized方法，锁定的对象 创建一个有4个方法的对象syncBlock，其中一个synchronized方法，一个synchronized代码块，锁定this对象，还有两个synchronized代码块，锁定syncBlock中的一个对成员变量 先启动一个线程(Thread-0), 并让其进入syncBlock对象的sychronized方法(add)内, 并使其停在synchronized方法内 再启动一个线程(Thread-1),并执行syncBlock对象的一个synchronized(this)代码块的方法(minus), 看看能否进入此方法内 再启动一个线程(Thread-2),并执行syncBlock对象的一个synchronized(processing)代码块的方法(times), 看看能否进入此方法内 再启动一个线程(Thread-3),并执行syncBlock对象的一个synchronized(processing)代码块的方法(division), 看看能否进入此方法内123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160public class SyncBlock &#123; private int sum = 1; private Object processing = new Object(); public synchronized void add() &#123; try &#123; System.out.println(Thread.currentThread().getName() + " add方法, 已经获取内置锁`SyncBlock`"); Thread.sleep(5000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; sum += 100; System.out.println(Thread.currentThread().getName() + " add方法, 即将释放内置锁`SyncBlock`"); &#125; public void minus() &#123; synchronized (this) &#123; try &#123; System.out.println(Thread.currentThread().getName() + " minus方法, 已经获取内置锁`SyncBlock.this`"); Thread.sleep(3000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; sum -= 80; System.out.println(Thread.currentThread().getName() + " minus方法, 即将释放内置锁`SyncBlock.this`"); &#125; &#125; public void times() &#123; synchronized (processing) &#123; try &#123; System.out.println(Thread.currentThread().getName() + " times方法, 已经获取内置锁`processing`"); Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; sum *= 20; System.out.println(Thread.currentThread().getName() + " times方法, 即将释放内置锁`processing`"); &#125; &#125; public void division() &#123; synchronized (processing) &#123; try &#123; System.out.println(Thread.currentThread().getName() + " division方法, 已经获取内置锁`processing`"); Thread.sleep(4000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; sum /= 2; System.out.println(Thread.currentThread().getName() + " division方法, 即将释放内置锁`processing`"); &#125; &#125; public int getSum() &#123; return sum; &#125;&#125;class BlockThread1 extends Thread &#123; SyncBlock syncBlock; public BlockThread1(SyncBlock syncBlock) &#123; this.syncBlock = syncBlock; &#125; @Override public void run() &#123; System.out.println(Thread.currentThread().getName() + " running ..."); syncBlock.add(); &#125;&#125;class BlockThread2 extends Thread &#123; SyncBlock syncBlock; public BlockThread2(SyncBlock syncBlock) &#123; this.syncBlock = syncBlock; &#125; @Override public void run() &#123; System.out.println(Thread.currentThread().getName() + " running ..."); syncBlock.minus(); &#125;&#125;class BlockThread3 extends Thread &#123; SyncBlock syncBlock; public BlockThread3(SyncBlock syncBlock) &#123; this.syncBlock = syncBlock; &#125; @Override public void run() &#123; System.out.println(Thread.currentThread().getName() + " running ..."); syncBlock.times(); &#125;&#125;class BlockThread4 extends Thread &#123; SyncBlock syncBlock; public BlockThread4(SyncBlock syncBlock) &#123; this.syncBlock = syncBlock; &#125; @Override public void run() &#123; System.out.println(Thread.currentThread().getName() + " running ..."); syncBlock.division(); &#125;&#125;class BlockThread5 extends Thread &#123; SyncBlock syncBlock; public BlockThread5(SyncBlock syncBlock) &#123; this.syncBlock = syncBlock; &#125; @Override public void run() &#123; System.out.println(Thread.currentThread().getName() + " running ..."); while (true) &#123; System.out.println("sum=" + syncBlock.getSum()); try &#123; sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125;class TestSynBlock &#123; public static void main(String[] args) throws InterruptedException &#123; SyncBlock syncBlock = new SyncBlock(); BlockThread1 thread1 = new BlockThread1(syncBlock); BlockThread2 thread2 = new BlockThread2(syncBlock); BlockThread3 thread3 = new BlockThread3(syncBlock); BlockThread4 thread4 = new BlockThread4(syncBlock); BlockThread5 thread5 = new BlockThread5(syncBlock); thread5.start(); thread1.start();//先执行, 以便抢占锁 Thread.sleep(500); //放弃cpu, 让thread1执行, 以便获的锁 thread2.start();//先执行, 以便抢占锁 Thread.sleep(500); //放弃cpu, 让thread2执行, 以便获的锁 thread3.start();//先执行, 以便抢占锁 Thread.sleep(500); //放弃cpu, 让thread3执行, 以便获的锁 thread4.start();//先执行, 以便抢占锁 Thread.sleep(500); //放弃cpu, 让thread4执行, 以便获的锁 &#125;&#125; 程序输出如下:1234567891011121314151617181920212223Thread-0 running ...Thread-4 running ...Thread-0 add方法, 已经获取内置锁`SyncBlock`sum=1Thread-1 running ...Thread-2 running ...Thread-2 times方法, 已经获取内置锁`processing`sum=1Thread-3 running ...sum=1Thread-2 times方法, 即将释放内置锁`processing`Thread-3 division方法, 已经获取内置锁`processing`sum=20sum=20Thread-0 add方法, 即将释放内置锁`SyncBlock`Thread-1 minus方法, 已经获取内置锁`SyncBlock.this`sum=120sum=120Thread-3 division方法, 即将释放内置锁`processing`sum=60Thread-1 minus方法, 即将释放内置锁`SyncBlock.this`sum=-20sum=-20 结果分析: 观察显示, 在输出Thread-1 running ...后会暂停数秒(Thread-1无法获得锁而被挂起, 因为锁已经被Thread-0持有). 在输出Thread-3 running ...后会暂停数秒(Thread-3无法获得锁而被挂起, 因为锁已经被Thread-2持有) synchronized方法和synchronized(this)的代码块，二者取得的是同一个锁，都是当前对象的实例 synchronized(processing)代码块，二者取得的锁是processing对象]]></content>
      <categories>
        <category>Java</category>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[内存溢出、OutOfMemoryError、StackOverflowError]]></title>
    <url>%2FJava%2FJVM%2F%E5%86%85%E5%AD%98%E6%BA%A2%E5%87%BA%E3%80%81OutOfMemoryError%E3%80%81StackOverflowError%2F</url>
    <content type="text"><![CDATA[虽然Java不需要开发人员显示的分配和回收内存，但了解JVM内存管理和回收机制，有助于我们在日常工作中排查各种内存溢出或泄露问题，解决性能瓶颈，达到更高的并发量，写出更高效的程序。本文重点介绍Java中的几个内存管理相关的异常 我们可以带着以下几个问题去学习自动内存管理机制： 什么操作可能导致内存溢出？ 有哪些种类的内存溢出？ 都是在内存的哪些区域溢出？ 运行时的数据区域Java虚拟机在执行Java程序的过程中会把它所管理的内存划分为若干个不同的数据区域，如下图所示 其中虚拟机栈、本地方法栈和程序计数器是线程私有的，方法区和堆是线程共享的. 本篇着重关注每个区域可能抛出的异常，对JVM中每个区域的描述，也可以参考Java内存管理及GC机制中的相关描述 程序计数器作用:当前线程所执行的字节码的行号指示器 字节码解释器工作时通过改变它的值来选取下一条需要执行的字节码指令 分支、循环、跳转、异常处理和线程恢复都依赖于它 虚拟机栈栈的作用：栈用于存储局部变量表、操作数栈、动态链接和方法出口等信息. 其中局部变量表用于存放8种基本数据类型(boolean，byte，char，short，int，float，long，double)和reference类型 reference类型： 指向对象起始地址的引用指针 指向一个代表对象的句柄 指向一条字节码指令的地址 可抛出两种异常状况： 线程请求的栈深度大于虚拟机所允许的栈深度，抛出StackOverflowError异常 当扩展时无法申请到足够的内存时会抛出OutOfMemoryError异常 本地方法栈与虚拟机栈的作用非常相似.其区别是虚拟机栈执行Java方法服务，而本地方法栈则为虚拟机使用到的Native方法服务 同时也会抛出StackOverflowError和OutOfMemoryError异常 堆堆的作用：分配所有的对象实例和数组 可以抛出OutOfMemoryError异常。 方法区方法区的作用:用于存储已被虚拟机加载的类信息(Class)、常量(final修饰)、静态变量(static)和即时编译器编译后的代码(code) 可以抛出OutOfMemoryError异常 运行时常量池属于方法区的一部分，用于存放编译期生成的各种字面量和符号引用，在类加载后存放到方法区的运行时常量池中。可抛出OutOfMemoryError异常 对象访问 参考Java内存管理及GC机制中的相关描述 OutOfMemoryError异常在Java虚拟机规范的描述中，除了程序计数器外，虚拟机内存的其他几个运行时区域都有发生OutOfMemoryError异常的可能。 下面通过若干实例来验证异常发生的场景。以下代码的开头都注释了执行时所需要设置的虚拟机启动参数，这些参数对实验结果有直接影响，请调试代码的时候不要忽略掉。 Java堆溢出堆里放的是new出来的对象，所以这部分很简单不断的new对象就可以了，但是为了防止对象new出来之后被GC，所以把对象new出来的对象放到一个List中去即可。为了有更好的效果，可以在运行前，调整堆的参数。1234567891011121314151617/** * author: winsky * date: 2018/3/10 * description:堆溢出 * VM Args: -Xms20m -Xmx20m -XX:+HeapDumpOnOutOfMemoryError */public class HeapOOM &#123; private static class OOMObject &#123; &#125; public static void main(String[] args) &#123; List&lt;OOMObject&gt; list = new ArrayList&lt;&gt;(); while (true) &#123; list.add(new OOMObject()); &#125; &#125;&#125; 输出结果：1234567891011java.lang.OutOfMemoryError: Java heap spaceDumping heap to java_pid949.hprof ...Heap dump file created [28011659 bytes in 0.165 secs]Exception in thread &quot;main&quot; java.lang.OutOfMemoryError: Java heap space at java.util.Arrays.copyOf(Arrays.java:3210) at java.util.Arrays.copyOf(Arrays.java:3181) at java.util.ArrayList.grow(ArrayList.java:261) at java.util.ArrayList.ensureExplicitCapacity(ArrayList.java:235) at java.util.ArrayList.ensureCapacityInternal(ArrayList.java:227) at java.util.ArrayList.add(ArrayList.java:458) at com.winsky.learn.jvm.HeapOOM.main(HeapOOM.java:19) 虚拟机栈溢出Java里面每个线程都有独立的、固定大小的栈空间， Java在解释执行的时候采用的是栈式的架构。 方法调用、方法内的局部变量都是在栈空间申请的。 空间的大小依赖于JDK版本，JDK1.6应该是512K，超过了这个空间就会产生StackOverflowError。 不断的递归会使栈空间不断增大导致溢出。 对于堆栈stack(或heap)来说，如果线程需要的空间大于允许值，则为StackOverflowError；如果stack空间可以动态增加，但最后内存还是不够，则为OutOfMemoryError。1234567891011121314151617181920212223/** * author: winsky * date: 2018/3/10 * description: */public class JavaVMStackSOF &#123; private int stackLength = 1; private void stackLeak() &#123; stackLength++; stackLeak(); &#125; public static void main(String[] args) &#123; JavaVMStackSOF oom = new JavaVMStackSOF(); try &#123; oom.stackLeak(); &#125; catch (Throwable e) &#123; System.out.println("Stack length:" + oom.stackLength); throw e; &#125; &#125;&#125; 输出结果：1234Exception in thread &quot;main&quot; java.lang.StackOverflowErrorStack length:18775 at com.winsky.learn.jvm.JavaVMStackSOF.stackLeak(JavaVMStackSOF.java:13) at com.winsky.learn.jvm.JavaVMStackSOF.stackLeak(JavaVMStackSOF.java:13) 方法区溢出最近看深入理解Java虚拟机，在实战OutOfMemoryError的运行时常量池溢出时，我的Intellij提示如下:12Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=10m; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=10m; support was removed in 8.0 具体描述可以参见Java8移除永久代 运行时常量池溢出同方法区溢出的道理，Java8之后也不会看到java.lang.OutOfMemoryError: PermGen space了]]></content>
      <categories>
        <category>Java</category>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>JVM</tag>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java8移除永久代]]></title>
    <url>%2FJava%2FJVM%2FJava8%E7%A7%BB%E9%99%A4%E6%B0%B8%E4%B9%85%E4%BB%A3%2F</url>
    <content type="text"><![CDATA[最近看深入理解Java虚拟机， 在实战OutOfMemoryError的运行时常量池溢出时， 我的Intellij提示如下:12Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=10m; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=10m; support was removed in 8.0 原书没有说会出现这个警告，所以上网详细查下相关资料，汇总如下。 在JDK1.7中，已经把原本放在永久代的字符串常量池移出，放在堆中。为什么这样做呢? 因为使用永久代来实现方法区不是个好主意， 很容易遇到内存溢出的问题。我们通常使用PermSize和MaxPermSize设置永久代的大小，这个大小就决定了永久代的上限，但是我们不是总是知道应该设置为多大的，如果使用默认值容易遇到OOM错误。 找下JDK1.8的Milestones 其中 JEP 122: Remove the Permanent Generation说的就是移除永久代。 文中说实现目标: 类的元数据，字符串池，类的静态变量将会从永久代移除，放入Java heap或者native memory。 其中建议JVM的实现中将类的元数据放入native memory 将字符串池和类的静态变量放入Java堆中。 这样可以加载多少类的元数据就不在由MaxPermSize控制，而由系统的实际可用空间来控制。 为什么这么做呢? 减少OOM只是表因 更深层的原因还是要合并HotSpot和JRockit的代码 JRockit从来没有一个叫永久代的东西，但是运行良好，也不需要开发运维人员设置这么一个永久代的大小。 当然不用担心运行性能问题了，在覆盖到的测试中，程序启动和运行速度降低不超过1%，但是这一点性能损失换来了更大的安全保障。 转自Java8移除永久代]]></content>
      <categories>
        <category>Java</category>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>JVM</tag>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线程的状态]]></title>
    <url>%2FJava%2FJava%E5%9F%BA%E7%A1%80%2F%E7%BA%BF%E7%A8%8B%E7%9A%84%E7%8A%B6%E6%80%81%2F</url>
    <content type="text"><![CDATA[并发编程是Java中重难点之一。本文介绍了Java中线程的各种状态，以及状态间相互转换图。 线程间的状态转换： 新建(new)：新创建了一个线程对象。 可运行(runnable)：线程对象创建后，其他线程(比如main线程）调用了该对象的start()方法。该状态的线程位于可运行线程池中，等待被线程调度选中，获取cpu 的使用权 。 运行(running)：可运行状态(runnable)的线程获得了cpu 时间片（timeslice） ，执行程序代码。 阻塞(block)：阻塞状态是指线程因为某种原因放弃了cpu 使用权，也即让出了cpu timeslice，暂时停止运行。直到线程进入可运行(runnable)状态，才有机会再次获得cpu timeslice 转到运行(running)状态。阻塞的情况分三种： 等待阻塞：运行(running)的线程执行o.wait()方法，JVM会把该线程放入等待队列(waitting queue)中。 同步阻塞：运行(running)的线程在获取对象的同步锁时，若该同步锁被别的线程占用，则JVM会把该线程放入锁池(lock pool)中。 其他阻塞：运行(running)的线程执行Thread.sleep(long ms)或t.join()方法，或者发出了I/O请求时，JVM会把该线程置为阻塞状态。当sleep()状态超时、join()等待线程终止或者超时、或者I/O处理完毕时，线程重新转入可运行(runnable)状态。 死亡(dead)：线程run()、main() 方法执行结束，或者因异常退出了run()方法，则该线程结束生命周期。死亡的线程不可再次复生。]]></content>
      <categories>
        <category>Java</category>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线程池原理]]></title>
    <url>%2FJava%2FJava%E5%9F%BA%E7%A1%80%2F%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[Java中的线程池是运用场景最多的并发框架，几乎所有需要异步或并发执行任务的程序都可以使用线程池。在开发过程中，合理地使用线程池能够带来3个好处。 降低资源消耗 提高响应速度 提高线程的可管理性 线程是稀缺资源，如果无限制地创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一分配、调优和监控。 线程池的实现原理 从图中可以看出，当提交一个新任务到线程池时，线程池的处理流程如下： 线程池判断核心线程池里的线程是否都在执行任务。如果不是，则创建一个新的工作线程来执行任务。如果核心线程池里的线程都在执行任务，则进入下个流程。 线程池判断工作队列是否已经满。如果工作队列没有满，则将新提交的任务存储在这个工作队列里。如果工作队列满了，则进入下个流程。 线程池判断线程池的线程是否都处于工作状态。如果没有，则创建一个新的工作线程来执行任务。如果已经满了，则交给饱和策略来处理这个任务。 线程池的状态123456// runState is stored in the high-order bitsprivate static final int RUNNING = -1 &lt;&lt; COUNT_BITS;private static final int SHUTDOWN = 0 &lt;&lt; COUNT_BITS;private static final int STOP = 1 &lt;&lt; COUNT_BITS;private static final int TIDYING = 2 &lt;&lt; COUNT_BITS;private static final int TERMINATED = 3 &lt;&lt; COUNT_BITS; RUNNING：可以接受新的任务，也可以处理阻塞队列里的任务 SHUTDOWN：不接受新的任务，但是可以处理阻塞队列里的任务 STOP：不接受新的任务，不处理阻塞队列里的任务，中断正在处理的任务 TIDYING：过渡状态，也就是说所有的任务都执行完了，当前线程池已经没有有效的线程，这个时候线程池的状态将会TIDYING，并且将要调用terminated方法 TERMINATED：终止状态。terminated方法调用完成以后的状态 线程池的创建我们可以通过ThreadPoolExecutor来创建一个线程池。12345678public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue) &#123; this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, Executors.defaultThreadFactory(), defaultHandler);&#125; 创建一个线程池时需要输入几个参数，如下： corePoolSize（线程池的基本大小）：当提交一个任务到线程池时，线程池会创建一个线程来执行任务，即使其他空闲的基本线程能够执行新任务也会创建线程，等到需要执行的任务数大于线程池基本大小时就不再创建。如果调用了线程池的prestartAllCoreThreads()方法，线程池会提前创建并启动所有基本线程。 runnableTaskQueue（任务队列）：用于保存等待执行的任务的阻塞队列。可以选择以下几个阻塞队列 ArrayBlockingQueue：是一个基于数组结构的有界阻塞队列，此队列按FIFO（先进先出）原则对元素进行排序。 LinkedBlockingQueue：一个基于链表结构的阻塞队列，此队列按FIFO排序元素，吞吐量通常要高于ArrayBlockingQueue。静态工厂方法Executors.newFixedThreadPool()使用了这个队列。 SynchronousQueue：一个不存储元素的阻塞队列。每个插入操作必须等到另一个线程调用移除操作，否则插入操作一直处于阻塞状态，吞吐量通常要高于LinkedBlockingQueue，静态工厂方法Executors.newCachedThreadPool使用了这个队列。 PriorityBlockingQueue：一个具有优先级的无限阻塞队列。 maximumPoolSize（线程池最大数量）：线程池允许创建的最大线程数。如果队列满了，并且已创建的线程数小于最大线程数，则线程池会再创建新的线程执行任务。值得注意的是，如果使用了无界的任务队列这个参数就没什么效果。 ThreadFactory用于设置创建线程的工厂，可以通过线程工厂给每个创建出来的线程设置更有意义的名字。 RejectedExecutionHandler（饱和策略）：当队列和线程池都满了，说明线程池处于饱和状态，那么必须采取一种策略处理提交的新任务。这个策略默认情况下是AbortPolicy，表示无法处理新任务时抛出异常。在JDK 1.5中Java线程池框架提供了以下4种策略。 AbortPolicy：直接抛出异常。 CallerRunsPolicy：只用调用者所在线程来运行任务。 DiscardOldestPolicy：丢弃队列里最近的一个任务，并执行当前任务。 DiscardPolicy：不处理，丢弃掉。 向线程池提交任务可以使用两个方法向线程池提交任务，分别为execute()和submit()方法。 execute()方法用于提交不需要返回值的任务，所以无法判断任务是否被线程池执行成功。通过以下代码可知execute()方法输入的任务是一个Runnable类的实例。 123456threadsPool.execute(new Runnable() &#123; @Override public void run() &#123; // TODO Auto-generated method stub &#125;&#125;); submit()方法用于提交需要返回值的任务。线程池会返回一个future类型的对象，通过这个future对象可以判断任务是否执行成功，并且可以通过future的get()方法来获取返回值，get()方法会阻塞当前线程直到任务完成，而使用get（long timeout，TimeUnit unit）方法则会阻塞当前线程一段时间后立即返回，这时候有可能任务没有执行完。 1234567891011Future&lt;Object&gt; future = executor.submit(harReturnValuetask); try &#123; Object s = future.get(); &#125; catch (InterruptedException e) &#123; // 处理中断异常 &#125; catch (ExecutionException e) &#123; // 处理无法执行任务异常 &#125; finally &#123; // 关闭线程池 executor.shutdown(); &#125; 关闭线程池可以通过调用线程池的shutdown或shutdownNow方法来关闭线程池。它们的原理是遍历线程池中的工作线程，然后逐个调用线程的interrupt方法来中断线程，所以无法响应中断的任务可能永远无法终止。但是它们存在一定的区别，shutdownNow首先将线程池的状态设置成STOP，然后尝试停止所有的正在执行或暂停任务的线程，并返回等待执行任务的列表，而shutdown只是将线程池的状态设置成SHUTDOWN状态，然后中断所有没有正在执行任务的线程。 只要调用了这两个关闭方法中的任意一个，isShutdown方法就会返回true。当所有的任务都已关闭后，才表示线程池关闭成功，这时调用isTerminaed方法会返回true。至于应该调用哪一种方法来关闭线程池，应该由提交到线程池的任务特性决定，通常调用shutdown方法来关闭线程池，如果任务不一定要执行完，则可以调用shutdownNow方法。]]></content>
      <categories>
        <category>Java</category>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线程interrupt、interrupted 、isInterrupted 区别]]></title>
    <url>%2FJava%2FJava%E5%9F%BA%E7%A1%80%2F%E7%BA%BF%E7%A8%8Binterrupt%E3%80%81interrupted%20%E3%80%81isInterrupted%20%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[Java线程中跟中断线程有关的函数有三个，分别是interrupt、interrupted和isInterrupted。本文介绍了三种方法的作用，并介绍了他们之间的区别。 interruptinterrupt方法用于中断线程。调用该方法的线程的状态为将被置为”中断”状态。 注意：线程中断仅仅是置线程的中断状态位，不会停止线程。需要用户自己去监视线程的状态为并做处理。支持线程中断的方法（也就是线程中断后会抛出interruptedException的方法）就是在监视线程的中断状态，一旦线程的中断状态被置为“中断状态”，就会抛出中断异常。 interrupted 和 isInterrupted首先看一下该方法的实现：123public static boolean interrupted () &#123; return currentThread().isInterrupted(true);&#125; 该方法就是直接调用当前线程的isInterrupted(true)方法。 然后再来看一下isInterrupted的实现：123public boolean isInterrupted () &#123; return isInterrupted( false);&#125; 这两个方法有两个主要区别： interrupted 是作用于当前线程，isInterrupted 是作用于调用该方法的线程对象所对应的线程。（线程对象对应的线程不一定是当前运行的线程。例如我们可以在A线程中去调用B线程对象的isInterrupted方法。真实对象，比如这里的B线程对象） 这两个方法最终都会调用同一个方法，只不过参数一个是true，一个是false 第二个区别主要体现在调用的方法的参数上，让我们来看一看这个参数是什么含义 先来看一看被调用的方法 isInterrupted(boolean arg)的定义：1private native boolean isInterrupted( boolean ClearInterrupted); 原来这是一个本地方法，看不到源码。不过没关系，通过参数名我们就能知道，这个参数代表是否要清除状态位。 如果这个参数为true，说明返回线程的状态位后，要清掉原来的状态位（恢复成原来情况）。 这个参数为false，就是直接返回线程的状态位。 这两个方法很好区分，只有当前线程才能清除自己的中断位（对应interrupted()方法）]]></content>
      <categories>
        <category>Java</category>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDK各版本区别(简明版)]]></title>
    <url>%2FJava%2FJVM%2FJDK%E5%90%84%E7%89%88%E6%9C%AC%E5%8C%BA%E5%88%AB(%E7%AE%80%E6%98%8E%E7%89%88)%2F</url>
    <content type="text"><![CDATA[Java是一种广泛使用的计算机编程语言，拥有跨平台、面向对象、泛型编程的特性，广泛应用于企业级Web应用开发和移动应用开发。 1995年5月23日，Java语言诞生。1996年1月，第一个JDK-JDK1.0诞生，到现在，已经有了JDK8了，甚至前一段时间连JDK9都出来了。 本文简单介绍了最近几代JDK的各版本中引入的重要新特性。 JDK 5 自动装箱与拆箱： 枚举 静态导入 可变参数 内省 泛型 For-Each循环 JDK 6 Desktop类和SystemTray类 使用JAXB2来实现对象与XML之间的映射 理解StAX 使用Compiler API 轻量级Http Server API 插入式注解处理API(Pluggable Annotation Processing API) 用Console开发控制台程序 对脚本语言的支持如: ruby, groovy, javascript. Common Annotations Web服务元数据 JTable的排序和过滤 更简单,更强大的JAX-WS 嵌入式数据库 Derby JDK 7 对集合（Collections）的增强支持 在Switch中可用String 数值可加下划线 例如：int one_million = 1_000_000; 支持二进制文字 例如：int binary = 0b1001_1001; 简化了可变参数方法的调用 运用List tempList = new ArrayList&lt;&gt;(); 即泛型实例化类型自动推断 语法上支持集合，而不一定是数组 新增一些取环境信息的工具方法 Boolean类型反转，空指针安全,参与位运算 两个char间的equals 安全的加减乘除 map集合支持并发请求，且可以写成 Map map = {name:”xxx”,age:18}; JDK 8 允许在接口中有默认方法和静态方法实现 常量 抽象方法 默认方法 静态方法 Lambda 表达式 函数式接口 @FunctionalInterface 方法与构造函数引用 使用 ::关键字来传递方法或者构造函数引用 1bomDetail.forEach(System.out::println); java.util.stream 支持多重注解 IO/NIO 的改进 安全性上的增强 新的日期/时间API java.time JDK 9 模块化系统 JShell 平台日志API和服务:System.LoggerFinder用来管理JDK使用的日志记录器实现 统一 JVM 日志，可以使用新的命令行选项-Xlog 来控制 JVM 上 所有组件的日志记录 CMS垃圾回收器已经被声明为废弃 Java 9 允许在接口中使用私有方法和私有静态方法 常量 抽象方法 默认方法 静态方法 私有方法 私有静态方法]]></content>
      <categories>
        <category>Java</category>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>JDK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java内存管理及GC机制]]></title>
    <url>%2FJava%2FJVM%2FJava%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E5%8F%8AGC%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[Java GC（Garbage Collection，垃圾收集，垃圾回收）机制，是Java与C++/C的主要区别之一。作为Java开发者，一般不需要专门编写内存回收和垃圾清理代码，对内存泄露和溢出的问题，也不需要像C程序员那样战战兢兢。经过这么长时间的发展，Java GC机制已经日臻完善，几乎可以自动的为我们做绝大多数的事情。 虽然Java不需要开发人员显示的分配和回收内存，这对开发人员确实降低了不少编程难度，但也可能带来一些副作用： 有可能不知不觉浪费了很多内存 JVM花费过多时间来进行内存回收 内存泄露 因此，作为一名Java编程人员，必须学会JVM内存管理和回收机制，这可以帮助我们在日常工作中排查各种内存溢出或泄露问题，解决性能瓶颈，达到更高的并发量，写出更高效的程序。 Java内存管理根据JVM规范，JVM把内存划分了如下几个区域： 方法区 堆区 本地方法栈 虚拟机栈 程序计数器 其中，方法区和堆是所有线程共享的。 方法区方法区存放了要加载的类的信息(如类名，修饰符)、类中的静态变量、final定义的常量、类中的field、方法信息，当开发人员调用类对象中的getName、isInterface等方法来获取信息时，这些数据都来源于方法区。 方法区是全局共享的，在一定条件下它也会被GC。当方法区使用的内存超过它允许的大小时，就会抛出OutOfMemory：PermGen Space异常。 在Hotspot虚拟机中，这块区域对应的是Permanent Generation(持久代)，一般的，方法区上执行的垃圾收集是很少的，因此方法区又被称为持久代的原因之一，但这也不代表着在方法区上完全没有垃圾收集，其上的垃圾收集主要是针对常量池的内存回收和对已加载类的卸载。在方法区上进行垃圾收集，条件苛刻而且相当困难，关于其会后面再介绍。 运行时常量池（Runtime Constant Pool）是方法区的一部分，用于存储编译期就生成的字面常量、符号引用、翻译出来的直接引用（符号引用就是编码是用字符串表示某个变量、接口的位置，直接引用就是根据符号引用翻译出来的地址，将在类链接阶段完成翻译）； 运行时常量池除了存储编译期常量外，也可以存储在运行时间产生的常量，比如String类的intern()方法，作用是String维护了一个常量池，如果调用的字符abc已经在常量池中，则返回池中的字符串地址，否则，新建一个常量加入池中，并返回地址。 堆区堆区是理解JavaGC机制最重要的区域。 在JVM所管理的内存中，堆区是最大的一块，堆区也是JavaGC机制所管理的主要内存区域，堆区由所有线程共享，在虚拟机启动时创建。堆区用来存储对象实例及数组值，可以认为Java中所有通过new创建的对象都在此分配。 对于堆区大小，可以通过参数-Xms和-Xmx来控制 -Xms为JVM启动时申请的最小Heap内存，默认为物理内存的1/64但小于1GB; -Xmx为JVM可申请的最大Heap内存，默认为物理内存的1/4但小于1GB 默认当剩余堆空间小于40%时，JVM会增大Heap到-Xmx大小，可通过-XX:MinHeapFreeRadio参数来控制这个比例； 当空余堆内存大于70%时，JVM会减小Heap大小到-Xms指定大小，可通过-XX:MaxHeapFreeRatio来指定这个比例 对于系统而言，为了避免在运行期间频繁的调整Heap大小，我们通常将-Xms和-Xmx设置成一样。 为了让内存回收更加高效（后面会具体讲为何要分代划分），从Sun JDK 1.2开始对堆采用了分代管理方式，如下图所示： 年轻代对象在被创建时，内存首先是在年轻代进行分配（注意，大对象可以直接在老年代分配）。当年轻代需要回收时会触发Minor GC(也称作Young GC)。 年轻代由Eden Space和两块相同大小的Survivor Space（又称S0和S1）构成，可通过-Xmn参数来调整新生代大小，也可通过-XX:SurvivorRadio来调整Eden Space和Survivor Space大小。 不同的GC方式会按不同的方式来按此值划分Eden Space和Survivor Space，有些GC方式还会根据运行状况来动态调整Eden、S0、S1的大小。 年轻代的Eden区内存是连续的，所以其分配会非常快；同样Eden区的回收也非常快（因为大部分情况下Eden区对象存活时间非常短，而Eden区采用的复制回收算法，此算法在存活对象比例很少的情况下非常高效，后面会详细介绍）。 如果在执行垃圾回收之后，仍没有足够的内存分配，也不能再扩展，将会抛出OutOfMemoryError:Java Heap Space异常。 老年代老年代用于存放在年轻代中经多次垃圾回收仍然存活的对象，可以理解为比较老一点的对象，例如缓存对象。 新建的对象也有可能在老年代上直接分配内存，这主要有两种情况： 一种为大对象，可以通过启动参数设置-XX:PretenureSizeThreshold=1024，表示超过多大时就不在年轻代分配，而是直接在老年代分配。此参数在年轻代采用Parallel Scavenge GC时无效，因为其会根据运行情况自己决定什么对象直接在老年代上分配内存； 另一种为大的数组对象，且数组对象中无引用外部对象。 当老年代满了的时候就需要对老年代进行垃圾回收，老年代的垃圾回收称作Major GC（也称作Full GC）。 老年代所占用的内存大小为-Xmx对应的值减去-Xmn对应的值。 本地方法栈本地方法栈用于支持native方法的执行，存储了每个native方法调用的状态。 本地方法栈和虚拟机方法栈运行机制一致，它们唯一的区别就是，虚拟机栈是执行Java方法的，而本地方法栈是用来执行native方法的，在很多虚拟机中（如Sun的JDK默认的HotSpot虚拟机），会将本地方法栈与虚拟机栈放在一起使用。 虚拟机栈虚拟机栈占用的是操作系统内存，每个线程都对应着一个虚拟机栈，它是线程私有的，而且分配非常高效。一个线程的每个方法在执行的同时，都会创建一个栈帧（Statck Frame），栈帧中存储的有局部变量表、操作栈、动态链接、方法出口等，当方法被调用时，栈帧在JVM栈中入栈，当方法执行完成时，栈帧出栈。 局部变量表中存储着方法的相关局部变量，包括各种基本数据类型，对象的引用，返回地址等。 在局部变量表中，只有long和double类型会占用2个局部变量空间（Slot，对于32位机器，一个Slot就是32个bit），其它都是1个Slot。 需要注意的是，局部变量表是在编译时就已经确定好的，方法运行所需要分配的空间在栈帧中是完全确定的，在方法的生命周期内都不会改变。 虚拟机栈中定义了两种异常，如果线程调用的栈深度大于虚拟机允许的最大深度，则抛出StatckOverFlowError（栈溢出）；不过多数Java虚拟机都允许动态扩展虚拟机栈的大小(有少部分是固定长度的)，所以线程可以一直申请栈，直到内存不足，此时，会抛出OutOfMemoryError（内存溢出）。 程序计数器程序计数器是一个比较小的内存区域，可能是CPU寄存器或者操作系统内存，其主要用于指示当前线程所执行的字节码执行到了第几行，可以理解为是当前线程的行号指示器。字节码解释器在工作时，会通过改变这个计数器的值来取下一条语句指令。每个程序计数器只用来记录一个线程的行号，所以它是线程私有（一个线程就有一个程序计数器）的。 如果程序执行的是一个Java方法，则计数器记录的是正在执行的虚拟机字节码指令地址；如果正在执行的是一个本地（native，由C语言编写完成）方法，则计数器的值为Undefined，由于程序计数器只是记录当前指令地址，所以不存在内存溢出的情况，因此，程序计数器也是所有JVM内存区域中唯一一个没有定义OutOfMemoryError的区域。 Java对象访问方式一般来说，一个Java的引用访问涉及到3个内存区域：JVM栈，堆，方法区。以最简单的本地变量引用：Object objRef = new Object()为例： Object objRef表示一个本地引用，存储在JVM栈的本地变量表中，表示一个reference类型数据； new Object()作为实例对象数据存储在堆中； 堆中还记录了能够查询到此Object对象的类型数据（接口、方法、field、对象类型等）的地址，实际的数据则存储在方法区中； 在Java虚拟机规范中，只规定了指向对象的引用，对于通过reference类型引用访问具体对象的方式并未做规定，不过目前主流的实现方式主要有两种 通过句柄访问通过句柄访问的实现方式中，JVM堆中会划分单独一块内存区域作为句柄池，句柄池中存储了对象实例数据（在堆中）和对象类型数据（在方法区中）的指针。这种实现方法由于用句柄表示地址，因此十分稳定。 通过直接指针访问通过直接指针访问的方式中，reference中存储的就是对象在堆中的实际地址，在堆中存储的对象信息中包含了在方法区中的相应类型数据。这种方法最大的优势是速度快，在HotSpot虚拟机中用的就是这种方式。 JVM内存分配Java对象所占用的内存主要在堆上实现，因为堆是线程共享的，因此在堆上分配内存时需要进行加锁，这就导致了创建对象的开销比较大。当堆上空间不足时，会出发GC，如果GC后空间仍然不足，则会抛出OutOfMemory异常。 为了提升内存分配效率，在年轻代的Eden区HotSpot虚拟机使用了两种技术来加快内存分配 ，分别是bump-the-pointer和TLAB（Thread-Local Allocation Buffers）。 由于Eden区是连续的，因此bump-the-pointer技术的核心就是跟踪最后创建的一个对象，在对象创建时，只需要检查最后一个对象后面是否有足够的内存即可，从而大大加快内存分配速度； 而对于TLAB技术是对于多线程而言的， 它会为每个新创建的线程在新生代的Eden Space上分配一块独立的空间，这块空间称为TLAB（Thread Local Allocation Buffer），其大小由JVM根据运行情况计算而得。可通过-XX:TLABWasteTargetPercent来设置其可占用的Eden Space的百分比，默认是1%。在TLAB上分配内存不需要加锁，一般JVM会优先在TLAB上分配内存，如果对象过大或者TLAB空间已经用完，则仍然在堆上进行分配。因此，在编写程序时，多个小对象比大的对象分配起来效率更高。可在启动参数上增加-XX:+PrintTLAB来查看TLAB空间的使用情况。 对象如果在年轻代存活了足够长的时间而没有被清理掉（即在几次Minor GC后存活了下来），则会被复制到年老代，年老代的空间一般比年轻代大，能存放更多的对象，在年老代上发生的GC次数也比年轻代少。当年老代内存不足时，将执行Major GC，也叫 Full GC。 可以使用-XX:+UseAdaptiveSizePolicy开关来控制是否采用动态控制策略，如果动态控制，则动态调整Java堆中各个区域的大小以及进入老年代的年龄。 如果对象比较大（比如长字符串或大数组），年轻代空间不足，则大对象会直接分配到老年代上（大对象可能触发提前GC，应少用，更应避免使用短命的大对象）。用-XX:PretenureSizeThreshold来控制直接升入老年代的对象大小，大于这个值的对象会直接分配在老年代上。 内存的回收方式JVM通过GC来回收堆和方法区中的内存，这个过程是自动执行的。 说到Java GC机制，其主要完成3件事 确定哪些内存需要回收； 确定什么时候需要执行GC； 如何执行GC。 JVM主要采用收集器的方式实现GC，主要的收集器有引用计数收集器和跟踪收集器。 引用收集器引用计数器采用分散式管理方式，通过计数器记录对象是否被引用。当计数器为0时，说明此对象已经不再被使用，可进行回收，如图所示：在上图中，ObjectA释放了对ObjectB的引用后，ObjectB的引用计数器变为0，此时可回收ObjectB所占有的内存。 引用计数器需要在每次对象赋值时进行引用计数器的增减，他有一定消耗。 另外，引用计数器对于循环引用的场景没有办法实现回收。例如在上面的例子中，如果ObjectB和ObjectC互相引用，那么即使ObjectA释放了对ObjectB和ObjectC的引用，也无法回收ObjectB、ObjectC 因此对于Java这种会形成复杂引用关系的语言而言，引用计数器是非常不适合的，SunJDK在实现GC时也未采用这种方式。 跟踪收集器跟踪收集器采用的是集中式的管理方式，会全局记录数据引用的状态。基于一定条件的触发（例如定时、空间不足时），执行时需要从根集合来扫描对象的引用关系，这可能会造成应用程序暂停。 根集合元素：简单来讲，就是全局性的引用（常量和静态属性）和栈引用 方法区中：常量+静态变量 Java栈中的对象引用（存在于局部变量表中，注意，局部变量表中存放的是基本数据类型和对象引用） 传到本地方法中，还没有被本地方法释放的对象引用 主要有复制（Copying）、标记-清除(Mark-Sweep) 和 标记-压缩(Mark-Compact) 三种实现算法。 复制复制采用的方式为从根集合扫描出存活的对象，并将找到的存活的对象复制到一块新的完全未被使用的空间中，如图所示： 复制收集器方式仅需要从根集合扫描所有存活对象，当要回收的空间中存活对象较少时，复制算法会比较高效（年轻代的Eden区就是采用这个算法），其带来的成本是要增加一块空的内存空间及进行对象的移动。 标记 - 清除标记-清除采用的方式为从根集合开始扫描，对存活的对象进行标记，标记完毕后，再扫描整个空间中未标记的对象，并进行清除，标记和清除过程如下图所示： 上图中蓝色的部分是有被引用的存活的对象，褐色部分没被引用的可回收的对象。在marking阶段为了mark对象，所有的对象都会被扫描一遍，扫描这个过程是比较耗时的。 清除阶段回收的是没有被引用的对象，存活的对象被保留。内存分配器会持有空闲空间的引用列表，当有分配请求时会查询空闲空间引用列表进行分配。 标记-清除动作不需要进行对象移动，且仅对其不存活的对象进行处理。在空间中存活对象较多的情况下较为高效，但由于标记-清除直接回收不存活对象占用的内存，因此会造成内存碎片。 标记 - 压缩标记-压缩和标记-清除一样，是对活的对象进行标记，但是在清除后的处理不一样，标记-压缩在清除对象占用的内存后，会把所有活的对象向左端空闲空间移动，然后再更新引用其对象的指针，如下图所示： 很明显，标记-压缩在标记-清除的基础上对存活的对象进行了移动规整动作，解决了内存碎片问题，得到更多连续的内存空间以提高分配效率，但由于需要对对象进行移动，因此成本也比较高。 虚拟机中的GC过程为什么要分代回收在一开始的时候，JVM的GC就是采用标记-清除-压缩方式进行的，这么做并不是很高效，因为当对象分配的越来越多时，对象列表也越来也大，扫描和移动越来越耗时，造成了内存回收越来越慢。 然而，经过对Java应用的分析，发现大部分对象的存活时间都非常短，只有少部分数据存活周期是比较长的。 虚拟机中GC的过程经过上面介绍，我们已经知道了JVM为何要分代回收，下面我们就详细看一下整个回收过程。 年轻代的GC过程 在初始阶段，新创建的对象被分配到Eden区，survivor的两块空间都为空。 当Eden区满了的时候，minor garbage被触发 经过扫描与标记，存活的对象被复制到S0，不存活的对象被回收 在下一次的Minor GC中，Eden区的情况和上面一致，没有引用的对象被回收，存活的对象被复制到survivor区。然而在survivor区，S0的所有的数据都被复制到S1，需要注意的是，在上次minor GC过程中移动到S0中的两个对象在复制到S1后其年龄要加1。此时Eden区S0区被清空，所有存活的数据都复制到了S1区，并且S1区存在着年龄不一样的对象，过程如下图所示： 再下一次MinorGC则重复这个过程，这一次survivor的两个区对换，存活的对象被复制到S0，存活的对象年龄加1，Eden区和另一个survivor区被清空。 下面演示一下Promotion过程，在经过几次Minor GC之后，当存活对象的年龄达到一个阈值之后（可通过参数配置，默认是8），就会被从年轻代Promotion到老年代。 随着MinorGC一次又一次的进行，不断会有新的对象被promote到老年代 上面基本上覆盖了整个年轻代所有的回收过程。最终，MajorGC将会在老年代发生，老年代的空间将会被清除和压缩 从上面的过程可以看出，Eden区是连续的空间，且Survivor总有一个为空。经过一次GC和复制，一个Survivor中保存着当前还活着的对象，而Eden区和另一个Survivor区的内容都不再需要了，可以直接清空，到下一次GC时，两个Survivor的角色再互换。 因此，这种方式分配内存和清理内存的效率都极高，这种垃圾回收的方式就是著名的“停止-复制（Stop-and-copy）”清理法（将Eden区和一个Survivor中仍然存活的对象拷贝到另一个Survivor中），这不代表着停止复制清理法很高效，其实，它也只在这种情况下（基于大部分对象存活周期很短的事实）高效，如果在老年代采用停止复制，则是非常不合适的。 老年代的GC过程老年代存储的对象比年轻代多得多，而且不乏大对象，对老年代进行内存清理时，如果使用停止-复制算法，则相当低效。 一般，老年代用的算法是标记-压缩算法，即：标记出仍然存活的对象（存在引用的），将所有存活的对象向一端移动，以保证内存的连续。在发生Minor GC时，虚拟机会检查每次晋升进入老年代的大小是否大于老年代的剩余空间大小，如果大于，则直接触发一次Full GC，否则，就查看是否设置了-XX:+HandlePromotionFailure（允许担保失败），如果允许，则只会进行MinorGC，此时可以容忍内存分配失败；如果不允许，则仍然进行Full GC（这代表着如果设置-XX:+Handle PromotionFailure，则触发MinorGC就会同时触发Full GC，哪怕老年代还有很多内存，所以，最好不要这样做）。 永久代的GC过程关于方法区即永久代的回收，永久代的回收有两种： 常量池中的常量 常量的回收很简单，没有引用了就可以被回收 无用的类信息 对于无用的类进行回收，必须保证3点： 类的所有实例都已经被回收 加载类的ClassLoader已经被回收 类对象的Class对象没有被引用（即没有通过反射引用该类的地方） 永久代的回收并不是必须的，可以通过参数来设置是否对类进行回收。 垃圾收集器通过上面的介绍，我们已经了解到了JVM的内存回收过程，而在虚拟机中，GC是由垃圾回收器来具体执行的，所以，在实际应用场景中我们需要根据应用情况选择合适的垃圾收集器，下面我们就介绍一下垃圾收集器。 串行（Serial）收集器串行收集器Java SE5 和6中客户端虚拟机所采用的默认配置，它是最简单的收集器，比较适合于只有一个处理器的系统。在串行收集器中，minor和major GC过程都是用一个线程进行垃圾回收。 使用场景首先，串行GC一般用在对应用暂停要求不是很高和运行在客户端模式的场景，它仅仅利用一个CPU核心来进行垃圾回收。在现在的硬件条件下，串行GC可以管理很多小内存的应用，并且能够保证相对较小的暂停（在Full GC的情况下大约需要几秒的时间）。 另一个通常采用串行GC的场景就是一台机器运行多个JVM虚拟机的情况（JVM虚拟机个数大于CPU核心数），在这种场景下，当一个JVM进行垃圾回收时只利用一个处理器，不会对其它JVM造成较大的影响。 最后，在一些内存比较小和CPU核心数比较少的硬件设备中也比较适合采用串行收集器。 相关命令参数启用串行收集器：-XX:+UseSerialGC1java -Xmx12m -Xms3m -Xmn1m -XX:PermSize=20m -XX:MaxPermSize=20m -XX:+UseSerialGC -jar c:\demo.jar 并行收集器并行收集器采用多线程的方式来进行垃圾回收，采用并行的方式能够带来极大的CPU吞吐量。它在不进行垃圾回收的时候对正在运行的应用程序没有任何影响，在进程GC的时候采用多线程的方式来提高回收速度。 因此，并行收集器非常适用于批处理的情形。当然，如果应用对程序暂停要求很高的话，建议采用下面介绍的并发收集器。 默认一个N个cpu的机器上，并行回收的线程数为N。当然，并行的数量可以通过参数进行控制：-XX:ParallelGCThreads=&lt;desired number&gt;。 并行收集器是Server级别机器（CPU大于2且内存大于2G）上采用的默认回收方式。 在单核CPU的机器上，即使配置了并行收集器，实际回收时仍然采用的是默认收集器。如果一台机器上只有两个CPU，采用并行回收器和默认回收器的效果其实差不多，只有当CPU个数大于2时，年轻代回收的暂停时间才会减少。 应用场景并行回收器适用于多CPU、对暂停时间要求短的情况下。通常，一些批处理的应用如报告打印、数据库查询可采用并行收集器。 相关命令参数 在年轻代用多线程、老年代用单线程 启用命令：-XX:+UseParallelGC 1java -Xmx12m -Xms3m -Xmn1m -XX:PermSize=20m -XX:MaxPermSize=20m -XX:+UseParallelGC -jar c:\demo.jar 年轻代和老年代都用多线程 启用命令：-XX:+UseParallelOldGC 当启用-XX:+UseParallelOldGC 选项时,年轻代和老年代的垃圾收集都会用多线程进行，在压缩阶段也是多线程。因为HotSpot虚拟机在年轻代采用的是停止-复制算法，年轻代没有压缩过程，而老年代采用的是标记-清除-压缩算法，所以仅在老年代有compact过程。 1java -Xmx12m -Xms3m -Xmn1m -XX:PermSize=20m -XX:MaxPermSize=20m -XX:+UseParallelOldGC -jar c:\demo.jar CMS（Concurrent Mark Sweep）收集器CMS收集器试图用多线程并发的形式来减少垃圾收集过程中的暂停。CMS收集器不会对存活的对象进行复制或移动。CMS采用的基础算法是标记 - 清除 CMS过程 初始标记(STW initial mark) 并发标记(Concurrent marking) 并发预清理(Concurrent precleaning) 重新标记(STW remark) 并发清理(Concurrent sweeping) 并发重置(Concurrent reset) 初始标记：在这个阶段，需要虚拟机停顿正在执行的任务，官方的叫法STW(Stop The Word)。这个过程从垃圾回收的”根对象”开始，只扫描到能够和”根对象”直接关联的对象，并作标记。所以这个过程虽然暂停了整个JVM，但是很快就完成了。 并发标记：这个阶段紧随初始标记阶段，在初始标记的基础上继续向下追溯标记。并发标记阶段，应用程序的线程和并发标记的线程并发执行，所以用户不会感受到停顿。 并发预清理：并发预清理阶段仍然是并发的。在这个阶段，虚拟机查找在执行并发标记阶段新进入老年代的对象(可能会有一些对象从新生代晋升到老年代， 或者有一些对象被分配到老年代)。通过重新扫描，减少下一个阶段”重新标记”的工作，因为下一个阶段会Stop The World。 重新标记：这个阶段会暂停虚拟机，收集器线程扫描在CMS堆中剩余的对象。扫描从”根对象”开始向下追溯，并处理对象关联。 并发清理：清理垃圾对象，这个阶段收集器线程和应用程序线程并发执行。 并发重置：这个阶段，重置CMS收集器的数据结构，等待下一次垃圾回收。 CMS缺点 CMS回收器采用的基础算法是Mark-Sweep。所有CMS不会整理、压缩堆空间。这样就会有一个问题：经过CMS收集的堆会产生空间碎片。CMS不对堆空间整理压缩节约了垃圾回收的停顿时间，但也带来的堆空间的浪费。为了解决堆空间浪费问题，CMS回收器不再采用简单的指针指向一块可用堆空间来为下次对象分配使用。而是把一些未分配的空间汇总成一个列表，当JVM分配对象空间的时候，会搜索这个列表找到足够大的空间来hold住这个对象。 需要更多的CPU资源。为了让应用程序不停顿，CMS线程和应用程序线程并发执行，这样就需要有更多的CPU，单纯靠线程切换是不靠谱的。并且，重新标记阶段，为空保证STW快速完成，也要用到更多的甚至所有的CPU资源。当然，多核多CPU也是未来的趋势！ CMS的另一个缺点是它需要更大的堆空间。因为CMS标记阶段应用程序的线程还是在执行的，那么就会有堆空间继续分配的情况，为了保证在CMS回收完堆之前还有空间分配给正在运行的应用程序，必须预留一部分空间。也就是说，CMS不会在老年代满的时候才开始收集。相反，它会尝试更早的开始收集，已避免上面提到的情况：在回收完成之前，堆没有足够空间分配！默认当老年代使用68%的时候，CMS就开始行动了。– XX:CMSInitiatingOccupancyFraction =n来设置这个阀值。 总得来说，CMS回收器减少了回收的停顿时间，但是降低了堆空间的利用率。 应用场景CMS收集器主要用在应用程序对暂停时间要求很高的场景，比如桌面UI应用需要及时响应用户操作事件、服务器必须能快速响应客户端请求或者数据库要快速响应查询请求等等。 相关命令参数启用CMS收集器：-XX:+UseConcMarkSweepGC 设置线程数：-XX:ParallelCMSThreads=&lt;n&gt;1java -Xmx12m -Xms3m -Xmn1m -XX:PermSize=20m -XX:MaxPermSize=20m -XX:+UseConcMarkSweepGC -XX:ParallelCMSThreads=2 -jar c:\demo.jar G1收集器G1即Garbage First，它是在Java 7中出现的新的收集器，它的目标是替换掉现有的CMS收集器（产生于JDK 5）。G1具有并行、并发、分代收集、空间整合、可预测的停顿等特点。 并行与并发 G1能充分利用多CPU、多核环境下的硬件优势，使用多个CPU来缩短Stop-The-World停顿的时间，部分其他收集器原本需要停顿Java线程执行的GC动作，G1收集器仍然可以通过并发的方式让Java程序继续执行。 分代收集 与其他收集器一样，分代概念在G1中依然得以保留。虽然G1可以不需要其他收集器配合就能独立管理整个GC堆，但它能够采用不同的方式去处理新创建的对象和已经存活了一段时间、熬过多次GC的旧对象以获取更好的收集效果。 空间整合 与CMS的“标记—清理”算法不同，G1从整体来看是基于“标记—整理”算法实现的收集器，从局部（两个Region之间）上来看是基于“复制”算法实现的，但无论如何，这两种算法都意味着G1运作期间不会产生内存空间碎片，收集后能提供规整的可用内存。这种特性有利于程序长时间运行，分配大对象时不会因为无法找到连续内存空间而提前触发下一次GC。 可预测的停顿 这是G1相对于CMS的另一大优势，降低停顿时间是G1和CMS共同的关注点，但G1除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为M毫秒的时间片段内，消耗在垃圾收集上的时间不得超过N毫秒。 在G1之前的其他收集器进行收集的范围都是整个新生代或者老年代，而G1不再是这样。使用G1收集器时，Java堆的内存布局就与其他收集器有很大差别，它将整个Java堆划分为多个大小相等的独立区域（Region），虽然还保留有新生代和老年代的概念，但新生代和老年代不再是物理隔离的了，它们都是一部分Region（不需要连续）的集合。 G1收集器之所以能建立可预测的停顿时间模型，是因为它可以有计划地避免在整个Java堆中进行全区域的垃圾收集。G1跟踪各个Region里面的垃圾堆积的价值大小（回收所获得的空间大小以及回收所需时间的经验值），在后台维护一个优先列表，每次根据允许的收集时间，优先回收价值最大的Region（这也就是Garbage-First名称的来由）。这种使用Region划分内存空间以及有优先级的区域回收方式，保证了G1收集器在有限的时间内可以获取尽可能高的收集效率。 执行过程 初始标记（Initial Marking） 并发标记（Concurrent Marking） 最终标记（Final Marking） 筛选回收（Live Data Counting and Evacuation） 初始标记（Initial Marking）：初始标记阶段仅仅只是标记一下GC Roots能直接关联到的对象，并且修改TAMS（Next Top at Mark Start）的值，让下一阶段用户程序并发运行时，能在正确可用的Region中创建新对象，这阶段需要停顿线程，但耗时很短。 并发标记（Concurrent Marking）：并发标记阶段是从GC Root开始对堆中对象进行可达性分析，找出存活的对象，这阶段耗时较长，但可与用户程序并发执行。 最终标记（Final Marking）：最终标记阶段是为了修正在并发标记期间因用户程序继续运作而导致标记产生变动的那一部分标记记录，虚拟机将这段时间对象变化记录在线程Remembered Set Logs里面，最终标记阶段需要把Remembered Set Logs的数据合并到Remembered Set中，这阶段需要停顿线程，但是可并行执行。 筛选回收（Live Data Counting and Evacuation）：筛选回收阶段首先对各个Region的回收价值和成本进行排序，根据用户所期望的GC停顿时间来制定回收计划，这个阶段其实也可以做到与用户程序一起并发执行，但是因为只回收一部分Region，时间是用户可控制的，而且停顿用户线程将大幅提高收集效率。 相关命令参数启用G1收集器：-XX:+UseG1GC1java -Xmx12m -Xms3m -XX:+UseG1GC -jar c:\demo.jar JVM 参数汇总GC 优化配置 配置 描述 -Xms 初始化堆内存大小 -Xmm 堆内存最大值 -Xmn 新生代大小 -XX:PermSize 初始化永久代大小 -XX:MaxPermSize 永久代最大容量 GC 类型设置 配置 描述 -XX:+UseSerialGC 串行垃圾收集器 -XX:+UseParallelGC 并行垃圾收集器 -XX:+UseConcMarkSweepGC 并发标记扫描垃圾收集器 -XX:+UseG1GC G1垃圾收集器 -XX:ParallelCMSThreads= 并发标记扫描垃圾回收器 = 为使用的线程数量 垃圾回收统计信息 配置 描述 -XX:+PrintGC 每次GC时打印相关信息 -XX:+PrintGCDetails 每次GC时打印详细信息 -XX:+PrintGCTimeStamps 打印每次GC的时间戳 -XX:+PrintHeapAtGC 打印GC前后的详细堆栈信息 Minor GC、Full GC触发条件Minor GC触发条件当Eden区满时，触发Minor GC。 Full GC触发条件 调用System.gc()时，系统建议执行Full GC，但是不必然执行 老年代空间不足 方法区空间不足 【concurrent mode failure】通过Minor GC后进入老年代的平均大小大于老年代的可用内存 【promotion failed】由Eden区、From Space区向To Space区复制时，对象大小大于To Space可用内存，则把该对象转存到老年代，且老年代的可用内存小于该对象大小 JVM内存管理及GC机制 了解CMS(Concurrent Mark-Sweep)垃圾回收器 深入理解JVM（5）:Java垃圾收集器]]></content>
      <categories>
        <category>Java</category>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>JVM</tag>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[web开发中web容器的作用(如tomcat)]]></title>
    <url>%2Fweb%E5%BC%80%E5%8F%91%2Fweb%E5%BC%80%E5%8F%91%E4%B8%ADweb%E5%AE%B9%E5%99%A8%E7%9A%84%E4%BD%9C%E7%94%A8(%E5%A6%82tomcat)%2F</url>
    <content type="text"><![CDATA[servlet可以理解成服务器端处理数据的Java程序，那么谁来负责管理servlet呢？ 这时候我们就要用到web容器。它帮助我们管理着servlet等，使我们只需要将重心专注于业务逻辑。 什么是web容器？servlet没有main方法，那我们如何启动一个servlet，如何结束一个servlet，如何寻找一个servlet等等，都受控于另一个Java应用，这个应用我们就称之为web容器。 我们最常见的tomcat就是这样一个容器。如果web服务器应用得到一个指向某个servlet的请求，此时服务器不是把servlet交给servlet本身，而是交给部署该servlet的容器。要有容器向servlet提供http请求和响应，而且要由容器调用servlet的方法，如doPost或者doGet。 web容器的作用servlet需要由web容器来管理，那么采取这种机制有什么好处呢？ 通信支持 利用容器提供的方法，你可以简单的实现servlet与web服务器的对话。否则你就要自己建立server服务端，监听端口，创建新的流等等一系列复杂的操作。而容器的存在就帮我们封装这一系列复杂的操作。使我们能够专注于servlet中的业务逻辑的实现。 生命周期管理 容器负责servlet的整个生命周期。如何加载类，实例化和初始化servlet，调用servlet方法，并使servlet实例能够被垃圾回收。有了容器，我们就不用花精力去考虑这些资源管理垃圾回收之类的事情。 多线程支持 容器会自动为接收的每个servlet请求创建一个新的Java线程，servlet运行完之后，容器会自动结束这个线程。 声明式实现安全 利用容器，可以使用xml部署描述文件来配置安全性，而不必将其硬编码到servlet中。 JSP支持 容器将jsp翻译成Java 容器如何处理请求 client点击一个URL，其URL指向一个servlet而不是静态界面。 容器识别出这个请求索要的是一个servlet，所以创建两个对象httpservletrequest和httpservletresponse 容器根据请求中的URL找到对应的servlet，为这个请求创建或分配一个线程，并把两个对象request和response传递到servlet线程中。 容器调用servlet的service()方法。根据请求的不同类型，service()方法会调用doGet()或doPost()方法。 doGet()方法生成动态页面，然后把这个页面填入到response对象中，此时，容器仍然拥有response对象的引用。 线程结束。容器把response对象转换成http响应，传回client，并销毁response和request对象。 URL与servlet映射模式12345678&lt;servlet&gt; &lt;servlet-name&gt;Ch1Servlet&lt;/servlet-name&gt; &lt;servlet-class&gt;ch1Servlet.Ch1Servlet&lt;/servlet-class&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;Ch1Servlet&lt;/servlet-name&gt; &lt;url-pattern&gt;/Ch1Servlet&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; servlet有三个名字： 客户知道的URL名&lt;url-pattern&gt;/Ch1Servlet&lt;/url-pattern&gt; 部署人员知道的秘密的内部名&lt;servlet-name&gt;Ch1Servlet&lt;/servlet-name&gt; 实际文件名&lt;servlet-class&gt;ch1Servlet.Ch1Servlet&lt;/servlet-class&gt;]]></content>
      <categories>
        <category>web开发</category>
      </categories>
      <tags>
        <tag>web开发</tag>
        <tag>容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[区块链与传统分布式数据库的区别]]></title>
    <url>%2F%E5%8C%BA%E5%9D%97%E9%93%BE%2F%E5%8C%BA%E5%9D%97%E9%93%BE%E4%B8%8E%E4%BC%A0%E7%BB%9F%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[在比特币系统中，使用区块链作为交易记账的账本，存储了比特币所有的交易信息。由于比特币的分布式特性，所以可以将区块链视为一个分布式的数据库。 但是与传统的分布式数据库而言，又具有一些差异，目前总结如下： 区块链技术中的每一个节点保存的区块链前缀部分都是完全相同的，仅区块链末端有所差异。 区块链具有数据不可篡改的特性。源于区块链本身的数据结构和共识机制。数据结构而言：区块链之间的区块都是通过Hash，Merkle Tree，SHA256，ECC等密码学证明连接在一起的。因此，当主链足够长时，若要对其中的一个区块的数据进增加，删除，修改等操作，就需要对被修改块之后的所有块全部重新进行密码学的证明。如果被篡改的区块处于主链靠前的位置，那么篡改数据的代价将远远高于篡改过后的获利。 区块链较传统分布式数据库而言具有数据公开性，以及可溯源性。原因在于，区块链中除了区块之间是有连续性外，区块链中的数据的每次修改等变更都是通过数字签名合法的记录在区块链上。也就是说，区块链中记录了数据从产生到消亡之间的每次修改，在比特币系统中，体现在比特币的产生到消费的全过程都是有迹可循。这样一来，就提供了数据的可溯源性，保证了过程的公开性，数据的透明性。 事务特性下面分别针对ACID特性对分布式数据库和区块链进行对比 Atomicity（原子性） 分布式数据库 由全局数据库管理系统控制，保证所有节点均完成或均失败 区块链 由共识机制、分叉理论和最长链原则共同控制。 共识机制尽可能保证所有节点数据的原子性，但也会因为网络延迟和节点作恶导致部分节点更新失败。 如果出现有的节点更新数据成功，有的节点更新数据失败的情况，则发生分叉，各节点根据最长链原则更新自己的数据 Consistency（一致性） 分布式数据库 由业务层或全局数据库管理系统控制 区块链 共识机制控制各节点在一定时间窗口内“同步”更新，更新不成功即分叉 Isolation（隔离性） 分布式数据库 全局管理系统中维护事物序列号，根据序列号判定执行顺序 区块链 不存在隔离性问题。在共识机制控制下，单个时间点只有一个主体完成对区块链的更新操作，因此不存在并发事物操作。 Durability（持久性） 分布式数据库与区块链的单点都是传统的数据库，因此能保证持久性 篡改 分布式数据库 单个节点能独立管理自己存储的数据； 已经提交的数据可以被修改或抹除，但修改或抹除会留下可删除的日志 区块链 单个节点无法独立管理数据，如果任意更新的话即成为“恶意节点”，此类节点可以通过与其他节点的数据比对发现 不可篡改，已经提交的数据会被永久保留，无法被修改或抹除 中心化控制程度 分布式数据库 由全局数据库管理系统统一控制 DBA(数据库管理员)有“生杀大权”； 区块链 去中心化 不存在DBA的角色 节点间的关系 分布式数据库 各节点之间的关系是：信任，协作 区块链 各节点之间的关系是：怀疑，制约]]></content>
      <categories>
        <category>区块链</category>
      </categories>
      <tags>
        <tag>区块链</tag>
        <tag>传统分布式数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sleep和wait的区别]]></title>
    <url>%2FJava%2FJava%E5%9F%BA%E7%A1%80%2Fsleep%E5%92%8Cwait%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[多线程中会使用到两个延迟的函数，wait和sleep。 wait是Object类中的方法，而sleep是Thread类中的方法。 那，他们两者到底有什么区别呢？？ sleep和wait的区别sleep是Thread类中的静态方法。无论是在a线程中调用b的sleep方法，还是b线程中调用a的sleep方法，谁调用，谁睡觉。 最主要的是sleep方法调用之后，并没有释放锁。使得线程仍然可以同步控制。 sleep不会让出系统资源（这里的资源，应该是指锁资源，sleep会让出CPU）；而wait是进入线程等待池中等待，让出系统资源。 调用wait方法的线程，不会自己唤醒，需要线程调用 notify / notifyAll 方法唤醒等待池中的所有线程，才会进入就绪队列中等待系统分配资源。 sleep方法会自动唤醒，如果时间不到，想要唤醒，可以使用interrupt方法强行打断。 Thread.sleep(0) // 触发操作系统立刻重新进行一次CPU竞争。 使用范围： sleep可以在任何地方使用。而wait，notify，notifyAll只能在同步控制方法或者同步控制块中使用。 sleep必须捕获异常，而wait，notify，notifyAll的不需要捕获异常。 释放CPU sleep()方法: 当程序运行到Thread.sleep(100L);时,休眠100毫秒,同时交出CPU时间片,100毫秒后,重新进入可运行状态,等待CPU重新分配时间片,而线程交出时间片时,CPU拿到时间片,由操作系统负责在可运行状态的线程中选中并分配时间片 wait()方法: 程序在运行时,遇到wait()方法,这时线程进入当前对象的等待队列并交出CPU,等待其他线程notifyAll()时,才能重新回到可运行状态,等待OS分配CPU 释放锁调用obj.wait()会立即释放锁，以便其他线程可以执行obj.notify()，但是notify()不会立刻立刻释放sycronized（obj）中的obj锁，必须要等notify()所在线程执行完synchronized（obj）块中的所有代码才会释放这把锁. 而 yield(),sleep()不会释放锁。]]></content>
      <categories>
        <category>Java</category>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Extreme Programming explained读书笔记]]></title>
    <url>%2F%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%2FExtreme%20Programming%20explained%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[极限编程(eXtreme Programming)是近几年才时兴起的开发模型，主要是针对中小型开发团队在开发时间要求紧、需求不稳定的中小项目（大多数软件项目都是这个情况）时使用。 极限编程打破了传统软件工程的框架，非常新巧。如： 整个开发过程中文档很少，大量使用“卡片 （如CRC卡片）”来描述开发计划和内容 没有真正意义上的软件功能规格说明书，取而代之的是一系列可测试的用例 没有独立的设计和测试阶段，它们总是在迭代中增量反复进行 设计：尽可能小和简单；一般没有代码复审（code review)，大家共同拥有代码 而它的最显著的一个外在特征是它常使用“成对开发” 极限编程XP的特点可以用“快、小、灵”来概括，极限编程和传统瀑布模型（自顶向下）的区别在于它使用迭代增量（设计-&gt;代码-&gt;测试-&gt;设计-&gt;代码…)的方式。 第一部分：问题最基本的问题：风险常见的风险以及XP如何化解？ 进度延迟——XP发行周期较短；一个发行周期内，要求对客户提出的功能进行一到四周的迭代，优先实现优先级较高的功能 项目取消——XP让客户选择具有最大业务意义的最小版本，从而在投入生产前减少发生错误的几率 系统恶化——XP创建并维护一整套测试程序，每次发生变化后都要重新运行测试程序以确保质量 缺陷率高——XP测试时，既遵从程序员按逐条功能编写测试程序的观点，又遵从客户按逐个程序特性编写测试程序的观点 业务误解——XP要求客户成为整个团队的一部分 业务变更——XP缩短了版本周期，因此每个版本的开发过程中的变化就少 错误过多——XP强调只专注于具有最高优先级的任务 人员调整——XP要求程序员承担估算和完成自己工作的责任，并将实际花费时间进行反馈，以改进他们的估算 开发过程XP的完整开发过程： 结对编程 测试驱动开发 结对开发不仅要运行测试用例，还要改进系统设计 开发之后接着进行集成，以及集成测试 极限编程的基本出发是认为成对开发的效率在一定条件下要高于两个人独立开发的和。在很多项目中，这种做法的有效性已经被证实。 软件开发的经济学我们需要通过减少开销，更快地获得收入，并增加项目的可能生产周期，从而使我们的软件开发在经济上更具价值。 但最重要的是，我们需要增加商业决策的选项。 我们可以制定一个最大化项目经济价值的战略。 减少开销，但是这很困难 赚取更多，但这对一个优秀的营销组织可能更加容易 以后花钱并且赚得更快，所以我们支付的利息更少，我们花的钱也更多，我们赚的钱也更多。 增加项目存货的可能性，所以我们更有可能在项目后期获得巨大回报。 四大变量软件开发过程中的四大变量 成本——钱多一点可以促进工作顺利进行，但短时间内投入太多金钱会产生更多无法解决的问题 时间——更长的交付时间有助于提高质量和扩大范围 质量——有意识的牺牲质量，可能在短时间内获利，但是未来耗费的成本是巨大的 范围——更小的范围有助于提高质量 专注于范围许多人知道成本，质量和时间作为控制变量，但不承认第四种。对于软件开发来说，范围是需要注意的最重要的变量。程序员和商业人士对开发中的软件有什么价值都没有模糊的概念。项目管理中最强有力的决定之一就是消除范围。 关于范围的一个很棒的事情是它是一个变化很大的变量。 几十年来，程序员一直在抱怨：“客户不能告诉我们他们想要什么，当我们给他们他们想说的话时，他们不喜欢它。” 这是软件开发的绝对真理。 起初，这些要求从来不明确。 顾客永远无法确切地告诉你他们想要什么。 通过不尝试做太多，可以保持按时生产所需质量的能力。 如果我们根据这个模型创建了一个开发规则，我们将确定一个软件的日期，质量和成本。 我们会看前三个变量所隐含的范围。 然后，随着发展的进展，我们会不断调整范围以符合我们发现它们的条件。 变化的成本软件工程的普遍假设之一是改变程序的成本随着时间呈指数级增长。 以下几个因素，可以使得我们的代码即使经过长时间开发也可以很容易修改 简单的设计 自动化测试 不断改进设计 学会开车我们需要通过一些小的调整来控制软件的开发，而不是做一些大的调整，就像驾驶汽车一样。这意味着我们需要反馈意见来了解我们何时休假，我们需要很多机会进行更正，而且我们必须能够以合理的成本进行更正。 软件中的一切都在变化。需求改变，设计更改，技术变化，团队改变，团队成员改变。只有变化才是永远不变的。 四个价值观XP的四大价值观 沟通——项目问题总是可以追溯到 有人不跟其他人谈论重要的事情。XP旨在在团队成员之间保持良好的沟通。 简化——简单和沟通有着美妙的互相支持的关系。你越多沟通，更清晰，你可以看到究竟需要做什么和更多的信心你有什么真的不需要做。你的系统越简单，你就越少 沟通，这会导致更完整的沟通，尤其是如果可以的话简化系统足以减少程序员。 反馈——系统绝对是无价的。乐观是编程的职业危害，反馈是对这一危害的治疗。 勇气——要勇敢地修正体系结构中的错误，勇敢地抛弃原有的代码。 基本原则XP的基本原则 快速反馈——行动与反馈之间的间隔对学习至关重要。 假设简单性——把每个问题都看做可以用近乎荒谬的简单方法来解决。 递增更改——每次修改一点点，即使采用XP也必须采用小步骤。 拥抱变化——最佳的策略就是在实践中运用最多的策略，优先解决最重要的问题。 优质工作——出色完成工作是项目最终成功的保证。 回到基础开发中的四项基本活动 编码——编码是了解最佳结构的方式，同时也能提供清晰简洁的沟通。 测试——测试既是资源也是责任。 倾听——如果准备提问，那么最好准备好倾听答案。 设计——良好的设计可以可以降低系统变更的成本，提高系统的可扩展性 第二部分：解决方案快速概述商业考虑和技术考虑都不应该是最重要的。软件发展始终是可能和理想之间的一个不断发展的对话。 商务人士需要决定： 范围——该系统必须解决多少问题才有价值生产？商人有能力了解有多少钱是不够的，多少太多了。 优先级——如果您最初只能拥有A或B，您想要哪一个？生意人能够确定这一点，远比程序员更重要。 发布的组成——业务之前需要完成多少或多少工作用软件比没用它好？程序员对这个问题的直觉可能是疯狂的错误。 发布日期——什么是重要的日期，软件的存在（或一些软件）会产生很大的不同？ 技术人员决定： 估计——多久会采取功能来实现？ 后果——有战略业务决定，应该只在什么时候做出。了解技术后果。数据库的选择就是一个很好的例子。与创业公司相比，企业可能更愿意与一家大公司合作，生产力可能会使额外的风险或不适感到值得。 过程——如何将工作和团队组织？团队需要适应文化。 详细调度——程序员 需要自由安排最高风险的发展部分，以减少项目的总体风险。在这种限制之下，他们仍然倾向于改变业务优先级。 每次发布应该尽可能小，包含最有价值的业务 要求。版本必须作为一个整体来理解 - 也就是说，你无法实现一半 功能并发布它。 在任何时间，软件的正确设计都是这样的： 运行所有测试。 没有重复的逻辑。对像平行类层次结构这样的隐藏重复要小心。 说明对程序员重要的每一个意图。 尽可能少的类和方法。 任何没有自动化测试的程序功能都不存在。 在XP中，每个人都对整个系统负责。不是每个人都知道每个部分。同样，虽然每个人都知道每个部分的一些东西。如果一对正在工作，他们看到改进代码的机会，他们继续前进，如果让他们的生活更轻松，那就改进它。 真正的客户必须与团队坐在一起，可以回答问题，解决纠纷并进行设置小规模优先事项。 编码标准应该尽可能少的工作，符合一次和唯一。标准应该强调沟通。最后，标准必须由整个团队自愿采用 如何工作任何一种做法都不可能很好的单独进行。他们要与其他做法一起运用，以保持平衡。 管理策略管理方面的困境：一方面，你希望经理能够做到这一点决定。没有通信开销，因为只有一个人。有一个人对上级管理层负责。有一个人有远见。没有其他人需要了解它，因为所有的决定都来自一个人。 另一方面，相反的策略是行不通的，不能让每个人都去做决定。 基本的XP管理工作是度量标准。制作大的可视表，对任务总量进行度量。 理想的教练是一个很好的沟通者。 教练的工作职责： 作为新手程序员的开发伙伴。 参与长期重构目标，并鼓励小规模重构来解决部分问题。 帮助具有个人特殊擅长领域的程序员。 向上层管理人员解释开发过程。 跟踪是XP中管理的另一个主要组成部分。跟踪器的工作是收集当前正在跟踪的任何指标并进行确信团队知道实际测量的是什么。运行计划是跟踪器的一部分。 管理人员的干预，可能是调整团队人员结构，甚至也可能是直接终止项目。 设施战略我们将为我们的团队创建一个开放的工作空间，周围有小型私人空间外围和中间的一个公共编程区域。如果你没有合理的工作场所，你的项目将不会成功。 分裂商业和技术战略的关键之一是让技术人员关注技术问题，商业人员关注业务问题。 商业人士应该： 发布的范围和时间 建议功能的相对优先级 建议功能的明确范围 技术人员应该： 估计实现各种功能所需的时间 估计各种技术方案的后果 一个适合他们个性、商业环境和发展过程的公司文化 规划策略我们会迅速制定一个总体计划，然后再进一步细化更短更短的时间范围——年，月，周，天。我们会做的计划迅速且便宜，所以当我们必须改变它时，会有小的惯性。 计划的三个阶段 探索——找出系统能做出什么新东西 承诺——决定接下来要追求的所有可能要求的哪一部分 转向——为现实的开发做出指导计划 发展战略与管理战略不同，发展战略是一个彻底的相反的观点。从传统的观点来看，我们将为今天的问题精心制定解决方案。今天，相信明天我们将能够解决明天的问题。 发展战略始于迭代计划。持续集成减少 发展冲突并为发展事件创造自然结局。集体所有权鼓励整个团队改善整个系统。最后，结对编程将整个过程联系在一起。 持续集成显著降低了项目风险。 代码集体所有权的影响之一就是复杂代码不会存在很长时间。集体所有权倾向于防止复杂代码很早进入系统。集体所有权也倾向于围绕团队传播系统知识。 设计策略我们将不断完善系统的设计，从一个非常简单的设计开始，删除任何无用的灵活性。 最佳设计的定义是运行所有测试用例的最简单的设计。 测试策略在编码之前编写测试，一次又一次地运行测试。我们将永远保留这些测试，并经常一起运行它们。 XP中编写的测试是独立且自动的。 单元测试和功能测试是产品测试的核心。除了单元测试，还有其他同样很重要的测试：并行测试、压力测试、灵活测试。 第三部分：实施XP采用XP一次采用一种XP方法，解决团队遇到的最紧迫的问题。当当前问题不再是最紧迫的问题，继续解决下一个最紧迫的问题。 改进XP如何在现有团队中使用已经投入生产的软件来使用XP？必须在以下方面修改采用策略： 测试——试图返回并为所有现有代码编写测试是很有诱惑力的，但不建议这么做，测试应该是按需编写。 设计——一次只重构一点，在添加新功能时做好重构准备。 计划——切换到XP计划的最大挑战（和机会）是教会客户他们可以从团队中获得多少。 管理——管理向XP转变的最困难的方面之一是决定一个团队成员不再需要工作。 开发——重视结对编程，并准备随时丢弃已经开发出的代码。 理想XP项目的生命周期XP项目的生命周期： 探索——探索完成这个项目所需要的开发技能。 计划——让客户和程序员自信地达成一致的一个最小，最有价值的故事集合的日期。 第一版本迭代——承诺时间表被分成一到四周的迭代次数。每次迭代都会产生一个为该迭代计划的每个故事设置功能测试用例集。 生产——在开发过程中，会放慢软件开发的速度，评估系统风险是否变得更加重要。 维护——维护实际上是XP项目的正常状态。在生产新的功能的同时，维护系统已有功能正常稳定运行。 死亡——死亡的最好理由：客户对系统满意并且无法想到任何事情他们希望在可预见的未来增加。 任务角色 程序员——XP的核心 顾客——极限编程基本二元性的另一半 测试人员——编写测试用例，负责测试系统 跟踪者——做出好的估计，手机各种反馈 教练——为整个过程负责 顾问——帮助解决某个领域深入的技术知识问题 大老板——提供团队勇气、信心以及放权 20-80原则XP的全部价值在所有的实践到位之后才会实现。很多实践可以采取零碎的做法，但是它们的影响会在成倍增加在一起。 软件程序员习惯于处理20-80规则：80％的好处来自于此20％的工作。 什么让XP变得艰难主要是情绪，特别是恐惧，使XP变得困难。 XP的细节很简单，但是很难执行。 什么时候不应该使用XP大团队，不信任的客户，技术 不支持优美的变化。这些情况下，不应该再继续使用XP。 工作中的XP注意使用适用于XP的合同 结论XP反映了恐惧的事情： 没有意义的开销 因为没有做好充分的技术准备而被取消的项目 糟糕的商业决策 由业务人员作出技术决策 职业生涯的结束 项目结果不令人满意 XP也反映了不害怕的事情： 编码 拥抱变化 不知道未来的发展 依赖其他人 变化运行系统的分析与设计 编写测试]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库面试知识点整理]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E5%BA%93%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E9%9D%A2%E8%AF%95%E7%9F%A5%E8%AF%86%E7%82%B9%E6%95%B4%E7%90%86%2F</url>
    <content type="text"><![CDATA[数据库范式 第一范式 列不可分 eg:【联系人】（姓名，性别，电话），一个联系人有家庭电话和公司电话，那么这种表结构设计就没有达到 1NF 第二范式 有主键，保证完全依赖 eg:订单明细表【OrderDetail】（OrderID，ProductID，UnitPrice，Discount，Quantity，ProductName），Discount（折扣），Quantity（数量）完全依赖（取决）于主键（OderID，ProductID），而 UnitPrice，ProductName 只依赖于 ProductID，不符合2NF 第三范式 无传递依赖(非主键列 A 依赖于非主键列 B，非主键列 B 依赖于主键的情况) eg:订单表【Order】（OrderID，OrderDate，CustomerID，CustomerName，CustomerAddr，CustomerCity）主键是（OrderID），CustomerName，CustomerAddr，CustomerCity 直接依赖的是 CustomerID（非主键列），而不是直接依赖于主键，它是通过传递才依赖于主键，所以不符合 3NF 数据库索引 索引是对数据库表中一个或多个列的值进行排序的数据结构，以协助快速查询、更新数据表中的数据 索引的实现通常使用B-Tree及其变种 索引加速了数据的访问，因为存储引擎不会再去扫描整张表得到需要的数据 相反，它从根节点开始，根节点保存了子节点的指针，存储引擎会根据指针快速寻找数据 上图显示了一种索引方式 左边是数据库中的数据表，有col1和col2两列，一共15条记录 右边是以col2为索引列的B-Tree索引 每个节点包含索引的键值和对应数据表地址的指针 这样就可以通过B-Tree在O(logn)的时间复杂度内获取相应数据，明显加快检索速度 MyISAM索引实现MyISAM引擎使用B+Tree作为索引结构 叶节点的data域存放的是数据记录的地址 在MyISAM中，主索引和辅助索引（Secondary key）在结构上没有任何区别，只是主索引要求key是唯一的，而辅助索引的key可以重复。 MyISAM的索引方式也叫做“非聚集”的，之所以这么称呼是为了与InnoDB的聚集索引区分。 InnoDB索引实现虽然InnoDB也使用B+Tree作为索引结构，但具体实现方式却与MyISAM截然不同。 第一个重大区别是InnoDB的数据文件本身就是索引文件。从上文知道，MyISAM索引文件和数据文件是分离的，索引文件仅保存数据记录的地址。而在InnoDB中，表数据文件本身就是按B+Tree组织的一个索引结构，这棵树的叶节点data域保存了完整的数据记录。这个索引的key是数据表的主键，因此InnoDB表数据文件本身就是主索引。 第二个与MyISAM索引的不同是InnoDB的辅助索引data域存储相应记录主键的值而不是地址。换句话说，InnoDB的所有辅助索引都引用主键作为data域。使用辅助索引需要查找两次索引。聚集索引这种实现方式使得按主键的搜索十分高效，但是辅助索引搜索需要检索两遍索引：首先检索辅助索引获得主键，然后用主键到主索引中检索获得记录。 InnoDB的主键选择与插入优化在使用InnoDB存储引擎时，如果没有特别的需要，请永远使用一个与业务无关的自增字段作为主键。 上文讨论过InnoDB的索引实现，InnoDB使用聚集索引，数据记录本身被存于主索引（一颗B+Tree）的叶子节点上。这就要求同一个叶子节点内（大小为一个内存页或磁盘页）的各条数据记录按主键顺序存放，因此每当有一条新的记录插入时，MySQL会根据其主键将其插入适当的节点和位置，如果页面达到装载因子（InnoDB默认为15/16），则开辟一个新的页（节点）。 如果表使用自增主键，那么每次插入新的记录，记录就会顺序添加到当前索引节点的后续位置，当一页写满，就会自动开辟一个新的页。如下图所示： 这样就会形成一个紧凑的索引结构，近似顺序填满。由于每次插入时也不需要移动已有数据，因此效率很高，也不会增加很多开销在维护索引上。 如果使用非自增主键（如果身份证号或学号等），由于每次插入主键的值近似于随机，因此每次新纪录都要被插到现有索引页得中间某个位置： 此时MySQL不得不为了将新记录插到合适位置而移动数据，甚至目标页面可能已经被回写到磁盘上而从缓存中清掉，此时又要从磁盘上读回来，这增加了很多开销，同时频繁的移动、分页操作造成了大量的碎片，得到了不够紧凑的索引结构，后续不得不通过OPTIMIZE TABLE来重建表并优化填充页面。 只要可以，请尽量在InnoDB上采用自增字段做主键。 索引底层实现原理和优化 在数据结构中，我们常见的搜索结构就是二叉搜索树和AVL树（高度平衡的二叉搜索树，为了提高二叉搜索树的效率，减少树的平均搜索长度） 然而，无论是二叉搜索树还是AVL树，当数据量比较大的时候，都会由于树的深度过大而造成IO读写过于频繁，进而导致查询效率低下 因而对于索引而言，多叉搜索树成为不二选择 特别的，B-Tree的各种操作能使B树保持较低的高度，从而保证高的查询效率 B-Tree 平衡多路查找树 B-Tree是一种平衡多路查找树，是一种动态查找效率很高的树形结构 B-Tree中所有结点的孩子结点的最大值成为B-Tree的阶，B-Tree的阶通常用m表示，简称m叉树 一般来说，m&gt;3 一棵m阶的B-Tree或是一棵空树，或者是满足下列条件的m叉树 树中的每个结点最多有m个孩子结点 若根结点不是叶子结点，则根结点至少有2个孩子结点 除根结点外，其他结点至少有(m/2的上界)个孩子结点 结点的结构如下图所示， 其中n为结点中关键字个数，(m/2的上界)-1&lt;=n&lt;=m-1 di(1&lt;=i&lt;=n)为该结点的n个关键字值的第i个，且di&lt; d(i+1) ci(0&lt;=i&lt;=n)为该结点孩子结点的指针，且ci所指向的节点的关键字均大于或等于di且小于d(i+1) 所有的叶结点都在同一层上，并且不带信息（可以看作是外部结点或查找失败的结点，实际上这些结点不存在，指向这些结点的指针为空） 下图是一棵4阶B-Tree，4叉树结点的孩子结点的个数范围[2,4]，其中，有2个结点有4个孩子结点，有1个结点有3个孩子结点，有5个结点有2个孩子结点 B-Tree的查找类似二叉排序树的查找，所不同的是B-树每个结点上是多关键码的有序列表，在到达某个结点时，先在有序表中查找，若找到，则查找成功。否则，到按照对应的指针信息指向的子树中去查找，当到达叶子结点时，则说明树中没有对应的关键码 由于B-Tree的高检索效率，B-树主要应用在文件系统和数据库中，对于存储在硬盘上的大型数据库文件，可以极大程度减少访问硬盘次数，大幅度提高数据检索效率 B+ Tree InnoDB存储引擎的索引实现 B+Tree是应文件系统所需而产生的一种B-Tree树的变形树 一棵m阶的B+树和m阶的B_TREE的差异在于以下三点： n棵子树的结点中含有n个关键码 所有的叶子结点中包含了全部关键码的信息，及指向含有这些关键码记录的指针，且叶子结点本身依关键码的大小自小而大的顺序链接 非终端结点可以看成是索引部分，结点中仅含有其子树根结点中最大（或最小）关键码 下图为一棵3阶的B+树 通常在B+树上有两个头指针，一个指向根节点，另一个指向关键字最小的叶子节点 因此可以对B+树进行两种查找运算：一种是从最小关键字起顺序查找，另一种是从根节点开始，进行随机查找 在B+树上进行随机查找、插入和删除的过程基本上与B-树类似 只是在查找时，若非终端结点上的关键码等于给定值，并不终止，而是继续向下直到叶子结点 因此，对于B+树，不管查找成功与否，每次查找都是走了一条从根到叶子结点的路径 为什么说B+树比B树更适合实际应用中操作系统的文件索引和数据库索引 B+树的磁盘读写代价更低 B+树的内部结点并没有指向关键字具体信息的指针，因此其内部结点相对B树更小 如果把所有同一内部结点的关键字存放在同一盘块中，那么盘块所能容纳的关键字数量也越多 一次性读入内存中的需要查找的关键字也就越多，相对来说IO读写次数也就降低了 B+树的查询效率更加稳定 由于内部结点并不是最终指向文件内容的结点，而只是叶子结点中关键字的索引，所以，任何关键字的查找必须走一条从根结点到叶子结点的路 所有关键字查询的路径长度相同，导致每一个数据的查询效率相当 数据库索引采用B+树而不是B树的主要原因 B+树只要遍历叶子节点就可以实现整棵树的遍历，而且在数据库中基于范围的查询是非常频繁的 而B树只能中序遍历所有节点，效率太低 文件索引和数据库索引为什么使用B+树 文件与数据库都是需要较大的存储，也就是说，它们都不可能全部存储在内存中，故需要存储到磁盘上 而所谓索引，则为了数据的快速定位与查找 那么索引的结构组织要尽量减少查找过程中磁盘I/O的存取次数，因此B+树相比B树更为合适。 数据库系统巧妙利用了局部性原理与磁盘预读原理，将一个节点的大小设为等于一个页，这样每个节点只需要一次I/O就可以完全载入 而红黑树这种结构，高度明显要深的多，并且由于逻辑上很近的节点(父子)物理上可能很远，无法利用局部性 最重要的是，B+树还有一个最大的好处：方便扫库。 B树必须用中序遍历的方法按序扫库，而B+树直接从叶子结点挨个扫一遍就完了 B+树支持range-query非常方便，而B树不支持，这是数据库选用B+树的最主要原因 索引的优点 大大加快数据的检索速度，这也是创建索引的最主要的原因 加速表和表之间的连接 在使用分组和排序子句进行数据检索时，同样可以显著减少查询中分组和排序的时间 通过创建唯一性索引，可以保证数据库表中每一行数据的唯一性 什么情况下设置了索引但无法使用 以“%(表示任意0个或多个字符)”开头的LIKE语句，模糊匹配 OR语句前后没有同时使用索引 数据类型出现隐式转化（如varchar不加单引号的话可能会自动转换为int型） 对于多列索引，必须满足 最左匹配原则 eg：多列索引col1、col2和col3，则索引生效的情形包括 col1或col1，col2或col1，col2，col3) 什么样的字段适合创建索引 经常作查询选择的字段 经常作表连接的字段 经常出现在order by, group by, distinct 后面的字段 创建索引时需要注意什么 非空字段： 应该指定列为NOT NULL，除非你想存储NULL。 在mysql中，含有空值的列很难进行查询优化，因为它们使得索引、索引的统计信息以及比较运算更加复杂。 应该用0、一个特殊的值或者一个空串代替空值 取值离散大的字段 变量各个取值之间的差异程度 差异大的列放到联合索引的前面 可以通过count()函数查看字段的差异值，返回值越大说明字段的唯一值越多，字段的离散程度高 索引字段越小越好 数据库的数据存储以页为单位一页存储的数据越多一次IO操作获取的数据越大效率越高 索引的缺点 时间方面 创建索引和维护索引要耗费时间 具体地，当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护 这样就降低了数据的维护速度 空间方面 索引需要占物理空间 索引的分类 普通索引和唯一性索引 索引列的值的唯一性 单个索引和复合索引 索引列所包含的列数 聚簇索引与非聚簇索引 聚簇索引的叶子节点就是数据节点 非聚簇索引的叶子节点仍然是索引节点，只不过有指向对应数据块的指针 主键、自增主键、主键索引与唯一索引概念区别 主键 指字段唯一、不为空值的列 主键索引 指的就是主键，主键是索引的一种，是唯一索引的特殊类型 创建主键的时候，数据库默认会为主键创建一个唯一索引 自增主键 段类型为数字、自增、并且是主键 唯一索引 索引列的值必须唯一，但允许有空值 主键是唯一索引，这样说没错 但反过来说，唯一索引也是主键就错误了 因为唯一索引允许空值，主键不允许有空值 所以不能说唯一索引也是主键 主键就是聚集索引吗？主键和索引有什么区别？ 主键是一种特殊的唯一性索引，其可以是聚集索引，也可以是非聚集索引 在SQLServer中，主键的创建必须依赖于索引 默认创建的是聚集索引 但也可以显式指定为非聚集索引。 InnoDB作为MySQL存储引擎时，默认按照主键进行聚集 如果没有定义主键，InnoDB会试着使用唯一的非空索引来代替 如果没有这种索引，InnoDB就会定义隐藏的主键然后在上面进行聚集 所以，对于聚集索引来说，你创建主键的时候，自动就创建了主键的聚集索引 数据库事务 事务是一个不可分割的数据库操作序列，也是数据库并发控制的基本单位 其执行的结果必须使数据库从一种一致性状态变到另一种一致性状态 事务的特征 原子性(Atomicity) 事务所包含的一系列数据库操作要么全部成功执行，要么全部回滚 一致性(Consistency) 事务的执行结果必须使数据库从一个一致性状态到另一个一致性状态 隔离性(Isolation) 并发执行的事务之间不能相互影响 持久性(Durability) 事务一旦提交，对数据库中数据的改变是永久性的 事务并发带来的问题 脏读 一个事务读取了另一个事务未提交的数据 不可重复读 不可重复读的重点是修改 同样条件下两次读取结果不同，也就是说，被读取的数据可以被其它事务修改 幻读 幻读的重点在于新增或者删除 同样条件下两次读出来的记录数不一样 隔离级别 隔离级别决定了一个session中的事务可能对另一个session中的事务的影响 ANSI标准定义了4个隔离级别，MySQL的InnoDB都支持，分别是 READ UNCOMMITTED 最低级别的隔离 通常又称为dirty read 它允许一个事务读取另一个事务还没commit的数据 这样可能会提高性能 但是会导致脏读问题 READ COMMITTED 在一个事务中只允许对其它事务已经commit的记录可见 该隔离级别不能避免不可重复读问题 REPEATABLE READ 在一个事务开始后，其他事务对数据库的修改在本事务中不可见，直到本事务commit或rollback 但是，其他事务的insert/delete操作对该事务是可见的 也就是说，该隔离级别并不能避免幻读问题。在一个事务中重复select的结果一样，除非本事务中update数据库 SERIALIZABLE 最高级别的隔离 只允许事务串行执行 MySQL默认的隔离级别是REPEATABLE READ MySQL事务支持 MySQL的事务支持不是绑定在MySQL服务器本身，而是与存储引擎相关 MyISAM：不支持事务，用于只读程序提高性能 InnoDB：支持ACID事务、行级锁、并发 Berkeley DB：支持事务 设置数据库事务事务控制语句 BEGIN或START TRANSACTION；显式地开启一个事务； COMMIT；也可以使用COMMIT WORK，不过二者是等价的。COMMIT会提交事务，并使已对数据库进行的所有修改称为永久性的； ROLLBACK；也可以使用ROLLBACK WORK，不过二者是等价的。回滚会结束用户的事务，并撤销正在进行的所有未提交的修改； SAVEPOINT identifier；：SAVEPOINT允许在事务中创建一个保存点，一个事务中可以有多个SAVEPOINT RELEASE SAVEPOINT identifier；删除一个事务的保存点，当没有指定的保存点时，执行该语句会抛出一个异常； ROLLBACK TO identifier；把事务回滚到标记点； SET TRANSACTION；用来设置事务的隔离级别 MYSQL事务处理的两种方法： 用 BEGIN, ROLLBACK, COMMIT来实现 BEGIN 开始一个事务 ROLLBACK 事务回滚 COMMIT 事务确认 直接用 SET 来改变 MySQL 的自动提交模式 SET AUTOCOMMIT=0 禁止自动提交 SET AUTOCOMMIT=1 开启自动提交 MySQL存储引擎 MySQL5.5之前默认的存储引擎是MyISAM MySQL5.5之后默认的存储引擎改为InnoDB 同时作为维护mysql内部结构的mysql和information_schema两个databases中的表，依然使用MyISAM存储引擎，而且不能被更改为InnoDB MyISAM 在MySQL 5.5之前，MyISAM是mysql的默认数据库引擎，其由早期的ISAM（Indexed Sequential Access Method：有索引的顺序访问方法）所改良 虽然MyISAM性能极佳，但却有一个显著的缺点：不支持事务处理 不过，MySQL也导入了另一种数据库引擎InnoDB，以强化参考完整性与并发违规处理机制，后来就逐渐取代MyISAM InnoDB InnoDB是MySQL的数据库引擎之一 与传统的ISAM、MyISAM相比，InnoDB的最大特色就是支持ACID兼容的事务功能，类似于PostgreSQL MyISAM和InnoDB的区别存储结构 每个MyISAM在磁盘上存储成三个文件 第一个文件的名字以表的名字开始，扩展名指出文件类型。 .frm文件存储表定义 数据文件的扩展名为.MYD (MYData) 索引文件的扩展名是.MYI (MYIndex) InnoDB所有的表都保存在同一个数据文件中（也可能是多个文件，或者是独立的表空间文件） InnoDB表的大小只受限于操作系统文件的大小，一般为2GB 存储空间 MyISAM 可被压缩，占据的存储空间较小 支持静态表、动态表、压缩表三种不同的存储格式 InnoDB 需要更多的内存和存储 它会在主内存中建立其专用的缓冲池用于高速缓冲数据和索引 可移植性、备份及恢复 MyISAM 数据是以文件的形式存储 所以在跨平台的数据转移中会很方便 同时在备份和恢复时也可单独针对某个表进行操作 InnoDB 免费的方案可以是拷贝数据文件、备份binlog，或者用mysqldump，在数据量达到几十G的时候就相对痛苦了 事务支持 MyISAM 强调的是性能 每次查询具有原子性 其执行速度比InnoDB类型更快 但是不提供事务支持。 InnoDB 提供事务、外键等高级数据库功能 具有事务提交、回滚和崩溃修复能力 AUTO_INCREMENT MyISAM 在MyISAM中，可以和其他字段一起建立联合索引 引擎的自动增长列必须是索引 如果是组合索引，自动增长可以不是第一列 它可以根据前面几列进行排序后递增 InnoDB InnoDB中必须包含只有该字段的索引 并且引擎的自动增长列必须是索引 如果是组合索引也必须是组合索引的第一列 表锁差异 MyISAM只支持表级锁，用户在操作MyISAM表时，select、update、delete和insert语句都会给表自动加锁，如果加锁以后的表满足insert并发的情况下，可以在表的尾部插入新的数据 InnoDB支持事务和行级锁。行锁大幅度提高了多用户并发操作的性能，但是InnoDB的行锁，只是在WHERE的主键是有效的，非主键的WHERE都会锁全表的 全文索引 MyISAM支持FULLTEXT类型的全文索引 InnoDB不支持FULLTEXT类型的全文索引，但是InnoDB可以使用sphinx插件支持全文索引，并且效果更好 表主键 MyISAM允许没有任何索引和主键的表存在，索引都是保存行的地址 对于InnoDB，如果没有设定主键或者非空唯一索引，就会自动生成一个6字节的主键(用户不可见)，数据是主索引的一部分，附加索引保存的是主索引的值 表的具体行数 MyISAM保存表的总行数，select count() from table;会直接取出该值 而InnoDB没有保存表的总行数，如果使用select count() from table；就会遍历整个表，消耗相当大 但是在加了where条件后，MyISAM和InnoDB处理的方式都一样。 CURD操作 在MyISAM中，如果执行大量的SELECT，MyISAM是更好的选择 对于InnoDB，如果你的数据执行大量的INSERT或UPDATE，出于性能方面的考虑，应该使用InnoDB表。 DELETE从性能上InnoDB更优 但DELETE FROM table时，InnoDB不会重新建立表，而是一行一行的删除，在InnoDB上如果要清空保存有大量数据的表，最好使用truncate table这个命令 外键 MyISAM不支持外键 InnoDB支持外键 总结 通过上述的分析，基本上可以考虑使用InnoDB来替代MyISAM引擎了 原因是InnoDB自身很多良好的特点，比如事务支持、存储过程、视图、行级锁、外键等等 尤其在并发很多的情况下，相信InnoDB的表现肯定要比MyISAM强很多 另外，必须需要注意的是，任何一种表都不是万能的，合适的才是最好的，才能最大的发挥MySQL的性能优势 如果是不复杂的、非关键的Web应用，还是可以继续考虑MyISAM的，这个具体情况具体考虑 实践中如何优化MySQL 实践中，MySQL的优化主要涉及SQL语句及索引的优化、数据表结构的优化、系统配置的优化和硬件的优化四个方面，如下图所示 SQL语句及索引的优化SQL语句的优化 优化insert语句：一次插入多值 应尽量避免在where子句中使用!=或&lt;&gt;操作符，否则将引擎放弃使用索引而进行全表扫描 应尽量避免在where子句中对字段进行null值判断，否则将导致引擎放弃使用索引而进行全表扫描 优化嵌套查询：子查询可以被更有效率的连接(Join)替代 很多时候用exists代替in是一个好的选择 EXISTS比较快 因为EXISTS返回一个Boolean型 而IN返回一个值 索引的优化 建议在经常作查询选择的字段、经常作表连接的字段以及经常出现在order by、group by、distinct 后面的字段中建立索引 但必须注意以下几种可能会引起索引失效的情形 以“%(表示任意0个或多个字符)”开头的LIKE语句，模糊匹配 OR语句前后没有同时使用索引 数据类型出现隐式转化（如varchar不加单引号的话可能会自动转换为int型） 对于多列索引，必须满足最左匹配原则 eg,多列索引col1、col2和col3，则 索引生效的情形包括col1或col1，col2或col1，col2，col3 数据库表结构的优化表的垂直拆分 把含有多个列的表拆分成多个表，解决表宽度问题 具体包括以下几种拆分手段 把不常用的字段单独放在同一个表中 把大字段独立放入一个表中 把经常使用的字段放在一起 这样做的好处是非常明显的，具体包括 拆分后业务清晰 拆分规则明确 系统之间整合或扩展容易 数据维护简单 表的水平拆分 表的水平拆分用于解决数据表中数据过大的问题 水平拆分每一个表的结构都是完全一致的 一般地，将数据平分到N张表中的常用方法包括以下两种： 对ID进行hash运算，如果要拆分成5个表，mod(id,5)取出0~4个值 针对不同的hashID将数据存入不同的表中 表的水平拆分会带来一些问题和挑战 包括跨分区表的数据查询、统计及后台报表的操作等问题 但也带来了一些切实的好处： 表分割后可以降低在查询时需要读的数据和索引的页数，同时也降低了索引的层数，提高查询速度 表中的数据本来就有独立性，例如表中分别记录各个地区的数据或不同时期的数据，特别是有些数据常用，而另外一些数据不常用 需要把数据存放到多个数据库中，提高系统的总体可用性(分库，鸡蛋不能放在同一个篮子里) 系统配置的优化 操作系统配置的优化 增加TCP支持的队列数 mysql配置文件优化 Innodb缓存池设置 innodb_buffer_pool_size，推荐总内存的75% 缓存池的个数innodb_buffer_pool_instances 存储过程什么是存储过程 存储过程是事先经过编译并存储在数据库中的一段SQL语句的集合 进一步地说，存储过程是由一些T-SQL语句组成的代码块，这些T-SQL语句代码像一个方法一样实现一些功能（对单表或多表的增删改查） 然后再给这个代码块取一个名字，在用到这个功能的时候调用他就行了 存储过程有什么优缺点 存储过程只在创建时进行编译，以后每次执行存储过程都不需再重新编译，而一般 SQL 语句每执行一次就编译一次，所以使用存储过程可提高数据库执行效率 当SQL语句有变动时，可以只修改数据库中的存储过程而不必修改代码 减少网络传输，在客户端调用一个存储过程当然比执行一串SQL传输的数据量要小 通过存储过程能够使没有权限的用户在控制之下间接地存取数据库，从而确保数据的安全 drop、delete与truncate的区别 SQL中的drop、delete、truncate都表示删除 delete delete用来删除表的全部或者一部分数据行 执行delete之后，用户需要提交(commmit)或者回滚(rollback)来执行删除或者撤销删除 delete命令会触发这个表上所有的delete触发器 truncate truncate删除表中的所有数据 这个操作不能回滚，也不会触发这个表上的触发器 truncate比delete更快，占用的空间更小 drop drop命令从数据库中删除表 所有的数据行，索引和权限也会被删除 所有的DML触发器也不会被触发 这个命令也不能回滚 视图 视图是一种虚拟的表 通常是有一个表或者多个表的行或列的子集 具有和物理表相同的功能 视图的建立和删除不影响基本表 可以对视图进行增，删，改，查等操作 对视图内容的更新（增删改）直接影响基本表 当视图来自多个基本表时，不允许添加和删除数据，会报错Can not modify more than one base table through a join view &#39;view_name&#39; 相比多表查询，它使得我们获取数据更容易 游标 游标是对查询出来的结果集作为一个单元来有效的处理 游标可以定在该单元中的特定行，从结果集的当前行检索一行或多行 可以对结果集当前行做修改 一般不使用游标，但是需要逐条处理数据的时候，游标显得十分重要 在操作mysql的时候，我们知道MySQL检索操作返回一组称为结果集的行。这组返回的行都是与 SQL语句相匹配的行（零行或多行）。使用简单的SELECT语句，例如，没有办法得到第一行、下一行或前 10行，也不存在每次一行地处理所有行的简单方法（相对于成批地处理它们），有时，需要在检索出来的行中前进或后退一行或多行。这就是使用游标的原因 游标（cursor）是一个存储在MySQL服务器上的数据库查询 它不是一条SELECT语句，而是被该语句检索出来的结果集 在存储了游标之后，应用程序可以根据需要滚动或浏览其中的数据 游标主要用于交互式应用，其中用户需要滚动屏幕上的数据，并对数据进行浏览或做出更改 触发器 触发器是与表相关的数据库对象 在满足定义条件时触发，并执行触发器中定义的语句集合 触发器的这种特性可以协助应用在数据库端确保数据库的完整性 表级锁、页级索和行级锁 表级，直接锁定整张表，在你锁定期间，其它进程无法对该表进行写操作。如果你是写锁，则其它进程则读也不允许 行级，仅对指定的记录进行加锁，这样其它进程还是可以对同一个表中的其它记录进行操作。 页级，表级锁速度快，但冲突多，行级冲突少，但速度慢。所以取了折衷的页级，一次锁定相邻的一组记录。 MySQL 5.1支持对MyISAM和MEMORY表进行表级锁定，对BDB表进行页级锁定，对InnoDB表进行行级锁定。 为什么会出现死锁InnoDB使用行锁定，BDB使用页锁定。对于这两种存储引擎，都可能存在死锁。这是因为，在SQL语句处理期间，InnoDB自动获得行锁定和BDB获得页锁定，而不是在事务启动时获得。 锁特点 表级锁：开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突的概率最高，并发度最低。 行级锁：开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最高。 页面锁：开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般。 行级锁的优缺点优点 当在许多线程中访问不同的行时只存在少量锁定冲突。 回滚时只有少量的更改。 可以长时间锁定单一的行。 缺点 比页级或表级锁定占用更多的内存。 当在表的大部分中使用时，比页级或表级锁定速度慢，因为你必须获取更多的锁。 如果你在大部分数据上经常进行GROUP BY操作或者必须经常扫描整个表，比其它锁定明显慢很多。 用高级别锁定，通过支持不同的类型锁定，你也可以很容易地调节应用程序，因为其锁成本小于行级锁定。 什么情况下表锁定优先于行锁定 表的大部分语句用于读取。 对严格的关键字进行读取和更新，你可以更新或删除可以用单一的读取的关键字来提取的一行： UPDATE tbl_name SET column=value WHERE unique_key_col=key_value; DELETE FROM tbl_name WHERE unique_key_col=key_value; SELECT结合并行的INSERT语句，并且只有很少的UPDATE或DELETE语句。 在整个表上有许多扫描或GROUP BY操作，没有任何写操作。 悲观锁与乐观锁 悲观锁与乐观锁是两种常见的资源并发锁设计思路，也是并发编程中一个非常基础的概念 悲观锁原理 特点是先获取锁，再进行业务操作 即“悲观”的认为所有的操作均会导致并发安全问题，因此要先确保获取锁成功再进行业务操作 通常来讲，在数据库上的悲观锁需要数据库本身提供支持，即通过常用的select … for update操作来实现悲观锁 当数据库执行select … for update时会获取被select中的数据行的行锁 因此其他并发执行的select … for update如果试图选中同一行则会发生排斥（需要等待行锁被释放） select for update获取的行锁会在当前事务结束时自动释放，因此必须在事务中使用 特别注意 不同的数据库对select… for update的实现和支持都是有所区别的 例如oracle支持select for update no wait，表示如果拿不到锁立刻报错，而不是等待，mysql就没有no wait这个选项 mysql还有个问题是:select… for update语句执行中所有扫描过的行都会被锁上，这一点很容易造成问题。因此，如果在mysql中用悲观锁务必要确定使用了索引，而不是全表扫描 乐观锁 乐观锁的特点先进行业务操作，只在最后实际更新数据时进行检查数据是否被更新过 若未被更新过，则更新成功；否则，失败重试 乐观锁在数据库上的实现完全是逻辑的，不需要数据库提供特殊的支持 一般的做法是在需要锁的数据上增加一个版本号或者时间戳,然后按照如下方式实现 123456781. SELECT data AS old_data, version AS old_version FROM …;2. 根据获取的数据进行业务操作，得到new_data和new_version3. UPDATE SET data = new_data, version = new_version WHERE version = old_versionif (updated row &gt; 0) &#123; // 乐观锁获取成功，操作完成&#125; else &#123; // 乐观锁获取失败，回滚并重试&#125; 乐观锁是否在事务中其实都是无所谓的 底层机制: 在数据库内部update同一行的时候是不允许并发的，即数据库每次执行一条update语句时会获取被update行的写锁，直到这一行被成功更新后才释放 因此在业务操作进行前获取需要锁的数据的当前版本号，然后实际更新数据时再次对比版本号确认与之前获取的相同，并更新版本号，即可确认这其间没有发生并发的修改 如果更新失败，即可认为老版本的数据已经被并发修改掉而不存在了，此时认为获取锁失败，需要回滚整个业务操作并可根据需要重试整个过程 悲观锁与乐观锁的应用场景 一般情况下，读多写少更适合用乐观锁，读少写多更适合用悲观锁 乐观锁在不发生取锁失败的情况下开销比悲观锁小 但是一旦发生失败回滚开销则比较大 因此乐观锁适合用在取锁失败概率比较小的场景，可以提升系统并发性能 共享锁和排他锁行级锁是MySQL中锁定粒度最细的一种锁，行级锁能大大减少数据库操作的冲突。行级锁分为共享锁和排他锁两种。 共享锁（Share Lock）共享锁又称读锁，是读取操作创建的锁。其他用户可以并发读取数据，但任何事务都不能对数据进行修改（获取数据上的排他锁），直到已释放所有共享锁。 如果事务T对数据A加上共享锁后，则其他事务只能对A再加共享锁，不能加排他锁。获准共享锁的事务只能读数据，不能修改数据。 用法1SELECT ... LOCK IN SHARE MODE; 在查询语句后面增加LOCK IN SHARE MODE，MySQL会对查询结果中的每行都加共享锁 当没有其他线程对查询结果集中的任何一行使用排他锁时，可以成功申请共享锁，否则会被阻塞。 其他线程也可以读取使用了共享锁的表，而且这些线程读取的是同一个版本的数据。 排他锁（eXclusive Lock）排他锁又称写锁，如果事务T对数据A加上排他锁后，则其他事务不能再对A加任任何类型的封锁。获准排他锁的事务既能读数据，又能修改数据。 用法1SELECT ... FOR UPDATE; 在查询语句后面增加FOR UPDATE，MySQL会对查询结果中的每行都加排他锁，当没有其他线程对查询结果集中的任何一行使用排他锁时，可以成功申请排他锁，否则会被阻塞。 意向锁意向锁是表级锁，其设计目的主要是为了在一个事务中揭示下一行将要被请求锁的类型。InnoDB中的两个表锁： 意向共享锁（IS）：表示事务准备给数据行加入共享锁，也就是说一个数据行加共享锁前必须先取得该表的IS锁 意向排他锁（IX）：类似上面，表示事务准备给数据行加入排他锁，说明事务在一个数据行加排他锁前必须先取得该表的IX锁。 意向锁是InnoDB自动加的，不需要用户干预。 对于insert、update、delete，InnoDB会自动给涉及的数据加排他锁（X）； 对于一般的Select语句，InnoDB不会加任何锁，事务可以通过以下语句给显示加共享锁或排他锁。 共享锁：SELECT ... LOCK IN SHARE MODE; 排他锁：SELECT ... FOR UPDATE; JDBC对事物的支持 对于JDBC而言，每条单独的语句都是一个事务，即每个语句后都隐含一个commit 实际上，Connection 提供了一个auto-commit的属性来指定事务何时结束 当auto-commit为true时，当每个独立SQL操作的执行完毕，事务立即自动提交，也就是说，每个SQL操作都是一个事务 当auto-commit为false时，每个事务都必须显式调用commit方法进行提交，或者显式调用rollback方法进行回滚 auto-commit默认为true 123456789try &#123; conn.setAutoCommit(false); //将自动提交设置为false ps.executeUpdate(&quot;修改SQL&quot;); //执行修改操作 ps.executeQuery(&quot;查询SQL&quot;); //执行查询操作 conn.commit(); //当两个操作成功后手动提交 &#125; catch (Exception e) &#123; conn.rollback(); //一旦其中一个操作出错都将回滚，使两个操作都不成功 e.printStackTrace(); &#125; 为了能够将多条SQL当成一个事务执行 首先通过Connection关闭auto-commit模式 然后通过Connection的setTransactionIsolation()方法设置事务的隔离级别 最后分别通过Connection的commit()方法和rollback()方法来提交事务和回滚事务 参考文章]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[重写、覆盖、重载、多态]]></title>
    <url>%2FJava%2F%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%2F%E9%87%8D%E5%86%99%E3%80%81%E8%A6%86%E7%9B%96%E3%80%81%E9%87%8D%E8%BD%BD%E3%80%81%E5%A4%9A%E6%80%81%2F</url>
    <content type="text"><![CDATA[在南京实习期间，封装了一个工具类库，其中大量使用了重载，公司的小伙伴竟然有人不知道重载和多态的区别。本文特地介绍重写、重载和多态三者的区别，一方面给需要的同学查阅，另一方面自己也留作记录。Java基础知识还是要了熟于心呀~ override 重写override是重写（覆盖）了一个方法，以实现不同的功能。一般是用于子类在继承父类时，重写（重新实现）父类中的方法。 重写（覆盖）的规则： 重写方法的参数列表必须完全与被重写的方法的相同,否则不能称其为重写而是重载. 重写方法的访问修饰符一定要大于被重写方法的访问修饰符public&gt;protected&gt;default&gt;private。 重写的方法的返回值必须和被重写的方法的返回一致； 重写的方法所抛出的异常必须和被重写方法的所抛出的异常一致，或者是其子类； 被重写的方法不能为private，否则在其子类中只是新定义了一个方法，并没有对其进行重写。 静态方法不能被重写为非静态的方法（会编译出错）。 overload 重载一般是用于在一个类内实现若干重载的方法，这些方法的名称相同而参数形式不同。 重载的规则： 在使用重载时只能通过相同的方法名、不同的参数形式实现。不同的参数类型可以是不同的参数类型，不同的参数个数，不同的参数顺序（参数类型必须不一样）； 不能通过访问权限、返回类型、抛出的异常进行重载； 方法的异常类型和数目不会对重载造成影响； 多态多态的概念比较复杂，有多种意义的多态，一个有趣但不严谨的说法是：继承是子类使用父类的方法，而多态则是父类使用子类的方法。 一般，我们使用多态是为了避免在父类里大量重载引起代码臃肿且难于维护。 例子12345678910111213141516171819202122232425262728293031323334353637383940public class Shape &#123; public static void main(String[] args) &#123; Triangle tri = new Triangle(); System.out.println("Triangle is a type of shape? " + tri.isShape());// 继承 Shape shape = new Triangle(); System.out.println("My shape has " + shape.getSides() + " sides."); // 多态 Rectangle Rec = new Rectangle(); Shape shape2 = Rec; System.out.println("My shape has " + shape2.getSides(Rec) + " sides."); //重载 &#125; public boolean isShape() &#123; return true; &#125; public int getSides() &#123; return 0; &#125; public int getSides(Triangle tri) &#123; //重载 return 3; &#125; public int getSides(Rectangle rec) &#123; //重载 return 4; &#125;&#125;class Triangle extends Shape &#123; @Override public int getSides() &#123; //重写,实现多态 return 3; &#125;&#125;class Rectangle extends Shape &#123; public int getSides(int i) &#123; //重载 return i; &#125;&#125;]]></content>
      <categories>
        <category>Java</category>
        <category>面向对象</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>面向对象</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[五种设计原则]]></title>
    <url>%2FJava%2F%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%2F%E4%BA%94%E7%A7%8D%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99%2F</url>
    <content type="text"><![CDATA[面向对象设计五大原则”和良性依赖原则在应付变化方面的作用。本文重点介绍了这五种设计原则和良性依赖原则。 单一职责原则“对一个类而言，应该仅有一个引起它变化的原因。” 本原则是我们非常熟悉地”高内聚性原则”的引申，但是通过将”职责”极具创意地定义为”变化的原因”，使得本原则极具操作性，尽显大师风范。 同时，本原则还揭示了内聚性和耦合生，基本途径就是提高内聚性；如果一个类承担的职责过多，那么这些职责就会相互依赖，一个职责的变化可能会影响另一个职责的履行。 其实OOD的实质，就是合理地进行类的职责分配。 开放封闭原则“软件实体应该是可以扩展的，但是不可修改。” 本原则紧紧围绕变化展开，变化来临时，如果不必改动软件实体的源代码，就能扩充它的行为，那么这个软件实体设计就是满足开放封闭原则的。 如果说我们预测到某种变化，或者某种变化发生了，我们应当创建抽象类来隔离以后发生的同类变化。 在Java中，这种抽象是指抽象基类或接口；在C++中，这各抽象是指抽象基类或纯抽象基类。 当然，没有对所有情况都贴切的模型，我们必须对软件实体应该面对的变化做出选择。 Liskov替换原则“子类型必须能够替换掉它们的基类型。” 本原则和开放封闭原则关系密切，正是子类型的可替换性，才使得使用基类型模块无需修改就可扩充。 Liskov替换原则从基于契约的设计演化而来，契约通过为每个方法声明”先验条件”和”后验条件”；定义子类时，必须遵守这些”先验条件”和”后验条件”。 当前基于契的设计发展势头正劲，对实现”软件工厂”的”组装生产”梦想是一个有力的支持。 依赖倒置原则“抽象不应依赖于细节，细节应该依赖于抽象。” 本原则几乎就是软件设计的正本清源之道。 因为人解决问题的思考过程是先抽象后具体，从笼统到细节，所以我们先生产出的势必是抽象程度比较高的实体，而后才是更加细节化的实体。 于是，”细节依赖于抽象”就意味着后来的依赖于先前的，这是自然而然的重用之道。 而且，抽象的实体代表着笼而统之的认识，人们总是比较容易正确认识它们，而且本身也是不易变的，依赖于它们是安全的。 依赖倒置原则适应了人类认识过程的规律，是面向对象设计的标志所在。 接口隔离原则“多个专用接口优于一个单一的通用接口。” 本原则是单一职责原则用于接口设计的自然结果。 一个接口应该保证，实现该接口的实例对象可以只呈现为单一的角色；这样，当某个客户程序的要求发生变化，而迫使接口发生改变时，影响到其他客户程序的可能生性小。 良性依赖原则“不会在实际中造成危害的依赖关系，都是良性依赖。” 通过分析不难发现，本原则的核心思想是”务实”，很好地揭示了极限编程(Extreme Programming)中”简单设计”和”重构”的理论基础。 本原则可以帮助我们抵御”面向对象设计五大原则”以及设计模式的诱惑，以免陷入过度设计(Over-engineering)的尴尬境地，带来不必要的复杂性。]]></content>
      <categories>
        <category>Java</category>
        <category>面向对象</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>面向对象</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面向对象的三个基本特征]]></title>
    <url>%2FJava%2F%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%2F%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%9A%84%E4%B8%89%E4%B8%AA%E5%9F%BA%E6%9C%AC%E7%89%B9%E5%BE%81%2F</url>
    <content type="text"><![CDATA[面向对象的三个基本特征是：封装、继承、多态。在很多面试场合也会频频问起，今天我们也来聊一聊这几个特征。 封装封装最好理解了。封装是面向对象的特征之一，是对象和类概念的主要特性。 封装，也就是把客观事物封装成抽象的类，并且类可以把自己的数据和方法只让可信的类或者对象操作，对不可信的进行信息隐藏。 继承面向对象编程(OOP)语言的一个主要功能就是“继承”。继承是指这样一种能力：它可以使用现有类的所有功能，并在无需重新编写原来的类的情况下对这些功能进行扩展。 通过继承创建的新类称为“子类”或“派生类”。 被继承的类称为“基类”、“父类”或“超类”。 继承的过程，就是从一般到特殊的过程。 要实现继承，可以通过“继承”（Inheritance）和“组合”（Composition）来实现。 在某些OOP语言中，一个子类可以继承多个基类。但是一般情况下，一个子类只能有一个基类，要实现多重继承，可以通过多级继承来实现。 实现方式继承概念的实现方式有三类：实现继承、接口继承和可视继承。 实现继承是指使用基类的属性和方法而无需额外编码的能力； 接口继承是指仅使用属性和方法的名称、但是子类必须提供实现的能力； 可视继承是指子窗体（类）使用基窗体（类）的外观和实现代码的能力。 在考虑使用继承时，有一点需要注意，那就是两个类之间的关系应该是“属于”关系。例如，Employee是一个人，Manager也是一个人，因此这两个类都可以继承Person类。但是Leg类却不能继承Person类，因为腿并不是一个人。 抽象类仅定义将由子类创建的一般属性和方法，创建抽象类时，请使用关键字Interface而不是Class。 OO开发范式大致为：划分对象→抽象类→将类组织成为层次化结构(继承和合成) →用类与实例进行设计和实现几个阶段。 多态多态性（polymorphisn）是允许你将父对象设置成为和一个或更多的他的子对象相等的技术，赋值之后，父对象就可以根据当前赋值给它的子对象的特性以不同的方式运作。简单的说，就是一句话：允许将子类类型的指针赋值给父类类型的指针。 实现多态，有二种方式，覆盖，重载。 覆盖，是指子类重新定义父类的虚函数的做法。 重载，是指允许存在多个同名函数，而这些函数的参数表不同（或许参数个数不同，或许参数类型不同，或许两者都不同）。 其实，重载的概念并不属于“面向对象编程”，重载的实现是：编译器根据函数不同的参数表，对同名函数的名称做修饰，然后这些同名函数就成了不同的函数（至少对于编译器来说是这样的）。 如，有两个同名函数：function func(p:integer):integer;和function func(p:string):integer;。那么编译器做过修饰后的函数名称可能是这样的：int_func、str_func。对于这两个函数的调用，在编译器间就已经确定了，是静态的（记住：是静态）。也就是说，它们的地址在编译期就绑定了（早绑定），因此，重载和多态无关！ 真正和多态相关的是“覆盖”。当子类重新定义了父类的虚函数后，父类指针根据赋给它的不同的子类指针，动态（记住：是动态！）的调用属于子类的该函数，这样的函数调用在编译期间是无法确定的（调用的子类的虚函数的地址无法给出）。因此，这样的函数地址是在运行期绑定的（晚邦定）。 结论就是：重载只是一种语言特性，与多态无关，与面向对象也无关！引用一句Bruce Eckel的话：“不要犯傻，如果它不是晚绑定，它就不是多态。” 多态的作用封装可以隐藏实现细节，使得代码模块化； 继承可以扩展已存在的代码模块（类）； 它们的目的都是为了——代码重用。 而多态则是为了实现另一个目的——接口重用！多态的作用，就是为了类在继承和派生的时候，保证使用“家谱”中任一类的实例的某一属性时的正确调用。]]></content>
      <categories>
        <category>Java</category>
        <category>面向对象</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>面向对象</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LinkedList源码阅读]]></title>
    <url>%2FJava%2F%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%2FLinkedList%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%2F</url>
    <content type="text"><![CDATA[在日常的开发中，我们使用的比较多的可能是ArrayList，前面我们也进行了ArrayList源码阅读。ArrayList不能维护元素插入的顺序，当我们需要保持元素的插入顺序时，就需要使用LinkedList了。今天我们就来简单分析一下LinkedList的源码实现。 本文基于JDK1.8 内部数据结构Java中的LinkedList类实现了List接口和Deque接口，是一种链表类型的数据结构，支持高效的插入和删除操作，同时也实现了Deque接口，使得LinkedList类也具有队列的特性。LinkedList类的底层实现的数据结构是一个双端的链表。 LinkedList类中有一个内部私有类Node，这个类就代表双端链表的节点Node。这个类有三个属性，分别是前驱节点，本节点的值，后继结点。源码中的实现是这样的：1234567891011private static class Node&lt;E&gt; &#123; E item; Node&lt;E&gt; next; Node&lt;E&gt; prev; Node(Node&lt;E&gt; prev, E element, Node&lt;E&gt; next) &#123; this.item = element; this.next = next; this.prev = prev; &#125;&#125; 注意这个节点的初始化方法，给定三个参数，分别前驱节点，本节点的值，后继结点。这个方法将在LinkedList的实现中多次调用。 下图是LinkedList内部结构的可视化，能够帮我们更好的理解LinkedList内部的结构 双端链表由node组成，每个节点有两个reference指向前驱节点和后继结点，第一个节点的前驱节点为null，最后一个节点的后继节点为null。 LinkedList类有很多方法供我们调用。我们不会一一介绍，本文会详细介绍其中几个最核心最基本的方法，LinkedList的创建添加和删除基本都和这几个操作有关。 linkFirst() method这个方法是插入第一个节点1234567891011121314/** * Links e as first element. */private void linkFirst(E e) &#123; final Node&lt;E&gt; f = first; final Node&lt;E&gt; newNode = new Node&lt;&gt;(null, e, f); first = newNode; if (f == null) last = newNode; else f.prev = newNode; size++; modCount++;&#125; 我们发现出现了两个变量，first和last这两个变量是LinkedList的成员变量，分别指向头结点和尾节点。他们是如下定义的：123456/** * Pointer to first node. * Invariant: (first == null &amp;&amp; last == null) || * (first.prev == null &amp;&amp; first.item != null) */transient Node&lt;E&gt; first; 123456/** * Pointer to last node. * Invariant: (first == null &amp;&amp; last == null) || * (last.next == null &amp;&amp; last.item != null) */transient Node&lt;E&gt; last; 我们可以看到注释中的内容。first和last需要维持一个不变量，也就是first和last始终都要维持两种状态： 如果双端链表为空的时候，两个都必须为null 如果链表不为空，那么first的前驱节点一定是null，first的item一定不为null，同理，last的后继节点一定是null，last的item一定不为null。 知道了first和last之后，我们就可以开始分析linkFirst的代码了。 linkFirst的作用就是在first节点的前面插入一个节点，插入完之后，还要更新first节点为新插入的节点，并且同时维持last节点的不变量。 我们开始分析代码，首先用f来临时保存未插入前的first节点，然后调用的node的构造函数新建一个值为e的新节点，这个节点插入之后将作为first节点，所以新节点的前驱节点为null，值为e，后继节点是f,也就是未插入前的first节点。 然后就是维持不变量，首先第一种情况，如果f==null，那就说明插入之前，链表是空的，那么新插入的节点不仅是first节点还是last节点，所以我们要更新last节点的状态，也就是last现在要指向新插入的newNode。 如果f!=null那么就说明last节点不变，但是要更新f的前驱节点为newNode，维持first节点的不变量。 最后size加一就完成了操作。 linkLast() method分析了linkFirst方法，对于 linkLast()的代码就很容易理解了，只不过是变成了插入到last节点的后面。我们直接看代码1234567891011121314/** * Links e as last element. */void linkLast(E e) &#123; final Node&lt;E&gt; l = last; final Node&lt;E&gt; newNode = new Node&lt;&gt;(l, e, null); last = newNode; if (l == null) first = newNode; else l.next = newNode; size++; modCount++;&#125; 到这里我们发现有这个两个方法，我们已经可以实现一个简单队列的插入操作，上面两个方法就可以理解为插入队头元素和队尾元素，这也说明了LinkedList是实现了Deque接口的。 从源码中也可以看出，addfirst和addLast这两个方法内部就是直接调用了linkFirst和LinkLast12345678910111213141516171819/** * Inserts the specified element at the beginning of this list. * * @param e the element to add */public void addFirst(E e) &#123; linkFirst(e);&#125;/** * Appends the specified element to the end of this list. * * &lt;p&gt;This method is equivalent to &#123;@link #add&#125;. * * @param e the element to add */public void addLast(E e) &#123; linkLast(e);&#125; linkBefore(E e, Node succ)下面我们看一个linkBefore方法,从名字可以看出这个方法是在给定的节点前插入一个节点，可以说是linkFirst和linkLast方法的通用版。123456789101112131415/** * Inserts element e before non-null Node succ. */void linkBefore(E e, Node&lt;E&gt; succ) &#123; // assert succ != null; final Node&lt;E&gt; pred = succ.prev; final Node&lt;E&gt; newNode = new Node&lt;&gt;(pred, e, succ); succ.prev = newNode; if (pred == null) first = newNode; else pred.next = newNode; size++; modCount++;&#125; 我们可以看到代码的实现原理基本和前面的两个方法一致，这里是假设插入的这个节点的位置是非空的。 add(int index, E element)下面我们看add方法，这个方法就是最常用的，在指定下标插入一个节点。我们先来看下源码的实现，很简单1234567891011121314151617/** * Inserts the specified element at the specified position in this list. * Shifts the element currently at that position (if any) and any * subsequent elements to the right (adds one to their indices). * * @param index index at which the specified element is to be inserted * @param element element to be inserted * @throws IndexOutOfBoundsException &#123;@inheritDoc&#125; */public void add(int index, E element) &#123; checkPositionIndex(index); if (index == size) linkLast(element); else linkBefore(element, node(index));&#125; 首先判断给定的index是不是合法的，然后如果index==size，就说明要插入成为最后一个节点，直接调用linklast方法，否则就调用linkBefore方法，我们知道linkBefore需要给定两个参数，一个插入节点的值，一个指定的node，所以我们又调用了Node(index)去找到index的那个node。 我们看一下Node node(int index)方法，这个方法就是找到给定index的node并返回，类似于数组的随机读取，但由于这里是链表，所以要进行查找123456789101112131415161718/** * Returns the (non-null) Node at the specified element index. */Node&lt;E&gt; node(int index) &#123; // assert isElementIndex(index); if (index &lt; (size &gt;&gt; 1)) &#123; Node&lt;E&gt; x = first; for (int i = 0; i &lt; index; i++) x = x.next; return x; &#125; else &#123; Node&lt;E&gt; x = last; for (int i = size - 1; i &gt; index; i--) x = x.prev; return x; &#125;&#125; 我们看到node的实现并不是像我们想象的那样直接就线性从头查找，而是折半查找，有一个小优化，先判断index在前半段还是后半段，如果在前半段就从头开始找，如果在后半段就从后开始找，这样最坏情况也只要找一半就可以了。 转自 Java源码剖析之LinkedList]]></content>
      <categories>
        <category>Java</category>
        <category>源码阅读</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>源码阅读</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java中四种创建对象的方式]]></title>
    <url>%2FJava%2F%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%2FJava%E4%B8%AD%E5%9B%9B%E7%A7%8D%E5%88%9B%E5%BB%BA%E5%AF%B9%E8%B1%A1%E7%9A%84%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[调用new语句创建对象 调用对象的clone()方法 运用反射手段创建对象 运用反序列化手段 调用new语句创建对象12// 使用java语言的关键字 new 创建对象，初始化对象数据 MyObject mo = new MyObject() ; 调用对象的clone()方法12MyObject anotherObject = new MyObject();MyObject object = anotherObject.clone(); 使用 clone()方法克隆一个对象的步骤： 被克隆的类要实现 Cloneable 接口。 被克隆的类要重写 clone(）方法。 原型模式主要用于对象的复制，实现一个接口（实现Cloneable接口），重写一个方法（重写Object类中的clone方法），即完成了原型模式。 原型模式中的拷贝分为”浅拷贝”和”深拷贝”: 浅拷贝：对值类型的成员变量进行值的复制，对引用类型的成员变量只复制引用，不复制引用的对象. 深拷贝：对值类型的成员变量进行值的复制，对引用类型的成员变量也进行引用对象的复制. （Object 类的 clone 方法只会拷贝对象中的基本数据类型的值，对于数组、容器对象、引用对象等都不会拷贝，这就是浅拷贝。如果要实现深拷贝，必须将原型模式中的数组、容器对象、引用对象等另行拷贝。） 原型模式的优点 如果创建新的对象比较复杂时，可以利用原型模式简化对象的创建过程。 使用原型模式创建对象比直接 new 一个对象在性能上要好的多，因为Object 类的 clone 方法是一个本地方法，它直接操作内存中的二进制流，特别是复制大对象时，性能的差别非常明显。 原型模式的使用场景因为以上优点，所以在需要重复地创建相似对象时可以考虑使用原型模式。 比如需要在一个循环体内创建对象，假如对象创建过程比较复杂或者循环次数很多的话，使用原型模式不但可以简化创建过程，而且可以使系统的整体性能提高很多。 运用反射手段创建对象反射的定义 反射机制是在运行时， 对于任意一个类， 都能够知道这个类的所有属性和方法； 对于任意一个对象， 都能够调用它的任意一个方法。 在Java中，只要给定类的名字， 那么就可以通过反射机制来获得类的所有信息。 反射机制主要提供了以下功能： 在运行时判定任意一个对象所属的类； 在运行时创建对象； 在运行时判定任意一个类所具有的成员变量和方法； 在运行时调用任意一个对象的方法； 生成动态代理。 哪里用到反射机制？jdbc 中有一行代码：Class.forName(‘com.mysql.jdbc.Driver.class’);//加载MySql的驱动类。这就是反射。 现在很多框架都用到反射机制， hibernate， struts 都是用反射机制实现的。 反射的实现方式在 Java 中实现反射最重要的一步， 也是第一步就是获取Class对象，得到Class对象后可以通过该对象调用相应的方法来获取该类中的属性、方法以及调用该类中的方法。 有 4 种方法可以得到 Class 对象： Class.forName(“类的路径” ); 类名.class 对象名.getClass() 如果是基本类型的包装类， 则可以通过调用包装类的 Type 属性来获得该包装类的 Class 对象, Class&lt;?&gt; clazz = Integer.TYPE;实现Java反射的类 Class：它表示正在运行的 Java 应用程序中的类和接口。 Field：提供有关类或接口的属性信息， 以及对它的动态访问权限。 Constructor：提供关于类的单个构造方法的信息以及对它的访问权限 Method：提供关于类或接口中某个方法信息。 注意：Class类是Java反射中最重要的一个功能类，所有获取对象的信息(包括： 方法/属性/构造方法/访问权限)都需要它来实现。 反射机制的优缺点优点 能够运行时动态获取类的实例， 大大提高程序的灵活性。 与 Java 动态编译相结合， 可以实现无比强大的功能。 缺点 使用反射的性能较低。Java反射是要解析字节码，将内存中的对象进行解析。 解决方案： 由于JDK的安全检查耗时较多， 所以通过setAccessible(true)的方式关闭安全检查来（取消对访问控制修饰符的检查） 提升反射速度。 需要多次动态创建一个类的实例的时候，有缓存的写法会比没有缓存要快很多: ReflectASM 工具类 ， 通过字节码生成的方式加快反射速度。 使用反射相对来说不安全，破坏了类的封装性，可以通过反射获取这个类的私有方法和属性。 运用反序列化手段Java序列化是指把Java对象转换为字节序列的过程；而Java反序列化是指把字节序列恢复为Java对象的过程。 为什么需要序列化与反序列化我们知道，当两个进程进行远程通信时，可以相互发送各种类型的数据，包括文本、图片、音频、视频等，而这些数据都会以二进制序列的形式在网络上传送。那么当两个Java进程进行通信时，能否实现进程间的对象传送呢？答案是可以的。如何做到呢？这就需要Java序列化与反序列化了。换句话说，一方面，发送方需要把这个Java对象转换为字节序列，然后在网络上传送；另一方面，接收方需要从字节序列中恢复出Java对象。基本原理和网络通信是一致的，通过特殊的编码方式：写入数据将对象以及其内部数据编码，存在在数组或者文件里面然后发送到目的地后，再进行解码，读出数据。 当我们明晰了为什么需要Java序列化和反序列化后，我们很自然地会想Java序列化的好处。 实现了数据的持久化，通过序列化可以把数据永久地保存到硬盘上（通常存放在文件里） 利用序列化实现远程通信，即在网络上传送对象的字节序列。 对象序列化 java.io.ObjectOutputStream代表对象输出流，它的writeObject(Object obj)方法可对参数指定的obj对象进行序列化，把得到的字节序列写到一个目标输出流中。只有实现了Serializable和Externalizable接口的类的对象才能被序列化。 java.io.ObjectInputStream代表对象输入流，它的readObject()方法从一个源输入流中读取字节序列，再把它们反序列化为一个对象，并将其返回。]]></content>
      <categories>
        <category>Java</category>
        <category>面向对象</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>对象创建</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[forward和sendRedirect区别]]></title>
    <url>%2Fweb%E5%BC%80%E5%8F%91%2Fforward%E5%92%8CsendRedirect%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[之前知道forward后地址栏地址不变，redirect后地址了那会发生变化 但是今天遇到一个问题，forward和redirect后，其后的代码段还会继续执行吗？ redirect redirect 后，确认了要跳转的页面的 url，继续执行 redirect 下面的代码 执行完后，断开当前的与用户所发出的请求连接，即断开 request 的引用指向 request 里存放的信息也会丢失 然后再与用户建立新的请求连接，即创建新的 request 对象，地址栏地址变成新的页面的地址 forward forward后，确认了要跳转的页面的 url，停止继续执行后面的代码 先执行要跳转url里的代码 执行完毕后，再回来继续执行当前页面的代码 这期间二者共享一个 request 和 response 对象 这个过程，最后还是执行的原来的servlet，所以地址栏的地址不会变化 以登陆为例页面123456&lt;form action="CheckUser" method="post"&gt; username:&lt;input type="text" name="username" /&gt;&lt;br&gt; password:&lt;input type="password" name="password" /&gt;&lt;br&gt; &lt;input type="submit" value="submit" /&gt; &lt;input type="reset" value="reset" /&gt;&lt;/form&gt; check的servlet123456789101112String username = request.getParameter("username");String password = request.getParameter("password"); request.setAttribute("user", username); if (username.equals(password)) &#123; getServletContext().getRequestDispatcher("/success").forward(request, response); System.out.println("-----this is forward end-----");&#125; else &#123; response.sendRedirect("false"); System.out.println("---this is redirect end---");&#125; success的servlet12345678910PrintWriter out = response.getWriter();String username = (String) request.getAttribute("user");out.println("");out.println("");out.println("Welcome you " + username);out.println("");out.println("");System.out.println("---- this is success servlet end ----"); false的servlet12345678910PrintWriter out = response.getWriter(); String username = (String) request.getAttribute("user"); out.println("");out.println("");out.println(username + " password ERROR!");out.println("");out.println("");System.out.println("---- this is false servlet end ----"); 运行结果1234---- this is success servlet end -------- this is forward end--------- this is redirect end------- this is false servlet end ----]]></content>
      <categories>
        <category>web开发</category>
      </categories>
      <tags>
        <tag>web开发</tag>
        <tag>页面跳转</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java中静态初始化块、初始化块和构造方法]]></title>
    <url>%2FJava%2FJava%E5%9F%BA%E7%A1%80%2FJava%E4%B8%AD%E9%9D%99%E6%80%81%E5%88%9D%E5%A7%8B%E5%8C%96%E5%9D%97%E3%80%81%E5%88%9D%E5%A7%8B%E5%8C%96%E5%9D%97%E5%92%8C%E6%9E%84%E9%80%A0%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[在Java中,有两种初始化块:静态初始化块和非静态初始化块。它们都是定义在类中，用大括号{}括起来，静态代码块在大括号外还要加上static关键字。 本文详细介绍了静态初始化块、初始化块和构造方法三者之间的异同，以帮助读者理清Java类加载后的各代码块的执行顺序问题。 非静态初始化块（构造代码块）：作用：给对象进行初始化。对象一建立就运行，且优先于构造函数的运行。 与构造函数的区别：非静态初始化块给所有对象进行统一初始化，构造函数只给对应对象初始化。 应用：将所有构造函数共性的东西定义在构造代码块中。 对于普通的类而言，可以放在初始化块中的初始化工作其实完全可以放到构造函数中进行，只不过有时会带来些许不便，如有多个构造器，就要在多个地方加上初始化函数完成初始化工作，而如果放到初始化块中的话则只要写一次即可。 但是，如果只是这一点便利的话，还不足以使用初始化块，其实初始化块真正体现其独一无二的作用是在匿名内部类中，由于是匿名内部类，因而无法写构造方法，但是很多时候还是要完成相应的初始化工作，这时就需要用到初始化块了，特别是Android中大量地使用匿名内部类，初始化块的作用就十分突出。 静态初始化块作用：给类进行初始化。随着类的加载而执行，且只执行一次 与构造代码块的区别： 构造代码块用于初始化对象，每创建一个对象就会被执行一次；静态代码块用于初始化类，随着类的加载而执行，不管创建几个对象，都只执行一次。 静态代码块优先于构造代码块的执行 都定义在类中，一个带static关键字，一个不带static 比如可以记录第一次访问类的日志，或方便单例模式的初始化等。对于单例模式，可以先用static块初始化一些可能还被其他类访问的基础参数，等到真正需要加载大量资源的时候(getInstance)再构造单体，在构造函数中加载资源。 构造函数、非静态初始化块、静态代码块都是用于初始化，三者的执行顺序依次是：静态代码块 &gt; 构造代码块 &gt; 构造函数。 其实初始化块就是构造器的补充，初始化块是不能接收任何参数的，定义的一些所有对象共有的属性、方法等内容时就可以用初始化块初始化了。 静态初始化块的作用就是当JVM在装载类时，你想让它做一些事情，那么，就可以用静态初始化块。这几者的执行顺序是： （JVM在装载类时）先装载类的静态成员，再执行静态初始化块（同样，当一个类有继承自某类时，则会先装载该父类，那么，父类的装载或执行顺序，也都如句子所述）。 （在创建类的实例时）先执行实例初始化块，再执行构造方法；但对于一棵继承树中，会先调用父类的构造方法，那么其执行顺序也如句子所述。 执行顺序所有的静态初始化块都优先执行，其次才是非静态的初始化块和构造函数，它们的执行顺序是：123456父类的静态初始化块子类的静态初始化块父类的初始化块父类的构造函数子类的初始化块子类的构造函数 转自 深入理解Java中静态初始化块、初始化块和构造方法]]></content>
      <categories>
        <category>Java</category>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ArrayList源码阅读]]></title>
    <url>%2FJava%2F%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%2FArrayList%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%2F</url>
    <content type="text"><![CDATA[ArrayList是我们经常使用的一个数据结构，我们通常把其用作一个可变长度的动态数组使用，大部分时候，可以替代数组的作用，我们不用事先设定ArrayList的长度，只需要往里不断添加元素即可，ArrayList会动态增加容量。ArrayList是作为List接口的一个实现。 那么ArrayList背后使用的数据结构是什么呢？ ArrayList是如何保证动态增加容量，使得能够正确添加元素的呢？ 要回答上面的问题，我们就需要对ArrayList的源码进行一番分析，深入了解其实现原理的话，我们就自然能够解答上述问题。 本文基于JDK1.8 ArrayList使用的存储的数据结构从源码中我们可以发现，ArrayList使用的存储的数据结构是Object的对象数组。 其实这也不能想象，我们知道ArrayList是支持随机存取的类似于数组，所以自然不可能是链表结构。1234567/** * The array buffer into which the elements of the ArrayList are stored. * The capacity of the ArrayList is the length of this array buffer. Any * empty ArrayList with elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA * will be expanded to DEFAULT_CAPACITY when the first element is added. */transient Object[] elementData; // non-private to simplify nested class access 我想大家一定对这里出现的transient关键字很疑惑，我们都知道ArrayList对象是可序列化的，但这里为什么要用transient关键字修饰它呢？查看源码，我们发现ArrayList实现了自己的readObject和writeObject方法，所以这保证了ArrayList的可序列化。 ArrayList的初始化ArrayList提供了三个构造函数。下面我们依次来分析 public ArrayList(int initialCapacity)当我们初始化的时候，给ArrayList指定一个初始化大小的时候，就会调用这个构造方法。 源码中这个方法的实现： 12345678910111213141516/** * Constructs an empty list with the specified initial capacity. * * @param initialCapacity the initial capacity of the list * @throws IllegalArgumentException if the specified initial capacity * is negative */public ArrayList(int initialCapacity) &#123; if (initialCapacity &gt; 0) &#123; this.elementData = new Object[initialCapacity]; &#125; else if (initialCapacity == 0) &#123; this.elementData = EMPTY_ELEMENTDATA; &#125; else &#123; throw new IllegalArgumentException("Illegal Capacity: " + initialCapacity); &#125;&#125; 这里的EMPTY_ELEMENTDATA实际上就是一个共享的空的Object数组对象。 1234/** * Shared empty array instance used for empty instances. */private static final Object[] EMPTY_ELEMENTDATA = &#123;&#125;; 上述代码很容易理解，如果用户指定的初始化容量大于0，就new一个相应大小的数组，如果指定的大小为0，就复制为共享的那个空的Object数组对象。如果小于0，就直接抛出异常。 public ArrayList()默认的空的构造函数。 源码中的实现： 123456/** * Constructs an empty list with an initial capacity of ten. */public ArrayList() &#123; this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA;&#125; 其中DEFAULTCAPACITY_EMPTY_ELEMENTDATA定义为 123456/** * Shared empty array instance used for default sized empty instances. We * distinguish this from EMPTY_ELEMENTDATA to know how much to inflate when * first element is added. */private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = &#123;&#125;; 注释中解释的很清楚，就是说刚初始化的时候，会是一个共享的类变量，也就是一个Object空数组，当第一次add的时候，这个数组就会被初始化一个大小为10的数组。 public ArrayList(Collection&lt;? extends E&gt; c)如果我们想要初始化一个list，这个list包含另外一个特定的collection的元素，那么我们就可以调用这个构造函数。 源码中的实现 12345678910111213141516171819/** * Constructs a list containing the elements of the specified * collection, in the order they are returned by the collection's * iterator. * * @param c the collection whose elements are to be placed into this list * @throws NullPointerException if the specified collection is null */public ArrayList(Collection&lt;? extends E&gt; c) &#123; elementData = c.toArray(); if ((size = elementData.length) != 0) &#123; // c.toArray might (incorrectly) not return Object[] (see 6260652) if (elementData.getClass() != Object[].class) elementData = Arrays.copyOf(elementData, size,Object[].class); &#125; else &#123; // replace with empty array. this.elementData = EMPTY_ELEMENTDATA; &#125;&#125; 首先调用给定的collection的toArray方法将其转换成一个Array。 然后根据这个array的大小进行判断，如果不为0，就调用Arrays的copyOf的方法，复制到Object数组中，完成初始化，如果为0，就直接初始化为空的Object数组。 ArrayList是如何动态增长当我们像一个ArrayList中添加数组的时候，首先会先检查数组中是不是有足够的空间来存储这个新添加的元素。如果有的话，那就什么都不用做，直接添加。如果空间不够用了，那么就根据原始的容量增加原始容量的一半。源码中是如此实现的：1234567891011/** * Appends the specified element to the end of this list. * * @param e element to be appended to this list * @return &lt;tt&gt;true&lt;/tt&gt; (as specified by &#123;@link Collection#add&#125;) */public boolean add(E e) &#123; ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true;&#125; ensureCapacityInternal的实现如下：123456private void ensureCapacityInternal(int minCapacity) &#123; if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123; minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); &#125; ensureExplicitCapacity(minCapacity); &#125; DEFAULT_CAPACITY为：1private static final int DEFAULT_CAPACITY = 10; 这也就实现了当我们不指定初始化大小的时候，添加第一个元素的时候，数组会扩容为10.1234567private void ensureExplicitCapacity(int minCapacity) &#123; modCount++; // overflow-conscious code if (minCapacity - elementData.length &gt; 0) grow(minCapacity);&#125; 这个函数判断是否需要扩容，如果需要就调用grow方法扩容1234567891011121314151617/** * Increases the capacity to ensure that it can hold at least the * number of elements specified by the minimum capacity argument. * * @param minCapacity the desired minimum capacity */private void grow(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity);&#125; 我们可以看到grow方法将数组扩容为原数组的1.5倍，调用的是Arrays.copy方法。在JDK6及之前的版本中，采用的还不是右移的方法。1int newCapacity = (oldCapacity * 3)/2 + 1; 现在已经优化成右移了。1int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); ArrayList如何实现元素的移除我们移除元素的时候，有两种方法，一是指定下标，二是指定对象 public E remove(int index) 12345678910111213141516171819202122/** * Removes the element at the specified position in this list. * Shifts any subsequent elements to the left (subtracts one from their * indices). * * @param index the index of the element to be removed * @return the element that was removed from the list * @throws IndexOutOfBoundsException &#123;@inheritDoc&#125; */public E remove(int index) &#123; rangeCheck(index); modCount++; E oldValue = elementData(index); int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index,numMoved); elementData[--size] = null; // clear to let GC do its work return oldValue;&#125; 对于数组的元素删除算法我们应该很熟悉，删除一个数组元素，我们需要将这个元素后面的元素全部向前移动，并将size减1. 我们看到源码中，首先检查下标是否在可用范围内。然后调用System.arrayCopy方法将右边的数组向左移动，并且将size减一，并置为null。 public boolean remove(Object o) 1234567891011121314151617181920212223242526272829/** * Removes the first occurrence of the specified element from this list, * if it is present. If the list does not contain the element, it is * unchanged. More formally, removes the element with the lowest index * &lt;tt&gt;i&lt;/tt&gt; such that * &lt;tt&gt;(o==null ? get(i)==null : o.equals(get(i)))&lt;/tt&gt; * (if such an element exists). Returns &lt;tt&gt;true&lt;/tt&gt; if this list * contained the specified element (or equivalently, if this list * changed as a result of the call). * * @param o element to be removed from this list, if present * @return &lt;tt&gt;true&lt;/tt&gt; if this list contained the specified element */public boolean remove(Object o) &#123; if (o == null) &#123; for (int index = 0; index &lt; size; index++) if (elementData[index] == null) &#123; fastRemove(index); return true; &#125; &#125; else &#123; for (int index = 0; index &lt; size; index++) if (o.equals(elementData[index])) &#123; fastRemove(index); return true; &#125; &#125; return false;&#125; 我们可以看到，这个remove方法会移除数组中第一个符合的给定对象，如果不存在就什么也不做，如果存在多个只移除第一个。 fastRemove方法如下 1234567891011/* * Private remove method that skips bounds checking and does not * return the value removed. */private void fastRemove(int index) &#123; modCount++; int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index,numMoved); elementData[--size] = null; // clear to let GC do its work&#125; 可以理解为简化版的remove（index）方法。 ArrayList小结 ArrayList是List接口的一个可变大小的数组的实现 ArrayList的内部是使用一个Object对象数组来存储元素的 初始化ArrayList的时候，可以指定初始化容量的大小，如果不指定，就会使用默认大小，为10 当添加一个新元素的时候，首先会检查容量是否足够添加这个元素，如果够就直接添加，如果不够就进行扩容，扩容为原数组容量的1.5倍 当删除一个元素的时候，会将数组右边的元素全部左移 转自 Java源码剖析之ArrayList]]></content>
      <categories>
        <category>Java</category>
        <category>源码阅读</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>源码阅读</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scrum要素读书笔记]]></title>
    <url>%2F%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%2FScrum%E8%A6%81%E7%B4%A0%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Scrum是一种迭代增量式的软件开发过程，用于敏捷软件开发。Scrum是一个包括一系列实践和预定义角色的过程框架。 Scrum要素包括角色、周期、工件，以及如何确定用户故事、如何估算工作，如何召开每日站立会议。 当前，Scrum方法在国内已经逐渐普及，为众多知名IT公司和软件开发团队采用。 本书是帮助软件开发人员认识、初步了解Scrum方法的佳作。通过本书可以理清Scrum的相关知识和概念，为采用和实践Scrum方法做好充分准备。 Scrum团队周记工作项目的候选列表，新特性和错误修复的工作都有，这些都是项目上最重要的代办事项。 白纸板是用来文字记录的，例如，团队都认可的故事“完成”定义。 Sprint燃尽图，用来监测下一周任务完成过程中的速度变化。 Scrum日会是一种短会，用于团结和协调团队 。为了鼓励大家都简洁点，这个会是站着开的。它因此而得名“每日站会”。 团队成员轮流分享信息：前一天完成了什么任务，明天的Scrum日会前打算做哪个任务，有没有碰到什么障碍或是受到了什么拖累。 第一部分：敏捷力介绍瀑布模型将开发和交付且软件项目的流程分割为相互独立的阶段： 需求收集 设计 编码 测试 唯一能够指望的只有变化。所有种类的敏捷流程都有一个共同点：拥抱变化，视变化为成长的良机，而非障碍。 简单地看，敏捷开发和瀑布开发之间差别在于：瀑布开发必须先完成当前步骤之后才能头也不回地迈向下一步骤。这不是敏捷团队的方式。敏捷团队会一点点需求收集，一点点设计、编码、测试，最后交付一点点价值给客户。接着团队再重复此过程……周而复始，工作推进过程中不断改善、调整流程，一直到项目完成为止。 敏捷迭代（在Scrum中称为“Sprint”）可不是微型瀑布，敏捷流程可是真的没有什么步骤之说。敏捷开发是一个整体流程，即测试、设计、编码、和需求收集是完全整合彼此依赖的流程。 敏捷提倡的良好实践： 边做边测试 及早且频繁地交付产品 文档边做边写 构建跨职能团队 敏捷方式的核心思想在于迅速交付商业价值，体现维可工作的软件，还要以定期增量的形式持续地交付价值。 敏捷宣言 个体和互动 高于 流程和工具 工作的软件 高于 详尽的文档 客户合作 高于 合同谈判 响应变化 高于 遵循计划 遵循尽早且频繁交付软件的敏捷框架可以带来高效率，因而保障了客户可以实现所投入时间和金钱的最大化价值回报，进一步也排除了对前期合同洽谈的需要。 敏捷原则 我们最重要的目标，是通过持续不断地及早交付有价值的软件使客户满意。 欣然面对需求变化，即使在开发后期也一样。为了客户的竞争优势，敏捷过程掌控变化。 经常地交付可工作的软件，相隔几个星期或一两个月，倾向于采取较短的周期。 业务人员和开发人员必须相互合作，项目中的每一天都不例外。 激发个体的斗志，以他们为核心搭建项目。提供所需的环境和支援，辅以信任，从而达成目标。 不论团队内外，传递信息效果最好效率也最高的方式是面对面的交谈。 可工作的软件是进度首要度量标准。 敏捷过程倡导可持续开发。责任人、开发人员和用户要能够共同其步调稳定延续。 坚持不懈地追求技术卓越和良好设计，敏捷能力由此增强。 以简洁为本，它是极力减少不必要工作量的艺术。 最好的架构、需求和设计出自组织团队。 团队定期地反思如何能提高成效，并依次调整自身的举止表现。 早交付，频交付。 第二部分：ScrumScrum是一个基于团队进行复杂系统和产品开发的框架。 选择最佳方法以达成目标是团队的责任。 任何时候都可以决定是否要交货，或者再继续做一个sprint以便完成更多功能。 Scrum角色Scrum只承认三个互不相同的角色：产品负责人、Scrum master和团队成员。 角色：产品负责人产品负责人的责任在于，帮公司得到最高投资回报。做法是指引团队做最有价值的作业，并远离不那么有价值的作业。也就是说，产品负责人控制着团队列表上那些条目的优先级顺序。 在Scrum中，产品负责人是唯一有权要求团队做事以及改变列表条目优先级的人。这必然也就意味着，产品负责人需要和干系人密切地合作，判别需要何时构建何物，从而交付最多业务价值。 产品负责人是产品愿景的监护人。愿景包括，产品为谁而建、他们为何需要、如何使用。 产品负责人角色概要： 持有产品愿景 代表业务 代表客户 拥有产品列表 划定故事优先级 设立故事的接收标准 有空回答团队成员们的问题 角色：Scrum MasterScrum Master担当教练角色，引领团队达到更高级的凝聚力、自组织和表现。团队的交付物是产品，而Scrum Master的交付物就是自组织团队。 Scrum Master角色概要： Scrum专家和谏言者 教练 阻碍推土机 引导者 角色：团队成员Scrum团队是高度协作的，也是自组织的。 做具体实现工作的团队成员们，同样也需要负责估计实现特性需要的工作量。产品负责人可以决定故事的顺序，但完成特性或任务需要多少时间是开发人员说了算。 作为Scrum团队成员，要完成的不是你的工作，而是这份工作。 团队成员角色概要： 负责交付用户故事 做所有的开发工作 自组织地交付用户故事 支配估算流程 支配“如何干活”的决策 避免“与我无关” Sprint周期Sprint规划会议Sprint规划会议标志着Sprint的开始。通常来说这个会议分为两个部分。第一部分的目标是，团队要选择一组交付物作为当前Sprint的承诺。会议的第二部分，团队要罗列出交付用户故事所需完成的所有任务。 第一部分：要做什么？Sprint规划会议第一部分的目标在于，团队要找出他们有信心在Sprint结束时交付的一组“已承诺”故事。产品负责人引导这一部分的会议。产品负责人按照优先级顺序，逐个地介绍他希望团队在当前Sprint完成的那些故事。 团队成员们要和产品负责人探讨所有故事，审查其验收标准，确保大家对预期结果有一致的理解。团队成员协商解决依赖性问题，一般还会讨论实现故事要做哪些事情。接着团队成员就要决定他们是否承诺这个故事。 团队的速率（velocity）指的是每个Sprint团队所完成故事点数的平均值，它是一个有效的工具，可以帮助团队选择承诺适量的工作。 第二部分：要怎么做？会议的第二部分，团队卷起袖管就开工，把选定的故事分解成任务。要记得故事是交付物，他们是干系人、用户和客户想要的东西。团队成员需要完成这些任务才能交付故事。 Scrum日会Scrum日会有时候也被称为站立会议： 每天：大多数团队选择在一天工作刚开始的时候开这个会。 小：只有开发团队的成员们可以参加。 简要：这个会不是用来解决大问题的，而是用于保持交流渠道的畅通。站立的意义在于阻止各种离题跑调的情况发生，避免开会变遭罪。 直截了当：参会者轮流快速地分享： 在上一次Scrum日会之后，我已经完成的内容 到下一次Scrum日会之前，我期望完成的内容 导致我慢下来的障碍 故事时间也被称为“列表修整”会议 Sprint评审也被称为Sprint演示。即便是这个Sprint什么也没有完成，也要坚持召开Sprint评审会议，得让干系人了解情况，因为Scrum靠透明度而活。 回顾Scrum就是为了帮助团队持续地进行检验和适应而设计，能够带动绩效和幸福感不断提升。每个迭代都要召开回顾会议，在迭代最后才举行，是专门留给团队的时间，专注于讨论他们当前Sprint的心得体会，并用于继续改进。 检验和适应归根结底，我们以短周期形式做开发工作的原因在于，学习。经验是最好的老师，而Scrum短周期的设计家就是要为我们提供多方位接收反馈的机会，包括客户、团队和试吃昂，并从中学习。 检验和适应也被称为“持续改进”。 Scrum工件Scrum工件是可用于实现进度可视化的工具。 列表 燃尽图 任务板 完成之定义 产品列表产品列表是产品预期交付物的累积清单。这包括了特性、缺陷修复、文档变更和任何值得创建的东西。列表上的所有内容都能以某种弄方式帮助用户。 产品列表不断地改变。 列表的优点在于，绝不会浪费时间给那些可能永不见天日的特征写详细规格书。列表上的故事都是按照优先级排序的，可以说是精确排序。 产品负责人拥有列表，只有产品负责人可以增加、减少列表中的条目，或是进行优先级排序，尽管他也必须借助于和业务干系人、客户和团队成员的紧密配合才能做到。 Sprint列表Sprint列表是团队当前Sprint的任务清单。它只存活一个Sprint的时间。里面包括所有已承诺的故事以及相关联的任务，以及此外的附加工作。 Sprint列表在Sprint规划会议中产生，一旦Sprint规划会议结束，，产品负责人就不可以再修改Sprint列表的故事清单。这是Scrum中业务方和开发团队之间的基本协议，每次Sprint开始前，业务方都可以改变方向，然而Sprint开始后，则允许团队只专注于他们所承诺的故事。改变这个已承诺故事清单有一个方式，就是由干这个活的团队成员提出变更请求。 信息辐射器任务板贴满了便事贴，分为待办、进展中、已完成三种状态。 重要的是这些东西要做得大，而且要放在所有人都能看见的地方。要努力在墙上留住尽可能多的信息。 燃图燃尽图描述了剩余工作随时间变化的轨迹。纵坐标绘制剩余量，横坐标是时间。 发布燃尽图：产品负责人以此为工具追踪剩余工作随时间变化的过程。图表中有不同斜率的下降趋势线，反映了单位时间段内所完成点数的变化。 Sprint燃尽图：显示当前Sprint剩余工作量的变化。Sprint燃尽图的目的在于，能够让团队看清楚情况，知道自己能够交付迭代中已经承诺的全部。 燃耗图：团队的速率就是团队每个Sprint所完成的工作单位数量（例如故事点）。燃耗图绘制出已完成故事点随时间的变化的情况，是团队速率的可视化指示器。 任务板：最简单的任务板由待办（todo）、办理(doing)和已办（done）三部分组成。 完成之定义完成之定义和验收标准是有区别的。验收标准属于产品负责人或客户的领域，明确定义了被视为“可接受”产品必须满足的条件并记录在案。完成之定义归开发团队所有，关注的不是产品面向用户的功能，而是产品要可交付必须要做完的那些任务。 用户故事用户故事是产品列表的基础构件。用户故事通常都是手写在索引卡上的。123作为&lt;某类用户&gt;我想&lt;做某事&gt;从而&lt;创造出某些价值&gt; 基于上述版本，又演化出了聚集目标的用户故事模板123为了&lt;达成某类目标&gt;作为&lt;某类用户&gt;我想&lt;做某事&gt; 和聚焦价值的用户故事模板123为了&lt;创造某价值&gt;作为&lt;某类用户&gt;我想&lt;做某事&gt; 用户故事是交谈的敲门砖。用户故事不是完整的需求或说明书，它们是占位符。它们的信息量足以提醒团队有东西要完成，但我们刻意地不过多探讨细节……直到必需之时。 用户故事、交谈和接收标准结合起来组成了完整的需求规格说明书。用户故事让我们可以快速且完全地捕获想法。在交谈中，我们可以阐述所需完成的细节，并达成一致且可实现的理解。最终，我们用明确的、可测试的接收标准来记录共识。 用户故事大小估值工作生成估值的真正目标是要提供进度的可预测性度量。、 第三部分：辅助性实践Scrum是轻量级框架，它不告诉你如何规划发布，或开发产品原型，或编写及测试代码。 发布规划发布规划是为产品发布挑选故事（特性、功能增强、缺陷修复等）的流程，以及应该如何进行发布。 固定范围 固定日期 固定日期且固定范围 速度、成本和范围，这是项目管理周期中著名的“铁三角”。在软件项目中，速度指的是构建和发布系统所需的时间，成本通常受项目上人头数的影响，范围涉及所包含的故事数量。三者组成一个平衡方程式，改变其中任何一个，必然都会导致另外一个或两个相应地产生变化。 用户角色人物用户角色人物是整理妥当的简要档案，用作参考时方便手持。研究自然环境中真实用户的习惯、态度和行为，虚构人物往往是它们的混合体。 绘制故事地图绘制故事地图是另一种组织用户故事的方式，能够提供比传统产品列表更丰富的上下文，也有助于进行发布规划。 尽管故事地图并不完全是产品列表的替代品，但对照比较一下还是很有好处的。本质上来看，产品列表是唯一的，用户故事按优先级从最高到最低的顺序排列。故事地图是二维的，不仅指示了故事的优先级，也指出了它们彼此的关系和用户更高层次的目标。地图能帮助团队理解故事是如何组装起来形成可发布产品的。 纸上原型纸上原型并不是Scrum的一部分，但它具有敏捷的本质，是充实产品负责人工具箱的上佳之选。 这种低科技、高参与度的方式能移除障碍，让客户也可以参与到设计与开发流程中，还能让学习成效最大化，因为它的反馈循环快得只在转瞬间。 项目微章程微章程就是明确记载了项目关键信息的概要项目。项目伊始只不过是一个想法，而微章程就是有效捕获想法的一种方法。捕捉到之后，想法就会变得更易于分享、讨论和修订，这会极大地帮助你发现并排除掉范围及使命蔓延。 基础微章程包含如下元素： 代号：给项目取个名字 使命宣言：表达项目的目的 愿景宣言：描述项目想要创建的未来 电梯演讲：关注项目要解决的问题，以及公司或客户将由此获得的好处。 商业价值：项目对业务而言在金钱或其他方面的价值是什么 客户和用户：做购买决策的人 度量指标：讲清楚计划如何去度量之前所描述的那些价值 里程碑：重要的时间点 资源：完成如下描述的项目必须或必需用到的资源 风险：可能危害或颠覆项目的事情 权衡：现实地评估团队运作环境中的各种约束。 重构敏捷宣言声明：最好的架构、需求和设计出自自组织团队。 重构就是，在不改变代码外在行为的前提下，对代码做出修改，以改进程序内部结构的一种有纪律的方法。核心是通过一连串的小行为实施转变。每次转变（称之为一次“重构”）都只做一点点，但一系列的转变就能带来显著的结构重组。由于每次重构都不大，也就不容易出错 。每次重构之后系统也依然运作良好，减少了系统在重组过程中严重受损的几率。 把设计工作内置于开发流程之中，这就是敏捷开发迭代复迭代不断前行的方式。 测试驱动开发测试驱动开发的目标在于快速地开发出设计精良、正确性已获证实的代码。 通过先行测试，可以把注意力集中于开发人员需要让代码展现的外部行为。一旦开发人员切实地理解了代码需要做什么，就已经准备周全可以妥善地设计代码实现了。 结对编程结对编程更加快速地产出设计更精良、整洁的代码。它还能消灭知识简仓。]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[session和cookies会话机制详解]]></title>
    <url>%2Fweb%E5%BC%80%E5%8F%91%2Fsession%E5%92%8Ccookies%E4%BC%9A%E8%AF%9D%E6%9C%BA%E5%88%B6%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[web请求与响应基于http，而http是无状态协议。所以我们为了跨越多个请求保留用户的状态，需要利用某种工具帮助我们记录与识别每一次请求及请求的其他信息。 举个栗子，我们在淘宝购物的时候，首先添加了一本《C++ primer》进入购物车，然后我们又继续去搜索《thinking in Java》，继续添加购物车，这时购物车应该有两本书。但如果我们不采取session management会话管理的话，基于http无状态协议，我们在第二次向购物车发出添加请求时，他是无法知道我们第一次添加请求的信息的。 所以，我们就需要session management会话管理！ 会话管理的基本方式会话管理的基本主要有隐藏域，cookies，与URL重写这几种实现方式。用得较多的是后两种。 隐藏域实现会话管理以一个网络注册信息填写为例。 我们在填注册信息的时候，经常遇到填完一个页面的内容之后，还要继续填写下一个页面的内容。但由于http的无状态，那么容易造成的后果，当进入第二页填写的时候，服务器已经不记得我们上一页填写了什么。 怎么利用隐藏域解决这个问题呢？ 顾名思义，其实就是既然服务器不会记得两次请求间的关系，那就由浏览器在每次请求时主动告诉服务器多次请求间的必要信息，但是上一页的信息并不显示在第二页中，而是采用隐藏域的方式。 然而显然这种方式是存在各种问题的。比如关掉网页之后，就会遗失信息，而且查看网页源代码时，容易暴露信息，安全性不高。隐藏域并不是servlet/jsp实际会话管理的机制。 cookie实现会话管理cookie是什么？举个简单的例子，现在当我们浏览网站的时候，经常会自动保存账号与密码，这样下次访问的时候，就可以直接登录了。这种技术的实现就是利用了cookie技术。 cookie是存储key-value对的一个文件，务必记住，它是由服务器将cookie添加到response里一并返回给客户端，然后客户端会自动把response里的cookie接收下来，并且保存到本地，下次发出请求的时候，就会把cookie附加在request里，服务器在根据request里的cookie遍历搜索是否有与之符合的信息 URL重写实现会话管理URL重写就是将需要记录的信息附加在请求的链接背后，以链接参数的形式发送给服务器识别。具体实现的过程会在后文结合cookie详解。 servlet&amp;jsp中的session会话管理机制利用httpsession对象进行会话管理。httpsession对象可以保存跨同一个客户多个请求的会话状态。 换句话说，与一个特定客户的整个会话期间看，httpsession会持久储存。 对于会话期间客户做的所有请求，从中得到的所有信息都可以用httpsession对象保存。 httpsession的工作机制以问卷调查为例，当一个新客户小明填写问卷时，服务器会生成一个httpsession对象，用于保存会话期间小明所选择的信息，服务器会以setAttribute的方式将其保存到httpsession对象中。 每个客户会有一个独立的httpsession对象，保存这个客户所有请求所需要保存的信息。 服务器如何识别所有的请求是否来自同一个客户？ 客户需要一个会话ID（称之为jsessionid）来标识自己。就跟我们每个人的身份证号一样。对于客户的第一个请求，容器会生成一个唯一的会话ID，并通过相应把它返回给用户，客户在以后发回一个请求中发回这个会话ID，容器看到ID之后，就会找到匹配的会话，并把这个会话与请求关联。 实现存储会话ID的就是通过cookie！ cookie存储在客户端，是被服务器放在response里发回客户端的，以后每次request时，都会把cookie加入到request里。 而session是存在服务器的，以属性的形式将会话中的信息存到httpsession对象中。调用时，只要通过httpsession对象调用相应attribute即可。 很多地方总是把session与cookie分开单独讲。但我们通过前面的介绍，不难知道，session实现其会话管理机制时，在如何确定所有请求是否来自同一个客户时，是利用了cookie技术的。所以不应该将cookie与session完全分开讲。 这里产生这个误解的原因。是因为我们对session的会话管理机制不够了解。因为容器在创建session对象时，会帮我们实现所有cookie相关的工作，而我们只需要实现这一句：1HttpSession session = request.getSession(); 记住： 这个方法不只是创建一个会话，而是会完成所有与cookie相关的工作，只是容器都自动帮我们实现了。我们来看看容器在背后默默为我们做了什么： 建立新的httpsession对象 生成唯一的会话ID 建立新的会话对象 把会话ID与cookie关联 在响应中设置cookie cookie所有的工作都在后台进行。看到这里，是不是很爽？容器几乎帮我们实现了所有cookie工作。从请求中得到会话ID只需一行代码：1HttpSession session = request.getSession(); 与上一部分为响应生成会话ID是一致的其中也在后台实现了一些步骤：1234if（请求包含一个会话ID） 找到与该ID匹配的会话else if（没有会话ID或者没有匹配的ID） 创建一个新的会话。 还是那句话：cookie所有工作都在后台自动进行 cookie的更多用处cookie原先设计的初衷就是为了帮助支持会话状态。但是因为cookie的简便性，容器为我们封装了大量操作。现在cookie已经被越来越运用到各个方面。 首先， 我们明确cookie是存在客户端的，实际上就是在客户端与服务端交换的一小段数据（一个name/string对）。 由于session在用户关闭浏览器后，会话结束，就会消失，cookie随之应该也会消失。但servlet的API中提供了一些方法，可以让客户端的cookie存活的时间更久一点。这就是cookie相对于session的一大优势所在。我们目前常用的记住用户名和密码，下次登录就是利用cookie在session消失后，还能存活实现的。 所以，我们可以定制cookie为我们实现各种功能。]]></content>
      <categories>
        <category>web开发</category>
      </categories>
      <tags>
        <tag>web开发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈servlet的生命周期]]></title>
    <url>%2Fweb%E5%BC%80%E5%8F%91%2F%E6%B5%85%E8%B0%88servlet%E7%9A%84%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%2F</url>
    <content type="text"><![CDATA[servlet的生命周期是servlet相关知识中很重要的一部分。 servlet从被加载到销毁经历了多个阶段，其中需要我们十分了解每个阶段的意义作用，才能更好地编写相关的servlet程序。 servlet的生命周期详解下图很好的说明了servlet的各个阶段 首先，容器加载servlet类，从class字节码加载类 随后初始化servlet，使之成为一个对象！servlet的无参构造函数运行，这里不需要我们自己写构造函数，只需要使用编译器的提供的默认构造函数即可（ 相当于new操作，成为一个对象）。值得注意的是，此处的只是一个普通的对象，还不具备成为一个完整servlet的一些信息和功能，所以我们要进行下一步，也就是init()方法。 调用init()方法，此方法只在servlet的一生中调用一次，而且必须在容器调用service()之前完成。这一步主要是让上一步对象加上一些东西，使之不再是一个普通的对象，而是一个特殊的servlet对象。 调用service()方法，servlet的一生主要都在这里度过，处理用户请求，每个请求在一个单独的线程里运行。 调用destroy()方法，容器调用这个方法，从而在servlet被杀死之前有机会清理资源。与init一样，destroy也只能调用一次。 servlet生命周期中三大重要的时刻 init() 何时调用：servlet实例创建后，并在servlet能为客户请求提供service服务前，容器要对servlet调用init。 作用：使你在servlet处理客户请求之前有机会对其进行初始化 是否覆盖：有可能。如果由初始化代码（如得到一个数据库连接），就要调用init()方法 service() 何时调用：第一个客户请求到来时，容器会开始一个新线程，或者从线程池分配一个线程，并调用servlet的service()方法。 作用：这个方法会查看请求，确定http方法 是否覆盖：不太可能 doGet或者doPost() 何时调用：service方法根据请求的http方法调用doGet或者doPost。 作用：要在这里写代码，你的web需要实现的业务逻辑等 是否覆盖：一定要覆盖其中之一。 每个请求在一个单独的线程里运行。容器不关心是谁的请求，每个到来的请求意味着一个新的线程。]]></content>
      <categories>
        <category>web开发</category>
      </categories>
      <tags>
        <tag>web开发</tag>
        <tag>servlet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring框架的作用]]></title>
    <url>%2FSpring%2FSpring%E6%A1%86%E6%9E%B6%E7%9A%84%E4%BD%9C%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Spring框架是时下非常流行的Java web开发框架。Spring 可以做非常多的事情。 但归根结底， 支撑Spring的仅仅是少许的基本理念， 所有的理念都可以追溯到Spring最根本的使命上： 简化Java开发。 Spring的目标是致力于全方位的简化Java开发。 这势必引出更多的解释， Spring是如何简化Java开发的？ 为了降低Java开发的复杂性， Spring采取了以下4种关键策略： 基于POJO的轻量级和最小侵入性编程 通过依赖注入和面向接口实现松耦合 基于切面和惯例进行声明式编程 通过切面和模板减少样板式代码 几乎Spring所做的任何事情都可以追溯到上述的一条或多条策略。 本文通过具体的案例进一步阐述这些理念， 以此来证明Spring是如何完美兑现它的承诺的， 也就是简化Java开发。 激发POJO的潜能如果你从事Java编程有一段时间了， 那么你或许会发现（可能你也实际使用过） 很多框架通过强迫应用继承它们的类或实现它们的接口从而导致应用与框架绑死。 这种侵入式的编程方式在早期版本的Struts以及无数其他的Java规范和框架中都能看到。 Spring竭力避免因自身的API而弄乱你的应用代码。Spring不会强迫你实现Spring规范的接口或继承Spring规范的类， 相反， 在基于Spring构建的应用中，它的类通常没有任何痕迹表明你使用了Spring。 最坏的场景是，一个类或许会使用Spring注解， 但它依旧是POJO。 举例说明，参考下面的HelloWorldBean12345public class HelloWorldBean &#123; public String sayHello() &#123; return "Hello World"; &#125;&#125; 可以看到，这是一个简单普通的Java类——POJO。没有任何地方表明它是一个Spring组件。Spring的非侵入编程模型意味着这个类在Spring应用和非Spring应用中都可以发挥同样的作用。 Spring的非入侵式就是不强制类要实现Spring的任何接口或类，没有任何地方表明它是一个Spring组件。意味着这个类在Spring应用和非Spring应用中都可以发挥同样的作用。 尽管形式看起来很简单，但POJO一样可以具有魔力。Spring赋予POJO魔力的方式之一就是通过DI来装配它们。让我们看看DI是如何帮助应用对象彼此之间保持松散耦合的。 依赖注入任何一个有实际意义的应用（肯定比Hello World示例更复杂）都会由两个或者更多的类组成，这些类相互之间进行协作来完成特定的业务逻辑。按照传统的做法， 每个对象负责管理与自己相互协作的对象（即它所依赖的对象）的引用， 这将会导致高度耦合和难以测试的代码。 举个例子，考虑下程序所展现的Knight类123456789101112public class DamselRescuingKnight implements Knight &#123; private RescueDamselQuest quest; public DamselRescuingKnight() &#123; this.quest = new RescueDamselQuest(); &#125; public void embarkOnQuest() &#123; quest.embark(); &#125;&#125; DamselRescuingKnight只能执行RescueDamselQuest探险任务。 可以看到，DamselRescuingKnight在它的构造函数中自行创建了RescueDamselQuest。 这使得DamselRescuingKnight紧密地和RescueDamselQuest耦合到了一起， 因此极大地限制了这个骑士执行探险的能力。 如果一个少女需要救援，这个骑士能够召之即来。但是如果一条恶龙需要杀掉，那么这个骑士就爱莫能助了。 更糟糕的是，为这个DamselRescuingKnight编写单元测试将出奇地困难。在这样的一个测试中，你必须保证当骑士的embarkOnQuest()方法被调用的时候，探险的embark()方法也要被调用。但是没有一个简单明了的方式能够实现这一点。 很遗憾，DamselRescuingKnight将无法进行测试。 耦合具有两面性（two-headed beast） 一方面， 紧密耦合的代码难以测试、 难以复用、难以理解，并且典型地表现出“打地鼠”式的bug特性（修复一个bug， 将会出现一个或者更多新的bug） 另一方面，一定程度的耦合又是必须的——完全没有耦合的代码什么也做不了。 为了完成有实际意义的功能，不同的类必须以适当的方式进行交互。总而言之， 耦合是必须的，但应当被小心谨慎地管理。 通过DI， 对象的依赖关系将由系统中负责协调各对象的第三方组件在创建对象的时候进行设定。对象无需自行创建或管理它们的依赖关系， 如下图所示， 依赖关系将被自动注入到需要它们的对象当中去。 依赖注入会将所依赖的关系自动交给目标对象， 而不是让对象自己去获取依赖。 为了展示这一点，让我们看一看以下的BraveKnight，这个骑士不仅勇敢，而且能挑战任何形式的探险。123456789101112public class BraveKnight implements Knight &#123; private Quest quest; public BraveKnight(Quest quest) &#123; this.quest = quest; &#125; public void embarkOnQuest() &#123; quest.embark(); &#125;&#125; 我们可以看到，不同于之前的DamselRescuingKnight，BraveKnight没有自行创建探险任务，而是在构造的时候把探险任务作为构造器参数传入。这是依赖注入的方式之一，即构造器注入（constructor injection）。 更重要的是，传入的探险类型是Quest， 也就是所有探险任务都必须实现的一个接口。所以，BraveKnight能够响应RescueDamselQuest、SlayDragonQuest、 MakeRound TableRounderQuest等任意的Quest实现。 这里的要点是BraveKnight没有与任何特定的Quest实现发生耦合。对它来说， 被要求挑战的探险任务只要实现了Quest接口，那么具体是哪种类型的探险就无关紧要了。这就是DI所带来的最大收益——松耦合。 如果一个对象只通过接口（而不是具体实现或初始化过程）来表明依赖关系， 那么这种依赖就能够在对象本身毫不知情的情况下，用不同的具体实现进行替换。 现在BraveKnight类可以接受你传递给它的任意一种Quest的实现，但该怎样把特定的Query实现传给它呢？ 假设， 希望BraveKnight所要进行探险任务是杀死一只怪龙，那么以下程序中的SlayDragonQuest也许是挺合适的。1234567891011121314import java.io.PrintStream;public class SlayDragonQuest implements Quest &#123; private PrintStream stream; public SlayDragonQuest(PrintStream stream) &#123; this.stream = stream; &#125; public void embark() &#123; stream.println("Embarking on quest to slay the dragon!"); &#125;&#125; SlayDragonQuest是要注入到BraveKnight中的Quest实现 我们可以看到，SlayDragonQuest实现了Quest接口，这样它就适合注入到BraveKnight中去了。与其他的Java入门样例有所不同，SlayDragonQuest没有使用System.out.println()，而是在构造方法中请求一个更为通用的PrintStream。 这里最大的问题在于: 我们该如何将SlayDragonQuest交给BraveKnight呢？ 又如何将PrintStream交给SlayDragonQuest呢？ 创建应用组件之间协作的行为通常称为装配（wiring）。Spring有多种装配bean的方式，采用XML是很常见的一种装配方式。 以下程序展现了一个简单的Spring配置文件：knights.xml，该配置文件将BraveKnight、SlayDragonQuesth和PrintStream装配到了一起。1234567891011121314&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd"&gt; &lt;bean id="knight" class="sia.knights.BraveKnight"&gt; &lt;constructor-arg ref="quest" /&gt; &lt;/bean&gt; &lt;bean id="quest" class="sia.knights.SlayDragonQuest"&gt; &lt;constructor-arg value="#&#123;T(System).out&#125;" /&gt; &lt;/bean&gt;&lt;/beans&gt; 装配的作用就是创建类的实例，同时将类的构造函数或者setter函数参数设置好，这是为了配置对象和对象之间的关系。 在这里， BraveKnight和SlayDragonQuest被声明为Spring中的bean。就BraveKnight bean来讲，它在构造时传入了对SlayDragonQuest bean的引用， 将其作为构造器参数。 同时， SlayDragonQuest bean的声明使用了Spring表达式语言（Spring Expression Language），将System.out（这是一个PrintStream）传入到了SlayDragonQuest的构造器中。 Spring还支持使用Java来描述配置。123456789101112131415161718192021import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import sia.knights.BraveKnight;import sia.knights.Knight;import sia.knights.Quest;import sia.knights.SlayDragonQuest;@Configurationpublic class KnightConfig &#123; @Bean public Knight knight() &#123; return new BraveKnight(quest()); &#125; @Bean public Quest quest() &#123; return new SlayDragonQuest(System.out); &#125;&#125; 尽管BraveKnight依赖于Quest，但是它并不知道传递给它的是什么类型的Quest， 也不知道这个Quest来自哪里。与之类似， SlayDragonQuest依赖于PrintStream，但是在编码时它并不需要知道这个PrintStream是什么样子的。只有Spring通过它的配置，能够了解这些组成部分是如何装配起来的。这样的话， 就可以在不改变所依赖的类的情况下， 修改依赖关系。 观察它如何工作Spring通过应用上下文（Application Context）装载bean的定义并把它们组装起来。 Spring应用上下文全权负责对象的创建和组装。Spring自带了多种应用上下文的实现，它们之间主要的区别仅仅在于如何加载配置。 因为knights.xml中的bean是使用XML文件进行配置的，所以选择ClassPathXmlApplicationContext作为应用上下文相对是比较合适的。该类加载位于应用程序类路径下的一个或多个XML配置文件。 以下程序中的main()方法调用ClassPathXmlApplicationContext加载knights.xml，并获得Knight对象的引用。1234567891011121314import org.springframework.context.support. ClassPathXmlApplicationContext;public class KnightMain &#123; public static void main(String[] args) throws Exception &#123; ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext( "META-INF/spring/knight.xml"); Knight knight = context.getBean(Knight.class); knight.embarkOnQuest(); context.close(); &#125;&#125; 这里的main()方法基于knights.xml文件创建了Spring应用上下文。随后它调用该应用上下文获取一个ID为knight的bean。得到Knight对象的引用后，只需简单调用embarkOnQuest()方法就可以执行所赋予的探险任务了。注意这个类完全不知道我们的英雄骑士接受哪种探险任务，而且完全没有意识到这是由BraveKnight来执行的。只有knights.xml文件知道哪个骑士执行哪种探险任务。 应用切面DI能够让相互协作的软件组件保持松散耦合，而面向切面编程（aspect-oriented programming，AOP）允许你把遍布应用各处的功能分离出来形成可重用的组件。 面向切面编程往往被定义为促使软件系统实现关注点的分离一项技术。系统由许多不同的组件组成，每一个组件各负责一块特定功能。除了实现自身核心的功能之外，这些组件还经常承担着额外的职责。诸如日志、事务管理和安全这样的系统服务经常融入到自身具有核心业务逻辑的组件中去，这些系统服务通常被称为横切关注点，因为它们会跨越系统的多个组件。 如果将这些关注点分散到多个组件中去，你的代码将会带来双重的复杂性。 实现系统关注点功能的代码将会重复出现在多个组件中。这意味着如果你要改变这些关注点的逻辑，必须修改各个模块中的相关实现。 即使你把这些关注点抽象为一个独立的模块，其他模块只是调用它的方法， 但方法的调用还是会重复出现在各个模块中。 组件会因为那些与自身核心业务无关的代码而变得混乱。一个向地址簿增加地址条目的方法应该只关注如何添加地址，而不应该关注它是不是安全的或者是否需要支持事务 在整个系统内，关注点（例如日志和安全）的调用经常散布到各个模块中， 而这些关注点并不是模块的核心业务 AOP能够使这些服务模块化，并以声明的方式将它们应用到它们需要影响的组件中去。所造成的结果就是这些组件会具有更高的内聚性并且会更加关注自身的业务，完全不需要了解涉及系统服务所带来复杂性。总之，AOP能够确保POJO的简单性。 我们可以把切面想象为覆盖在很多组件之上的一个外壳。应用是由那些实现各自业务功能的模块组成的。借助AOP，可以使用各种功能层去包裹核心业务层。这些层以声明的方式灵活地应用到系统中，你的核心应用甚至根本不知道它们的存在。这是一个非常强大的理念，可以将安全、事务和日志关注点与核心业务逻辑相分离。 利用AOP，系统范围内的关注点覆盖在它们所影响组件之上 为了示范在Spring中如何应用切面，让我们重新回到骑士的例子，并为它添加一个切面。 每一个人都熟知骑士所做的任何事情，这是因为吟游诗人用诗歌记载了骑士的事迹并将其进行传唱。假设我们需要使用吟游诗人这个服务类来记载骑士的所有事迹。如下程序展示了我们会使用的Minstrel类。12345678910111213141516171819import java.io.PrintStream;public class Minstrel &#123; private PrintStream stream; public Minstrel(PrintStream stream) &#123; this.stream = stream; &#125; public void singBeforeQuest() &#123; stream.println("Fa la la, the knight is so brave!"); &#125; public void singAfterQuest() &#123; stream.println("Tee hee hee, the brave knight " + "did embark on a quest!"); &#125;&#125; Minstrel是只有两个方法的简单类。在骑士执行每一个探险任务之前，singBeforeQuest()方法会被调用；在骑士完成探险任务之后，singAfterQuest()方法会被调用。在这两种情况下，Minstrel都会通过一个PrintStream类来歌颂骑士的事迹，这个类是通过构造器注入进来的。 把Minstrel加入你的代码中并使其运行起来，这对你来说是小事一桩。我们适当做一下调整从而让BraveKnight可以使用Minstrel。如下是将BraveKnight和Minstrel组合起来的第一次尝试。12345678910111213141516public class BraveKnight implements Knight &#123; private Quest quest; private Minstrel minstrel; public BraveKnight(Quest quest, Minstrel minstrel) &#123; this.quest = quest; this.minstrel = minstrel; &#125; public void embarkOnQuest() &#123; minstrel.singBeforeQuest(); quest.embark(); minstrel.singAfterQuest(); &#125;&#125; 这应该可以达到预期效果。现在，你所需要做的就是回到Spring配置中，声明Minstrel bean并将其注入到BraveKnight的构造器之中。 但是， 请稍等…… 我们似乎感觉有些东西不太对。管理他的吟游诗人真的是骑士职责范围内的工作吗？在我看来，吟游诗人应该做他份内的事，根本不需要骑士命令他这么做。毕竟，用诗歌记载骑士的探险事迹，这是吟游诗人的职责。为什么骑士还需要提醒吟游诗人去做他份内的事情呢？此外，因为骑士需要知道吟游诗人，所以就必须把吟游诗人注入到BarveKnight类中。这不仅使BraveKnight的代码复杂化了，而且还让我疑惑是否还需要一个不需要吟游诗人的骑士呢？如果Minstrel为null会发生什么呢？我是否应该引入一个空值校验逻辑来覆盖该场景？ 简单的BraveKnight类开始变得复杂，如果你还需要应对没有吟游诗人时的场景，那代码会变得更复杂。但利用AOP，你可以声明吟游诗人必须歌颂骑士的探险事迹，而骑士本身并不用直接访问Minstrel的方法。 要将Minstrel抽象为一个切面， 你所需要做的事情就是在一个Spring配置文件中声明它。 下面是更新后的knights.xml文件， Minstrel被声明为一个切面。12345678910111213141516171819202122232425262728293031323334&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:aop="http://www.springframework.org/schema/aop" xsi:schemaLocation="http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd"&gt; &lt;bean id="knight" class="sia.knights.BraveKnight"&gt; &lt;constructor-arg ref="quest" /&gt; &lt;/bean&gt; &lt;bean id="quest" class="sia.knights.SlayDragonQuest"&gt; &lt;constructor-arg ref="fakePrintStream" /&gt; &lt;/bean&gt; &lt;bean id="minstrel" class="sia.knights.Minstrel"&gt; &lt;constructor-arg ref="fakePrintStream" /&gt; &lt;/bean&gt; &lt;bean id="fakePrintStream" class="sia.knights.FakePrintStream" /&gt; &lt;aop:config&gt; &lt;aop:aspect ref="minstrel"&gt; &lt;aop:pointcut id="embark" expression="execution(* *.embarkOnQuest(..))"/&gt; &lt;aop:before pointcut-ref="embark" method="singBeforeQuest"/&gt; &lt;aop:after pointcut-ref="embark" method="singAfterQuest"/&gt; &lt;/aop:aspect&gt; &lt;/aop:config&gt;&lt;/beans&gt; 这里使用了Spring的aop配置命名空间把Minstrel bean声明为一个切面。 首先，需要把Minstrel声明为一个bean 然后在元素中引用该bean 为了进一步定义切面，声明（使用）在embarkOnQuest()方法执行前调用Minstrel的singBeforeQuest()方法。这种方式被称为前置通知（before advice） 同时声明（使用）在embarkOnQuest()方法执行后调用singAfterQuest()方法。这种方式被称为后置通知（after advice） 首先，Minstrel仍然是一个POJO， 没有任何代码表明它要被作为一个切面使用。当我们按照上面那样进行配置后，在Spring的上下文中，Minstrel实际上已经变成一个切面了。 其次，也是最重要的，Minstrel可以被应用到BraveKnight中，而BraveKnight不需要显式地调用它。实际上，BraveKnight完全不知道Minstrel的存在。 必须还要指出的是，尽管我们使用Spring魔法把Minstrel转变为一个切面，但首先要把它声明为一个Spring bean。能够为其他Spring bean做到的事情都可以同样应用到Spring切面中， 例如为它们注入依赖。 使用模板消除样板式代码你是否写过这样的代码，当编写的时候总会感觉以前曾经这么写过？我的朋友，这不是似曾相识。这是样板式的代码（boilerplate code）。通常为了实现通用的和简单的任务，你不得不一遍遍地重复编写这样的代码。 遗憾的是，它们中的很多是因为使用Java API而导致的样板式代码。样板式代码的一个常见范例是使用JDBC访问数据库查询数据。举个例子，如果你曾经用过JDBC，那么你或许会写出类似下面的代码。 正如你所看到的，这段JDBC代码查询数据库获得员工姓名和薪水。我打赌你很难把上面的代码逐行看完，这是因为少量查询员工的代码淹没在一堆JDBC的样板式代码中。首先你需要创建一个数据库连接， 然后再创建一个语句对象， 最后你才能进行查询。 为了平息JDBC可能会出现的怒火，你必须捕捉SQLException，这是一个检查型异常，即使它抛出后你也做不了太多事情。 最后，毕竟该说的也说了，该做的也做了，你不得不清理战场，关闭数据库连接、 语句和结果集。同样为了平息JDBC可能会出现的怒火，你依然要捕SQLException。 上面的代码和你实现其他JDBC操作时所写的代码几乎是相同的。只有少量的代码与查询员工逻辑有关系，其他的代码都是JDBC的样板代码。 JDBC不是产生样板式代码的唯一场景。在许多编程场景中往往都会导致类似的样板式代码，JMS、JNDI和使用REST服务通常也涉及大量的重复代码。 Spring旨在通过模板封装来消除样板式代码。Spring的JdbcTemplate使得执行数据库操作时，避免传统的JDBC样板代码成为了可能。 举个例子，使用Spring的JdbcTemplate（利用了Java5特性的JdbcTemplate实现）重写的getEmployeeById()方法仅仅关注于获取员工数据的核心逻辑，而不需要迎合JDBC API的需求。 下图展示了修订后的getEmployeeById()方法。 正如你所看到的，新版本的getEmployeeById()简单多了，而且仅仅关注于从数据库中查询员工。模板的queryForObject()方法需要一个SQL查询语句，一个RowMapper对象（把数据映射为一个域对象），零个或多个查询参数。GetEmployeeById()方法再也看不到以前的JDBC样板式代码了，它们全部被封装到了模板中。]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java根类Object的方法说明]]></title>
    <url>%2FJava%2F%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%2FJava%E6%A0%B9%E7%B1%BBObject%E7%9A%84%E6%96%B9%E6%B3%95%E8%AF%B4%E6%98%8E%2F</url>
    <content type="text"><![CDATA[这个问题最早源自16年暑假在阿里云转正面试时被问及的，当时磕磕绊绊说了几个。今天系统的学习一下。 Java中的Object类是所有类的父类，它提供了总共11个方法，今天我们一个个方法进行分析，看这些方法到底有什么作用。 public final native Class&lt;?&gt; getClass() public native int hashCode() public boolean equals(Object obj) protected native Object clone() throws CloneNotSupportedException public String toString() public final native void notify() public final native void notifyAll() public final native void wait(long timeout) throws InterruptedException public final void wait(long timeout, int nanos) throws InterruptedException public final void wait() throws InterruptedException protected void finalize() throws Throwable { } getClass方法getClass方法是一个final方法，不允许子类重写，并且也是一个native方法。 返回当前运行时对象的Class对象，注意这里是运行时，比如以下代码中n是一个Number类型的实例，但是Java中数值默认是Integer类型，所以getClass方法返回的是java.lang.Integer：1234"str".getClass() // class java.lang.String"str".getClass == String.class // trueNumber n = 0;Class&lt;? extends Number&gt; c = n.getClass(); // class java.lang.Integer hashCode方法hashCode方法也是一个native方法。 该方法返回对象的哈希码，主要使用在哈希表中，比如JDK中的HashMap。 哈希码的通用约定如下： 在Java程序执行过程中，在一个对象没有被改变的前提下，无论这个对象被调用多少次，hashCode方法都会返回相同的整数值。对象的哈希码没有必要在不同的程序中保持相同的值。 如果2个对象使用equals方法进行比较并且相同的话，那么这2个对象的hashCode方法的值也必须相等。 如果根据equals方法，得到两个对象不相等，那么这2个对象的hashCode值不需要必须不相同。但是，不相等的对象的hashCode值不同的话可以提高哈希表的性能。 通常情况下，不同的对象产生的哈希码是不同的。默认情况下，对象的哈希码是通过将该对象的内部地址转换成一个整数来实现的。 String的hashCode方法实现如下， 计算方法是s[0] * 31^(n-1) + s[1] * 31^(n-2) + … + s[n-1]，其中s[0]表示字符串的第一个字符，n表示字符串长度：123456789101112public int hashCode() &#123; int h = hash; if (h == 0 &amp;&amp; value.length &gt; 0) &#123; char val[] = value; for (int i = 0; i &lt; value.length; i++) &#123; h = 31 * h + val[i]; &#125; hash = h; &#125; return h;&#125; hashCode在哈希表HashMap中的应用：12345678910111213141516171819202122// Student类，只重写了hashCode方法public static class Student &#123; private String name; private int age; public Student(String name, int age) &#123; this.name = name; this.age = age; &#125; @Override public int hashCode() &#123; return name.hashCode(); &#125;&#125;Map&lt;Student, String&gt; map = new HashMap&lt;Student, String&gt;();Student stu1 = new Student("fo", 11);Student stu2 = new Student("fo", 22);map.put(stu1, "fo");map.put(stu2, "fo"); 上面这段代码中，map中有2个元素stu1和stu2。但是这2个元素是在哈希表中的同一个数组项中的位置，也就是在同一串链表中。 但是为什么stu1和stu2的hashCode相同，但是两条元素都插到map里了，这是因为map判断重复数据的条件是两个对象的哈希码相同并且(两个对象是同一个对象或者两个对象相等[equals为true])。 所以再给Student重写equals方法，并且只比较name的话，这样map就只有1个元素了。1234567@Overridepublic boolean equals(Object o) &#123; if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; Student student = (Student) o; return this.name.equals(student.name);&#125; 这个例子直接说明了hashCode中通用约定的第三点： 第三点：如果根据equals方法，得到两个对象不相等，那么这2个对象的hashCode值不需要必须不相同。但是，不相等的对象的hashCode值不同的话可以提高哈希表的性能。 上面例子一开始没有重写equals方法，导致两个对象不相等，但是这两个对象的hashCode值一样，所以导致这两个对象在同一串链表中，影响性能。 当然，还有第三种情况，那就是equals方法相等，但是hashCode的值不相等。这种情况也就是违反了通用约定的第二点： 第二点：如果2个对象使用equals方法进行比较并且相同的话，那么这2个对象的hashCode方法的值也必须相等。 违反这一点产生的后果就是如果一个stu1实例是Student(“fo”, 11)，stu2实例是Student(“fo”, 11)，那么这2个实例是相等的，但是他们的hashCode不一样，这样是导致哈希表中都会存入stu1实例和stu2实例，但是实际情况下，stu1和stu2是重复数据，只允许存在一条数据在哈希表中。所以这一点是非常重点的。 再强调一下：如果2个对象的equals方法相等，那么他们的hashCode值也必须相等，反之，如果2个对象hashCode值相等，但是equals不相等，这样会影响性能，所以还是建议2个方法都一起重写。 equals方法比较两个对象是否相等。Object类的默认实现，即比较2个对象的内存地址是否相等：123public boolean equals(Object obj) &#123; return (this == obj);&#125; equals方法在非空对象引用上的特性： reflexive，自反性。任何非空引用值x，对于x.equals(x)必须返回true symmetric，对称性。任何非空引用值x和y，如果x.equals(y)为true，那么y.equals(x)也必须为true transitive，传递性。任何非空引用值x、y和z，如果x.equals(y)为true并且y.equals(z)为true，那么x.equals(z)也必定为true consistent，一致性。任何非空引用值x和y，多次调用x.equals(y)始终返回true或始终返回false，前提是对象上equals比较中所用的信息没有被修改。对于任何非空引用值x，x.equals(null) 都应返回false Object类的equals方法对于任何非空引用值x和y，当x和y引用同一个对象时，此方法才返回true。这个也就是我们常说的地址相等。 注意点：如果重写了equals方法，通常有必要重写hashCode方法，这点已经在hashCode方法中说明了。 clone方法创建并返回当前对象的一份拷贝。 一般情况下，对于任何对象 x，表达式 x.clone() != x 为true，x.clone().getClass() == x.getClass() 也为true。 Object类的clone方法是一个protected的native方法。 由于Object本身没有实现Cloneable接口，所以不重写clone方法并且进行调用的话会发生CloneNotSupportedException异常。 toString方法Object对象的默认实现，即输出类的名字@实例的哈希码的16进制：123public String toString() &#123; return getClass().getName() + "@" + Integer.toHexString(hashCode());&#125; toString方法的结果应该是一个简明但易于读懂的字符串。建议Object所有的子类都重写这个方法。 notify方法notify方法是一个native方法，并且也是final的，不允许子类重写。 唤醒一个在此对象监视器上等待的线程(监视器相当于就是锁的概念)。如果所有的线程都在此对象上等待，那么只会选择一个线程。选择是任意性的，并在对实现做出决定时发生。一个线程在对象监视器上等待可以调用wait方法。 直到当前线程放弃对象上的锁之后，被唤醒的线程才可以继续处理。被唤醒的线程将以常规方式与在该对象上主动同步的其他所有线程进行竞争。例如，唤醒的线程在作为锁定此对象的下一个线程方面没有可靠的特权或劣势。 notify方法只能被作为此对象监视器的所有者的线程来调用。一个线程要想成为对象监视器的所有者，可以使用以下3种方法： 执行对象的同步实例方法 使用synchronized内置锁 对于Class类型的对象，执行同步静态方法 一次只能有一个线程拥有对象的监视器。 如果当前线程不是此对象监视器的所有者的话会抛出IllegalMonitorStateException异常 注意点： 因为notify只能在拥有对象监视器的所有者线程中调用，否则会抛出IllegalMonitorStateException异常 notifyAll方法跟notify一样，唯一的区别就是会唤醒在此对象监视器上等待的所有线程，而不是一个线程。 同样，如果当前线程不是对象监视器的所有者，那么调用notifyAll同样会发生IllegalMonitorStateException异常。 以下这段代码直接调用notify或者notifyAll方法会发生IllegalMonitorStateException异常，这是因为调用这两个方法需要当前线程是对象监视器的所有者：123Factory factory = new Factory();factory.notify();factory.notifyAll(); wait(long timeout) throws InterruptedException方法wait(long timeout)方法同样是一个native方法，并且也是final的，不允许子类重写。 wait方法会让当前线程等待直到另外一个线程调用对象的notify或notifyAll方法，或者超过参数设置的timeout超时时间。 跟notify和notifyAll方法一样，当前线程必须是此对象的监视器所有者，否则还是会发生IllegalMonitorStateException异常。 wait方法会让当前线程(我们先叫做线程T)将其自身放置在对象的等待集中，并且放弃该对象上的所有同步要求。出于线程调度目的，线程T是不可用并处于休眠状态，直到发生以下四件事中的任意一件： 其他某个线程调用此对象的notify方法，并且线程T碰巧被任选为被唤醒的线程 其他某个线程调用此对象的notifyAll方法 其他某个线程调用Thread.interrupt方法中断线程T 时间到了参数设置的超时时间。如果timeout参数为0，则不会超时，会一直进行等待 所以可以理解wait方法相当于放弃了当前线程对对象监视器的所有者(也就是说释放了对象的锁) 之后，线程T会被等待集中被移除，并且重新进行线程调度。然后，该线程以常规方式与其他线程竞争，以获得在该对象上同步的权利；一旦获得对该对象的控制权，该对象上的所有其同步声明都将被恢复到以前的状态，这就是调用wait方法时的情况。然后，线程T从wait方法的调用中返回。所以，从wait方法返回时，该对象和线程T的同步状态与调用wait方法时的情况完全相同。 在没有被通知、中断或超时的情况下，线程还可以唤醒一个所谓的虚假唤醒 (spurious wakeup)。虽然这种情况在实践中很少发生，但是应用程序必须通过以下方式防止其发生，即对应该导致该线程被提醒的条件进行测试，如果不满足该条件，则继续等待。换句话说，等待应总是发生在循环中，如下面的示例：12345synchronized (obj) &#123; while (&lt;condition does not hold&gt;) obj.wait(timeout); ... // Perform action appropriate to condition&#125; 如果当前线程在等待之前或在等待时被任何线程中断，则会抛出InterruptedException异常。在按上述形式恢复此对象的锁定状态时才会抛出此异常。 wait(long timeout, int nanos) throws InterruptedException方法跟wait(long timeout)方法类似，多了一个nanos参数，这个参数表示额外时间（以毫微秒为单位，范围是 0-999999）。 所以超时的时间还需要加上nanos毫秒。 需要注意的是 wait(0, 0)和wait(0)效果是一样的，即一直等待。 wait() throws InterruptedException方法跟之前的2个wait方法一样，只不过该方法一直等待，没有超时时间这个概念。 以下这段代码直接调用wait方法会发生IllegalMonitorStateException异常，这是因为调用wait方法需要当前线程是对象监视器的所有者：12Factory factory = new Factory();factory.wait(); 一般情况下，wait方法和notify方法会一起使用的，wait方法阻塞当前线程，notify方法唤醒当前线程，一个使用wait和notify方法的生产者消费者例子代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081public class WaitNotifyTest &#123; public static void main(String[] args) &#123; Factory factory = new Factory(); new Thread(new Producer(factory, 5)).start(); new Thread(new Producer(factory, 5)).start(); new Thread(new Producer(factory, 20)).start(); new Thread(new Producer(factory, 30)).start(); new Thread(new Consumer(factory, 10)).start(); new Thread(new Consumer(factory, 20)).start(); new Thread(new Consumer(factory, 5)).start(); new Thread(new Consumer(factory, 5)).start(); new Thread(new Consumer(factory, 20)).start(); &#125;&#125;class Factory &#123; public static final Integer MAX_NUM = 50; private int currentNum = 0; public void consume(int num) throws InterruptedException &#123; synchronized (this) &#123; while(currentNum - num &lt; 0) &#123; this.wait(); &#125; currentNum -= num; System.out.println("consume " + num + ", left: " + currentNum); this.notifyAll(); &#125; &#125; public void produce(int num) throws InterruptedException &#123; synchronized (this) &#123; while(currentNum + num &gt; MAX_NUM) &#123; this.wait(); &#125; currentNum += num; System.out.println("produce " + num + ", left: " + currentNum); this.notifyAll(); &#125; &#125;&#125;class Producer implements Runnable &#123; private Factory factory; private int num; public Producer(Factory factory, int num) &#123; this.factory = factory; this.num = num; &#125; @Override public void run() &#123; try &#123; factory.produce(num); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125;class Consumer implements Runnable &#123; private Factory factory; private int num; public Consumer(Factory factory, int num) &#123; this.factory = factory; this.num = num; &#125; @Override public void run() &#123; try &#123; factory.consume(num); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 注意的是Factory类的produce和consume方法都将Factory实例锁住了，锁住之后线程就成为了对象监视器的所有者，然后才能调用wait和notify方法。 输出：123456789produce 5, left: 5produce 20, left: 25produce 5, left: 30consume 10, left: 20produce 30, left: 50consume 20, left: 30consume 5, left: 25consume 5, left: 20consume 20, left: 0 finalize方法finalize方法是一个protected方法，Object类的默认实现是不进行任何操作。 该方法的作用是实例被垃圾回收器回收的时候触发的操作，就好比 “死前的最后一波挣扎”。 直接写个弱引用例子：123456789101112131415161718192021222324252627282930Car car = new Car(9999, "black");WeakReference&lt;Car&gt; carWeakReference = new WeakReference&lt;Car&gt;(car);int i = 0;while(true) &#123; if(carWeakReference.get() != null) &#123; i++; System.out.println("Object is alive for "+i+" loops - "+carWeakReference); &#125; else &#123; System.out.println("Object has been collected."); break; &#125;&#125;class Car &#123; private double price; private String colour; public Car(double price, String colour)&#123; this.price = price; this.colour = colour; &#125; // get set method @Override protected void finalize() throws Throwable &#123; System.out.println("i will be destroyed"); &#125;&#125; 输出：123456789....Object is alive for 26417 loops - java.lang.ref.WeakReference@7c2f1622Object is alive for 26418 loops - java.lang.ref.WeakReference@7c2f1622Object is alive for 26419 loops - java.lang.ref.WeakReference@7c2f1622Object is alive for 26420 loops - java.lang.ref.WeakReference@7c2f1622Object is alive for 26421 loops - java.lang.ref.WeakReference@7c2f1622Object is alive for 26422 loops - java.lang.ref.WeakReference@7c2f1622Object has been collected.i will be destroyed 转自 Java根类Object的方法说明]]></content>
      <categories>
        <category>Java</category>
        <category>源码阅读</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>源码阅读</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HashMap源码阅读]]></title>
    <url>%2FJava%2F%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%2FHashMap%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%2F</url>
    <content type="text"><![CDATA[HashMap是我们平台开发中最经常使用数据结构之一。很多人肯定是使用过很多次HashMap，但是至于HashMap的源码实现，可能很多人就没什么概念了。本文来介绍一下Java中HashMap的源码实现。 本文基于JDK1.8 常量定义DEFAULT_INITIAL_CAPACITY1234/** * The default initial capacity - MUST be a power of two. */static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16 map的默认大小，默认是1 &lt;&lt; 4=16，必须是2的幂 MAXIMUM_CAPACITY123456/** * The maximum capacity, used if a higher value is implicitly specified * by either of the constructors with arguments. * MUST be a power of two &lt;= 1&lt;&lt;30. */static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30; map的最大容量，默认是1 &lt;&lt; 30 DEFAULT_LOAD_FACTOR1234/** * The load factor used when none specified in constructor. */static final float DEFAULT_LOAD_FACTOR = 0.75f; 默认装载因子，默认是0.75f，超过这个值就需要进行扩容 TREEIFY_THRESHOLD123456789/** * The bin count threshold for using a tree rather than list for a * bin. Bins are converted to trees when adding an element to a * bin with at least this many nodes. The value must be greater * than 2 and should be at least 8 to mesh with assumptions in * tree removal about conversion back to plain bins upon * shrinkage. */static final int TREEIFY_THRESHOLD = 8; 由链表转换成树的阈值，一个桶中bin（箱子）的存储方式由链表转换成树的阈值 即当桶中bin的数量超过TREEIFY_THRESHOLD时使用树来代替链表。默认值是8 UNTREEIFY_THRESHOLD123456/** * The bin count threshold for untreeifying a (split) bin during a * resize operation. Should be less than TREEIFY_THRESHOLD, and at * most 6 to mesh with shrinkage detection under removal. */static final int UNTREEIFY_THRESHOLD = 6; 由树转换成链表的阈值 当执行resize操作时，当桶中bin的数量少于UNTREEIFY_THRESHOLD时使用链表来代替树。默认值是6 MIN_TREEIFY_CAPACITY1234567/** * The smallest table capacity for which bins may be treeified. * (Otherwise the table is resized if too many nodes in a bin.) * Should be at least 4 * TREEIFY_THRESHOLD to avoid conflicts * between resizing and treeification thresholds. */static final int MIN_TREEIFY_CAPACITY = 64; 当桶中的bin被树化时最小的hash表容量 如果没有达到这个阈值，即hash表容量小于MIN_TREEIFY_CAPACITY，当桶中bin的数量太多时会执行resize扩容操作 这个MIN_TREEIFY_CAPACITY的值至少是TREEIFY_THRESHOLD的4倍 成员变量table1234567/** * The table, initialized on first use, and resized as * necessary. When allocated, length is always a power of two. * (We also tolerate length zero in some operations to allow * bootstrapping mechanics that are currently not needed.) */transient Node&lt;K,V&gt;[] table; 存放KV数据的数组 第一次使用的时候被初始化，根据需要可以重新resize 分配的长度总是2的幂 entrySet12345/** * Holds cached entrySet(). Note that AbstractMap fields are used * for keySet() and values(). */transient Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet; 当被调用entrySet时被赋值 通过keySet()方法可以得到map key的集合 通过values方法可以得到map value的集合 size1234/** * The number of key-value mappings contained in this map. */transient int size; 存放在map中的KV映射的总数 modCount12345678/** * The number of times this HashMap has been structurally modified * Structural modifications are those that change the number of mappings in * the HashMap or otherwise modify its internal structure (e.g., * rehash). This field is used to make iterators on Collection-views of * the HashMap fail-fast. (See ConcurrentModificationException). */transient int modCount; HashMap被结构性修改的次数 结构性修改是指改变了KV映射数量的操作或者修改了HashMap的内部结构（如 rehash） 这个用于fail-fast threshold12345678910/** * The next size value at which to resize (capacity * load factor). * * @serial */// (The javadoc description is true upon serialization.// Additionally, if the table array has not been allocated, this// field holds the initial array capacity, or zero signifying// DEFAULT_INITIAL_CAPACITY.)int threshold; 当需要resize时的阈值 即当HashMap中KV映射的数量（即size）超过了threshold就会resize threshold=capacity*loadFactor loadFactor123456/** * The load factor for the hash table. * * @serial */final float loadFactor; 装载因子 capacity 成员变量中并没有capacity这个数据 当然capacity可以通过threshold和loadFactor计算得来 内部数据结构node12345678910111213141516171819202122232425262728293031323334353637383940414243/** * Basic hash bin node, used for most entries. (See below for * TreeNode subclass, and in LinkedHashMap for its Entry subclass.) */static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; final K key; V value; Node&lt;K,V&gt; next; Node(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.value = value; this.next = next; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return value; &#125; public final String toString() &#123; return key + "=" + value; &#125; public final int hashCode() &#123; return Objects.hashCode(key) ^ Objects.hashCode(value); &#125; public final V setValue(V newValue) &#123; V oldValue = value; value = newValue; return oldValue; &#125; public final boolean equals(Object o) &#123; if (o == this) return true; if (o instanceof Map.Entry) &#123; Map.Entry&lt;?,?&gt; e = (Map.Entry&lt;?,?&gt;)o; if (Objects.equals(key, e.getKey()) &amp;&amp; Objects.equals(value, e.getValue())) return true; &#125; return false; &#125;&#125; hash指的是key对应的hash值 其成员方法hashCode为node对象的hash值 在成员变量table中引用的就是这个Node transient Node&lt;K,V&gt;[] table; 其实在HashMap中大部分用到的是链表存储结构，很少用到树形存储结构 其实，理想情况下，hash函数设计的好，链表存储结构都用不到 静态工具方法hash()1234567891011121314151617181920/** * Computes key.hashCode() and spreads (XORs) higher bits of hash * to lower. Because the table uses power-of-two masking, sets of * hashes that vary only in bits above the current mask will * always collide. (Among known examples are sets of Float keys * holding consecutive whole numbers in small tables.) So we * apply a transform that spreads the impact of higher bits * downward. There is a tradeoff between speed, utility, and * quality of bit-spreading. Because many common sets of hashes * are already reasonably distributed (so don't benefit from * spreading), and because we use trees to handle large sets of * collisions in bins, we just XOR some shifted bits in the * cheapest possible way to reduce systematic lossage, as well as * to incorporate impact of the highest bits that would otherwise * never be used in index calculations because of table bounds. */static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; 为什么要有HashMap的hash()方法 难道不能直接使用KV中K原有的hash值吗？ 在HashMap的put、get操作时为什么不能直接使用K中原有的hash值？ 为什么要这么干呢？ 这个与HashMap中table下标的计算有关indexFor方法 12n = table.length;index = （n-1） &amp; hash; 因为table的长度都是2的幂，因此index仅与hash值的低n位有关，hash的高n位都被与操作置0了 假设table.length=2^4=16 由上图可以看到，只有hash值的低4位参与了运算 这样做很容易产生碰撞，这样就算散列值分布再松散，要是只取最后几位的话，碰撞也会很严重。更要命的是如果散列本身做得不好，分布上成等差数列的漏洞，恰好使最后几个低位呈现规律性重复，碰撞问题会更明显。 为了解决上述问题，设计者权衡了speed、utility, and quality，将高16位与低16位异或来减少这种影响 仅仅异或一下，既减少了系统的开销，也不会造成因为高位没有参与下标的计算（table的长度较小时），从而引起的碰撞 从上面的代码可以看出，key的hash值的计算方法 key的hash值高16位不变，低16位与高16位异或作为key的最终hash值 h &gt;&gt;&gt; 16，表示无符号右移16位，高位补0，任何数跟0异或都是其本身，因此key的hash值高16位不变。 混合原始哈希码的高位和低位，以此来加大低位的随机性。而且混合后的低位掺杂了高位的部分特征，这样高位的信息也被变相保留下来。 为什么HashMap的容量要是2的幂 因为这样（数组长度-1）正好相当于一个“低位掩码”。“与”操作的结果就是散列值的高位全部归零，只保留低位值，用来做数组下标访问。 以初始长度16为例，16-1=15。2进制表示是00000000 00000000 00001111。和某散列值做“与”操作如下，结果就是截取了最低的四位值。1234 10100101 11000100 00100101&amp; 00000000 00000000 00001111---------------------------------- 00000000 00000000 00000101 //高位全部归零，只保留末四位 tableSizeFor()123456789101112/** * Returns a power of two size for the given target capacity. */static final int tableSizeFor(int cap) &#123; int n = cap - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;&#125; 这个方法被调用的地方： 12345public HashMap(int initialCapacity, float loadFactor) &#123; /**省略此处代码**/ this.loadFactor = loadFactor; this.threshold = tableSizeFor(initialCapacity);&#125; 由此可以看到，当在实例化HashMap时，如果给定了initialCapacity，由于HashMap的capacity都是2的幂，因此这个方法用于找到大于等于initialCapacity的最小的2的次幂（initialCapacity如果就是2的幂，则返回的还是这个数） 下面分析这个算法 首先为什么要对cap做减1操作int n = cap - 1; 这是为了防止cap已经是2的幂 如果cap已经是2的幂，又没有执行这个减1操作，则执行完后面的 几条无符号右移操作后，返回的capacity将是这个cap的2倍 下面看几个无符号的右移操作 如果n这时为0（经历了cap-1之后），则经过后面的几次无符号右移依然是0，最后返回的capacity是1,（最后有个n+1的操作） 这里只讨论n不等于0的情况 第一次右移 n |= n &gt;&gt;&gt; 1; 由于n不等于0，则n的二进制表示中总会有一个bit为1，这时考虑最高位的1 通过无符号右移1位，则将最高位的1右移了一位，在做或操作，便得n的二进制表示中与最高位的1紧邻的右边一位也为1，如000011xxxxxx 第二次右移 n |= n &gt;&gt;&gt; 2; 注意这个n已经做过n |= n &gt;&gt;&gt; 1;操作 假设此时n为000011xxxxxx 则n无符号右移两位，会讲最高位两个连续的1右移两位，然后再与原来的n做或操作，这样n的二进制表示的高位中会有4个连续的1，如00001111xxxxxx 第三次右移 n |= n &gt;&gt;&gt; 4; 这次把已经有的高位中的连续的4个1，右移4位 再做或操作 这样n的二进制表示的高位中会有8个连续的1。如00001111 1111xxxxxx 依次类推 注意，容量最大也就是32bit的正数 因此最后n |= n &gt;&gt;&gt; 16; 最多也就32个1，但是这时已经大于了MAXIMUM_CAPACITY 所以取值到MAXIMUM_CAPACITY 举个例子 注意，得到的这个capacity却被赋值给了threshold this.threshold = tableSizeFor(initialCapacity); 这不是一个bug，因为在构造方法中，并没有对table这个成员变量进行初始化 table的初始化被推迟到了put方法中，在put方法中会对threshold重新计算 get()123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/** * Returns the value to which the specified key is mapped, * or &#123;@code null&#125; if this map contains no mapping for the key. * * &lt;p&gt;More formally, if this map contains a mapping from a key * &#123;@code k&#125; to a value &#123;@code v&#125; such that &#123;@code (key==null ? k==null : * key.equals(k))&#125;, then this method returns &#123;@code v&#125;; otherwise * it returns &#123;@code null&#125;. (There can be at most one such mapping.) * * &lt;p&gt;A return value of &#123;@code null&#125; does not &lt;i&gt;necessarily&lt;/i&gt; * indicate that the map contains no mapping for the key; it's also * possible that the map explicitly maps the key to &#123;@code null&#125;. * The &#123;@link #containsKey containsKey&#125; operation may be used to * distinguish these two cases. * * @see #put(Object, Object) */public V get(Object key) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value;&#125;/** * Implements Map.get and related methods * * @param hash hash for key * @param key the key * @return the node, or null if none */final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; if ((e = first.next) != null) &#123; if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null;&#125; 上面的注释中说的比较重要的一点就是，如果返回值是null，并不是一定没有这种KV映射，也可能是该key映射的值value是null，即key-null映射 也就说，使用get方法并不能判断这个key是否存在，只能通过containsKey来实现 由此可见get方法调用的是getNode方法，返回一个Node getNode方法接受两个参数hash值和key值 首先判断first node，在判断的时候，先看hash值是否相等，再看地址是否相等，再看equals的返回值 然后再遍历，判断first是不是树节点，是的话，在树中查找，否则，遍历链表 containsKey()1234567891011/** * Returns &lt;tt&gt;true&lt;/tt&gt; if this map contains a mapping for the * specified key. * * @param key The key whose presence in this map is to be tested * @return &lt;tt&gt;true&lt;/tt&gt; if this map contains a mapping for the specified * key. */public boolean containsKey(Object key) &#123; return getNode(hash(key), key) != null;&#125; put()1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768/** * Associates the specified value with the specified key in this map. * If the map previously contained a mapping for the key, the old * value is replaced. * * @param key key with which the specified value is to be associated * @param value value to be associated with the specified key * @return the previous value associated with &lt;tt&gt;key&lt;/tt&gt;, or * &lt;tt&gt;null&lt;/tt&gt; if there was no mapping for &lt;tt&gt;key&lt;/tt&gt;. * (A &lt;tt&gt;null&lt;/tt&gt; return can also indicate that the map * previously associated &lt;tt&gt;null&lt;/tt&gt; with &lt;tt&gt;key&lt;/tt&gt;.) */public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true);&#125;/** * Implements Map.put and related methods * * @param hash hash for key * @param key the key * @param value the value to put * @param onlyIfAbsent if true, don't change existing value * @param evict if false, the table is in creation mode. * @return previous value, or null if none */final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null;&#125; put方法将KV放在map中 如果，该key已经存放在map中，则用新值直接替换旧值 put的返回值：如果该key已经存放在map中，则返回其映射的旧值；如果不存在，则返回null，表示没有该key对应的映射值（也可能原来的映射是key-null） 当new HashMap实例时，并没有初始化其成员变量transient Node&lt;K,V&gt;[] table;，也就是说并没有为table分配内存 只有当put元素时才通过resize方法对table进行初始化 因此，建议需要先了解一下resize方法 put方法分为两种情况 bucket是以链表形式存储还是以树形结构存储 如果key已存在则修改旧值，并返回旧值 如果key不存在，则执行插入操作，返回null 如果是插入操作还要modCount++ 但如果是链表存储时，如果插入元素之后超过了TREEIFY_THRESHOLD，还要进行树化操作 注意，put操作，当发生碰撞时，如果是使用链表处理冲突，执行尾插法。这个跟ConcurrentHashMap不同，ConcurrentHashMap执行的是头插法。因为，其HashEntry的next是final的 put的基本操作流程 通过hash值得到所在bucket的下标，如果为null，表示没有发生碰撞，则直接put 如果发生了put，则解决发生碰撞的实现方式：链表还是树 如果能够找到该key的节点，则执行更新操作，无需对modCount增1 如果没有找到该key的节点，则执行插入操作，需要对modCount增1 在执行插入操作时，如果bucket中bin的数量超过TREEIFY_THRESHOLD，则要树化 在执行插入操作之后，如果size超过了threshold，则需要扩容 resize()12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182/** * Initializes or doubles table size. If null, allocates in * accord with initial capacity target held in field threshold. * Otherwise, because we are using power-of-two expansion, the * elements from each bin must either stay at same index, or move * with a power of two offset in the new table. * * @return the table */final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; if (oldCap &gt; 0) &#123; if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; else &#123; // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr; @SuppressWarnings(&#123;"rawtypes","unchecked"&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; if (oldTab != null) &#123; for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // preserve order Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125; 注释翻译 初始化或者翻倍表大小。如果表为null，则根据存放在threshold变量中的初始化capacity的值来分配table内存（这个注释说的很清楚，在实例化HashMap时，capacity其实是存放在了成员变量threshold中，注意，HashMap中没有capacity这个成员变量）。如果表不为null，由于我们使用2的幂来扩容，则每个bin元素要么还在原来的bucket中，要么在2的幂中 代码解析newCap与newThr12345678910111213141516171819202122232425Node&lt;K,V&gt;[] oldTab = table;int oldCap = (oldTab == null) ? 0 : oldTab.length;int oldThr = threshold;int newCap, newThr = 0;if (oldCap &gt; 0) &#123; if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold&#125;else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr;else &#123; // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);&#125;if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE);&#125;threshold = newThr; 如果oldTab != null，则oldCap&gt;0; 如果此时oldCap &gt;= MAXIMUM_CAPACITY，则表示已经到了最大容量，这时还要往map中放数据，则阙值设置为整数的最大值Integer.MAX_VALUE，直接返回这个oldMap的内存地址 如果此时oldCap&lt; MAXIMUM_CAPACITY，表示还没到达最大容量 如果进行扩容后newCap &lt; MAXIMUM_CAPACITY并且 oldCap的初始化值大于等于DEFAULT_INITIAL_CAPACITY（16），则将threshold扩大一倍。因为threshold=capacity*loadFactor，capacity变成原来的2倍，则threshold也要变成原来的2倍。 如果oldTab==null，则oldCap=0： 如果oldThr&gt;0，表示在实例化HashMap时，调用了HashMap的带参构造方法，初始化了threshold，这时将阈值赋值给newCap，因为在构造方法 中是将capacity赋值给了threshold。 如果oldThr&lt;=0，表示在实例化HashMap时，调用的是HashMap的默认构造方法，则newCap和newThr都使用默认值 这时要判断newThr是否等于0 newThr等于0表示 123else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; &gt;= DEFAULT_INITIAL_CAPACITY)newThr = oldThr &lt;&lt; 1; // double thresholdoldCap&gt;0， 这一步判断失败，有可能是扩容后大于了MAXIMUM_CAPACITY，也有可能是oldCap小于DEFAULT_INITIAL_CAPACITY导致的和oldCap 12else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; 判断成功，oldThr大于0 然后根据newCap和loadFactor确定newThr。有可能这时newCap已经大于MAXIMUM_CAPACITY了，则将thresHold设置为最大的整数，否则直接使用计算得来的新的newThr。 下面就是分配内存，如果oldTab == null，则 返回newTab。 如果oldTab = null，则需要将原内存地址中的数据拷贝给newTab的地址 下标的变化 例如我们从16扩展为32时，具体的变化如下所示 其中n即表示容量capacity。resize之后，因为n变为2倍，那么n-1的mask范围在高位多1bit(红色)，因此新的index就会发生这样的变化 因此，我们在扩充HashMap的时候，不需要重新计算hash，只需要看看原来的hash值新增的那个bit是1还是0就好了，是0的话索引没变，是1的话索引变成“原索引+oldCap”。可以看看下图为16扩充为32的resize示意图： 这个设计确实非常的巧妙，既省去了重新计算hash值的时间，而且同时，由于新增的1bit是0还是1可以认为是随机的，因此resize的过程，均匀的把之前的冲突的节点分散到新的bucket了。 在链表中进行操作时，使用的是尾插法]]></content>
      <categories>
        <category>Java</category>
        <category>源码阅读</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>源码阅读</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HashMap为什么线程不安全]]></title>
    <url>%2FJava%2F%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%2FHashMap%E4%B8%BA%E4%BB%80%E4%B9%88%E7%BA%BF%E7%A8%8B%E4%B8%8D%E5%AE%89%E5%85%A8%2F</url>
    <content type="text"><![CDATA[HashMap是我们平台开发中最经常使用数据结构之一。我们都知道，HashMap不是一个线程安全的数据结构，那它到底为什么线程不安全呢？它的不安全体现在什么地方呢？ Map概述HashMap不保证遍历的顺序和插入的顺序是一致的。HashMap允许有一条记录的key为null，但是对值是否为null不做要求。 HashTable类是线程安全的，它使用synchronize来做线程安全，全局只有一把锁，在线程竞争比较激烈的情况下hashtable的效率是比较低下的。因为当一个线程访问hashtable的同步方法时，其他线程再次尝试访问的时候，会进入阻塞或者轮询状态，比如当线程1使用put进行元素添加的时候，线程2不但不能使用put来添加元素，而且不能使用get获取元素。所以，竞争会越来越激烈。 相比之下，ConcurrentHashMap使用了分段锁技术来提高了并发度，不在同一段的数据互相不影响，多个线程对多个不同的段的操作是不会相互影响的。每个段使用一把锁。所以在需要线程安全的业务场景下，推荐使用ConcurrentHashMap，而HashTable不建议在新的代码中使用，如果需要线程安全，则使用ConcurrentHashMap，否则使用HashMap就足够了。 LinkedHashMap属于HashMap的子类，与HashMap的区别在于LinkedHashMap保存了记录插入的顺序。 TreeMap实现了SortedMap接口，TreeMap有能力对插入的记录根据key排序，默认按照升序排序，也可以自定义比较强，在使用TreeMap的时候，key应当实现Comparable。 HashMap实现Java7和Java8在实现HashMap上有所区别，当然Java8的效率要更好一些，主要是Java8的HashMap在Java7的基础上增加了红黑树这种数据结构，使得在桶里面查找数据的复杂度从O(n)降到O(logn)，当然还有一些其他的优化，比如resize的优化等。 介于Java8的HashMap较为复杂，本文将基于Java7的HashMap实现来说明，主要的实现部分还是一致的，Java8的实现上主要是做了一些优化，内容还是没有变化的，依然是线程不安全的。 HashMap的实现使用了一个数组，每个数组项里面有一个链表的方式来实现，因为HashMap使用key的hashCode来寻找存储位置，不同的key可能具有相同的hashCode，这时候就出现哈希冲突了，也叫做哈希碰撞，为了解决哈希冲突，有开放地址方法，以及链地址方法。HashMap的实现上选取了链地址方法，也就是将哈希值一样的entry保存在同一个数组项里面，可以把一个数组项当做一个桶，桶里面装的entry的key的hashCode是一样的。 Hash冲突的解决办法 上面的图片展示了我们的描述，其中有一个非常重要的数据结构Node&lt;K,V&gt;，这就是实际保存我们的key-value对的数据结构，下面是这个数据结构的主要内容：1234final int hash; final K key;V value;Node&lt;K,V&gt; next; 一个Node就是一个链表节点，也就是我们插入的一条记录，明白了HashMap使用链地址方法来解决哈希冲突之后，我们就不难理解上面的数据结构，hash字段用来定位桶的索引位置，key和value就是我们的数据内容，需要注意的是，我们的key是final的，也就是不允许更改，这也好理解，因为HashMap使用key的hashCode来寻找桶的索引位置，一旦key被改变了，那么key的hashCode很可能就会改变了，所以随意改变key会使得我们丢失记录（无法找到记录）。next字段指向链表的下一个节点。 HashMap的初始桶的数量为16，loadFact为0.75,当桶里面的数据记录超过阈值的时候，HashMap将会进行扩容则操作，每次都会变为原来大小的2倍，直到设定的最大值之后就无法再resize了。 下面对HashMap的实现做简单的介绍，具体实现还得看代码，对于Java8中的HashMap实现，还需要能理解红黑树这种数据结构。 根据key的hashCode来决定应该将该记录放在哪个桶里面，无论是插入、查找还是删除，这都是第一步，计算桶的位置。因为HashMap的length总是2的n次幂，所以可以使用下面的方法来做模运算： 1h&amp;(length-1) h是key的hashCode值，计算好hashCode之后，使用上面的方法来对桶的数量取模，将这个数据记录落到某一个桶里面。当然取模是Java7中的做法，Java8进行了优化，做得更加巧妙，因为我们的length总是2的n次幂，所以在一次resize之后，当前位置的记录要么保持当前位置不变，要么就向前移动length就可以了。所以Java8中的HashMap的resize不需要重新计算hashCode。我们可以通过观察Java7中的计算方法来抽象出算法，然后进行优化，具体的细节看代码就可以了。 HashMap的put方法 上图展示了Java8中put方法的处理逻辑，比Java7多了红黑树部分，以及在一些细节上的优化，put逻辑和Java7中是一致的。 resize机制 HashMap的扩容机制就是重新申请一个容量是当前的2倍的桶数组，然后将原先的记录逐个重新映射到新的桶里面，然后将原先的桶逐个置为null使得引用失效。后面会讲到，HashMap之所以线程不安全，就是resize这里出的问题。 为什么HashMap线程不安全HashMap在resize操作的时候会造成线程不安全。下面将举两个可能出现线程不安全的地方。 put的时候导致的多线程数据不一致 比如有两个线程A和B，首先A希望插入一个key-value对到HashMap中，首先计算记录所要落到的桶的索引坐标，然后获取到该桶里面的链表头结点，此时线程A的时间片用完了，而此时线程B被调度得以执行，和线程A一样执行，只不过线程B成功将记录插到了桶里面，假设线程A插入的记录计算出来的桶索引和线程B要插入的记录计算出来的桶索引是一样的，那么当线程B成功插入之后，线程A再次被调度运行时，它依然持有过期的链表头但是它对此一无所知，以至于它认为它应该这样做，如此一来就覆盖了线程B插入的记录，这样线程B插入的记录就凭空消失了，造成了数据不一致的行为。 另外一个比较明显的线程不安全的问题是HashMap的get操作可能因为resize而引起死循环（cpu100%），具体分析如下： 下面的代码是resize的核心内容 12345678910111213141516void transfer(Entry[] newTable, boolean rehash) &#123; int newCapacity = newTable.length; for (Entry&lt;K,V&gt; e : table) &#123; while(null != e) &#123; Entry&lt;K,V&gt; next = e.next; if (rehash) &#123; e.hash = null == e.key ? 0 : hash(e.key); &#125; int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; &#125; &#125; &#125; 这个方法的功能是将原来的记录重新计算在新桶的位置，然后迁移过去 我们假设有两个线程同时需要执行resize操作，我们原来的桶数量为2，记录数为3，需要resize桶到4，原来的记录分别为：[3,A],[7,B],[5,C]，在原来的map里面，我们发现这三个entry都落到了第二个桶里面。 假设线程thread1执行到了transfer方法的Entry next = e.next这一句，然后时间片用完了，此时的e = [3,A], next = [7,B]。线程thread2被调度执行并且顺利完成了resize操作，需要注意的是，此时的[7,B]的next为[3,A]。此时线程thread1重新被调度运行，此时的thread1持有的引用是已经被thread2 resize之后的结果。线程thread1首先将[3,A]迁移到新的数组上，然后再处理[7,B]，而[7,B]被链接到了[3,A]的后面，处理完[7,B]之后，就需要处理[7,B]的next了啊，而通过thread2的resize之后，[7,B]的next变为了[3,A]，此时，[3,A]和[7,B]形成了环形链表。 在get的时候，如果get的key的桶索引和[3,A]和[7,B]一样，那么就会陷入死循环。 fail-fast策略如果在使用迭代器的过程中有其他线程修改了map，将抛出ConcurrentModificationException，这就是所谓fail-fast策略。 这一策略在源码中的实现是通过modCount域，modCount顾名思义就是修改次数，对HashMap内容的修改都将增加这个值，那么在迭代器初始化过程中会将这个值赋给迭代器的expectedModCount。12345678HashIterator() &#123; expectedModCount = modCount; if (size &gt; 0) &#123; // advance to first entry Entry[] t = table; while (index &lt; t.length &amp;&amp; (next = t[index++]) == null) ; &#125; &#125; 在迭代过程中，判断modCount跟expectedModCount是否相等，如果不相等就表示已经有其他线程修改了Map： 注意到modCount声明为volatile，保证线程之间修改的可见性。123final Entry&lt;K,V&gt; nextEntry() &#123; if (modCount != expectedModCount) throw new ConcurrentModificationException(); 为什么String, Interger这样的wrapper类适合作为键？String, Interger这样的wrapper类作为HashMap的键是再适合不过了，而且String最为常用。因为String是不可变的，也是final的，而且已经重写了equals()和hashCode()方法了。其他的wrapper类也有这个特点。 不可变性是必要的，因为为了要计算hashCode()，就要防止键值改变，如果键值在放入时和获取时返回不同的hashcode的话，那么就不能从HashMap中找到你想要的对象。 不可变性还有其他的优点如线程安全。如果你可以仅仅通过将某个field声明成final就能保证hashCode是不变的，那么请这么做吧。因为获取对象的时候要用到equals()和hashCode()方法，那么键对象正确的重写这两个方法是非常重要的。如果两个不相等的对象返回不同的hashcode的话，那么碰撞的几率就会小些，这样就能提高HashMap的性能。 为什么HashMap线程不安全]]></content>
      <categories>
        <category>Java</category>
        <category>源码阅读</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>源码阅读</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP中get和post的区别]]></title>
    <url>%2F%E7%BD%91%E7%BB%9C%2FHTTP%E4%B8%ADget%E5%92%8Cpost%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[作为Java web开发工程师，平时项目中经常会使用到http请求。一般情况下我们最常用的就是get和post方法了，一般需要后台Java调用接口的时候，我个人喜欢用post方法。 那么问题来了，你要问我get和post有什么具体的区别呢？大体上我还是能答得上来几条不同点的，但是系统全面的区别我可能就给不出了。所以今天来系统地整理下HTTP中get和post的区别。 Http定义了与服务器交互的不同方法，最基本的方法有4种，分别是GET，POST，PUT，DELETE。URL全称是资源描述符，我们可以这样认为：一个URL地址，它用于描述一个网络上的资源，而HTTP中的GET，POST，PUT，DELETE就对应着对这个资源的查，改，增，删4个操作。GET一般用于获取/查询资源信息，而POST一般用于更新资源信息。 原理性1. 根据HTTP规范，GET用于信息获取，而且应该是安全的和幂等的 所谓安全，意味着该操作用于获取信息而非修改信息。换句话说，GET请求一般不应产生副作用。就是说，它仅仅是获取资源信息，就像数据库查询一样，不会修改，增加数据，不会影响资源的状态。注意这里的安全仅仅指不会修改资源。 幂等的意味着对同一URL的多个请求应该返回同样的结果。 2. 根据HTTP规范，POST表示可能修改变服务器上的资源的请求 上面大概说了一下HTTP规范中GET和POST的一些原理性的问题。但在实际的做的时候，很多人却没有按照HTTP规范去做，导致这个问题的原因有很多: 很多人贪方便，更新资源时用了GET，因为用POST必须要到FORM（表单），这样会麻烦一点 对资源的增，删，改，查操作，其实都可以通过GET/POST完成，不需要用到PUT和DELETE 另外一个是，早期的Web MVC框架设计者们并没有有意识地将URL当作抽象的资源来看待和设计，所以导致一个比较严重的问题是传统的Web MVC框架基本上都只支持GET和POST两种HTTP方法，而不支持PUT和DELETE方法。RESTFUL也是近几年才火起来的概念。 表面现象说完原理性的问题，我们再从表面现像上面看看GET和POST的区别 1. 数据的位置不同 GET请求的数据会附在URL之后（就是把数据放置在HTTP协议头中），以?分割URL和传输数据，参数之间以&amp;相连 POST把提交的数据则放置在是HTTP包的包体中 2. GET方式提交的数据有长度限制，而POST提交的数据理论上没有长度限制 注意，这里没有具体指出长度限制是多少多少KB。 因为实际上HTTP协议并没有明确规定GET和POST分别能携带多少数据。限制数据量大小的是浏览器和服务器。 浏览器限制了URL的长度，所以GET的数据+URL自身的长度不能超过浏览器的限制 POST携带的数据多少主要是受服务器处理数据量大小的限制 3. POST的安全性要比GET的安全性高 这里所说的安全性和上面GET提到的“安全”不是同个概念。 上面“安全”的含义仅仅是不作数据修改，而这里安全的含义是真正的Security的含义 比如：通过GET提交数据，用户名和密码将明文出现在URL上，因为(1)登录页面有可能被浏览器缓存，(2)其他人查看浏览器的历史纪录，那么别人就可以拿到你的账号和密码了 除此之外，使用GET提交数据还可能会造成Cross-site request forgery攻击 总结Get是向服务器发索取数据的一种请求，而Post是向服务器提交数据的一种请求，在FORM（表单）中，Method默认为GET，实质上，GET和POST只是发送机制不同，并不是一个取一个发！ 浅谈HTTP中Get与Post的区别]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>网络</tag>
        <tag>HTTP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[修改Github项目的语言分类]]></title>
    <url>%2FGit%2F%E4%BF%AE%E6%94%B9Github%E9%A1%B9%E7%9B%AE%E7%9A%84%E8%AF%AD%E8%A8%80%E5%88%86%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[自己辛辛苦苦写的Java项目，因其中包含了一定了前端代码，在传到Github上被标记成了JavaScript语言，这是一件多么蛋疼的事儿 那如何修改Github项目的语言分类呢？？ 在开始正文之前，请允许我先描述一下遇到这个问题的背景。 早上起来看到论坛上有人说TaoCode快关闭了，打开电脑登录一看还真是 想起来，TaoCode SVN还是上软工2开始接触的第一个版本控制工具。当时，钦哥哥要求将代码部署在SVN上，本来是要自己搭建SVN服务器的，后来我们取巧，发现TaoCode已经帮我们部署好了SVN服务器，于是我们便欢快地注册账号使用起来（注册于2014-09-20） 这一用便是三四年的时间，虽然大四在阿里实习的时候，接触到了另一个版本控制工具Git，便不怎么使用了TaoCode SVN了。但可以说TaoCode陪伴我从一个啥都不会的小白成长为如今的合格 程序猿 研发工程师，此中别有一番感情[捂脸] 在TaoCode上还存在着大大小小近10个项目，如今TaoCode即将停止服务，所以我便想着将自己的代码迁移到Github上来。Github本身提供了从SVN导入仓库的功能，这个使用起来很方便。然鹅，导入的项目被Github自动识别成了JavaScript语言，我可是纯正的Java web项目啊。那要怎么修改Github项目的语言分类呢？？ 注意，前方高能~~~ 下面正式介绍如何修改Github项目的语言分类 在项目目录中创建一个名为.gitattributes的文件，添加以下代码：1*.js linguist-language=Java 然后重新上传到Github上，就可以发现项目的语言变成Java了，其他语言同理 根据我的理解，如果项目中没有.gitattributes的文件来显式指出项目的语言，那么Github会自动根据项目文件出现最多的后缀来判断项目的语言，所以，一个Java web项目很容易就被判定成JavaScript语言 上述的解决办法是通过将.js结尾的文件当成Java文件来统计，类似的还有1234*.html linguist-language=Java*.js linguist-language=Java*.css linguist-language=Java.... 其他可以自由变通]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[软件管理沉思录读书笔记]]></title>
    <url>%2F%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%2F%E8%BD%AF%E4%BB%B6%E7%AE%A1%E7%90%86%E6%B2%89%E6%80%9D%E5%BD%95%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[全书呼吁管理者要树立一个愿景：生产更好的产品，获得更大的职业满足，享受创造出某种能用而又有用的东西时的喜悦。 它向我们描述了一种全面而有序的方法，涵盖软件开发的各个层面，不论是工程师的单兵作战还是团队合作开发大型软件。这本书值得成为每位软件开发管理者和学生的案头必备。 本书要点本书是从沃茨的著作、论文以及专栏文章当中精选而成，包含了许多建议、故事以及来之不易的经验，而非具体介绍PSP或TSP如何实现。 本书是想让读者理解，成功的软件项目管理会遭遇许多障碍。为了成功完成项目，工程师需要管理的不仅仅是项目本身，他们必须利用自己和团队的经验，理解项目并提前做出计划。 这些文章解释了软件发展中固有的挑战，并帮助软件工程师明白如何才能成功。 精编书摘管理你的项目交付高质量的产品软件质量改进的需求是巨大的，并且这种改进已经不能单纯地靠延续过去那种基于测试的方法来实现 如果一件产品提供了对用户而言是最重要的功能，那么这就是一件高质量的产品 与漏洞相比，缺陷更像是定时炸弹。尽管不是所有的缺陷都会带来爆炸性影响，但是有一些缺陷的确会 只要技术还在不断进步，并且这种进步能引起新的和不同类型用户的兴趣，我们就会面临新的质量需求 为了完成重要的工作，首先要精确地知道什么是你要努力去实现的 编程是一项严谨的工作，所有从事此项工作的专业人员都是在完成对质量要求极为苛刻的任务。 在软件开发过程中，为了保证质量始终如一，必须遵循以下8个步骤： 确立质量控制的策略、目标和计划 正确训练、指导和支持开发人员及其团队 确立和维护软件需求的质量管理过程 确立和维护软件工程过程的统计控制 审查、检查并评估所有的产品制品 评估所有缺陷，加以更正并用以识别、纠正和预防其他类似问题 确立和维护配置管理和变更控制系统 持续改进开发过程 软件产品的质量应当被定义为产品对用户的有用性。 只有得到了清晰的需求，才有可能开发出高质量的程序。 软件工程师的工作就是在计划成本和计划进度内交付高质量的产品 虽然软件质量包括很多方面，但是首先应当关注的质量问题就是软件的缺陷 目标之所以重要，主要是基于以下两条原因：它们提供了努力的焦点，而且建立了一种优先次序。 为高质量项目制定计划做两类计划，第一类是基于时间段的计划，阶段计划关心在这一时间段内你准备如何利用时间。第二种是基于行动的计划，比如开发一个程序或者撰写一份报告 产品计划会帮助判断完成工作将需要多少时间，以及会在什么时候完成。计划还可以帮助在工作期间追踪过程 计划必须是易于理解、清晰明白、详细具体、精确缜密、准确无误的 动态计划是避免被种种细微变化蚕食至死的唯一解决拌饭 一份合格的产品计划应当包括三项内容： 将要生产的产品规格和重要的性能指标 估算工作需要的时间 进度预测 3.2.管理你的团队高效团队的基本要素高效团队必备的四个条件 团结 富有挑战性的目标 反馈 通用的工作架构 团队交流的三要素 透明 倾听 协商 自主指导型团队的特征是，自己设计开发策略，自己制定计划，并且积极主动、高质量地完成工作 目标追踪和反馈是极其重要的 共鸣性倾听是一种主动性倾听，它对软件团队尤其重要。 群体类型：工作型群体、过程型群体、对抗型群体 工作风格：开发型群体、随意型群体、封闭型群体、同步型群体 自主指导型团队的典型行为：团队成员会主动发现需要做什么并及时去做，而不用人告诉要做什么，为了能完成任务他们会去做任何需要做的事情。 凝聚力是把团队成员紧密联系在一起的纽带，而要想形成凝聚力就需要接触和交往。 组织-动荡-规范-执行 做一位高效的团队成员优秀的团队成员会去做任何需要做的事 承诺是一项必须要学习的道德规范 只有基于计划才能做出负责人的承诺 对目标状态精确、及时的反馈，是高效能团队绝对必须的前提条件 要想让团队运行顺畅，每一位成员都应当奉献出他所知道的一切。 原则式谈判之所以有效的原因是，它避免了立场的两极分化。原则式谈判的基础是，认识到立场只是满足利益的一种方式，把注意力集中在利益而不是立场上。 剔除不履行职责的团队成员通常会提升团队的整体表现。 支持的本质是帮助人们达到他所能达到的境界。 提供有效支持的关键是，帮助团队伙伴相信他们自己的能力。 领导和指导你的团队对于工作团队，有三条最重要的激励因素：恐惧、贪婪和承诺 可信的团队承诺有四条要求：自愿、可见、可信、和得到承认 只有当项目团队成员开始自由、开放地交流，开始有自己的主张并协商解决不同意见时，这支团队才有可能是凝胶团队 理性管理的四要素 首先，在确定产品或行动目标时，要检查当前的工作情况，并且根据业务目的确立行动目标。 其次，为短期目标制定计划 再次，评估和追踪计划完成情况，并且监督工作中的纪律执行情况 最后，持续监督业务执行情况 管理你的领导讨论项目并捍卫你的计划让团队聚焦于优先级最高的事情 在做出任何承诺之前都要先制定一个计划 制定一个合理、详细的计划是掌控项目的第一步 让计划保持更新是极其关键的，因为软件开发是一个时刻变化的事业，并且每一个意外对团队来说通常都意味着更多的工作。 管理者的层级之所以重要，是因为改进过程处理的是那些具有长期影响的议题，而这正是高层管理者关注的重点 对管理者来说，工作就是利用所有的团队资源去完成此项工作，其他所有的事情都是第二位。 要建立一支真正凝聚力和活力的团队，就必须始终高度重视团队目标，在工作表现中为队员作出表率，坚持高标准，并且为团队所有的工作负责。 管理你自己控制你的工作成功者赢得胜利，他们决不抱怨。正是那些永远失败的人才会抱怨人生的不公以及别人该如何为自己的失败负责。 要想生产出优致的产品，必须时不时地休息游戏。不过，为了有较高的效率，同时也是为了优质的完成工作，我们需要对这种休息掌握主动权。 软件项目失败的主要原因通常是团队合作问题而不是技术问题。 学习制定好计划的第一步就是制定计划 要想制定出更加精确的计划，就需要找出先前计划中哪些地方出了错以及怎样改进 要想管理好时间，就是要计划时间并遵守这个计划 真正达成一致是个人承诺中必最重要也是唯一的特征。 管理承诺最主要的原因是，这样就不会忽略或忘记它 管理承诺的另外一个原因是，当你要做的工作超出了可用的时间时，这样做就可以帮到你 不断挑战自己去完成更优质的工作 关键是要保持开放的心态，并时刻做好准备 执著追求卓越，才有可能臻于卓越 学会领导最成功的团队都有精力充沛、富有热情、满怀信心以及严格要求的领导者 在开发工作中，目标同样重要，但它们很少会是单一明确的。作为领导者，工作很重要的一个方面就是保持团队目标清晰明确，确保团队每一位成员都知道他当前的工作对实现目标有什么帮助 目标非常重要，它们提供了团队成功所需的关注点、激励和能量 工程的本质是质量 比质量标准更重要的是团队合作和支持的标准 领导力低下会有很多症状，但它们的根源通常是因为没有认识到什么是真正需要做的，没有明确一个能充分利用可用资源和机会的方向 领导者和管理者之间最主要的区别是，，管理者命令员工服从他们的指令，而领导者是带领他们完成任务 后记个人开发者正确地完成软件开发工作，这其中很重要的一部分就是，学习成为自己的管理者，学习制定自己的进度表，追踪自己的过程，管理自己的工作质量，做出个人承诺并始终如一地实现你的承诺 软件能力成熟度模型集成（CMMI）是一种过程改进成熟度模型，用于产品或服务开发。它由一些最佳的实践方法组成，涵盖了产品从概念到发布以及维护的整个生命周期 软件能力成熟度（CMM）是一种过程改进方法，旨在为组织提升其表现提供所需的高效过程。]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[软件随想录读书笔记]]></title>
    <url>%2F%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%2F%E8%BD%AF%E4%BB%B6%E9%9A%8F%E6%83%B3%E5%BD%95%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[这本书是一部关于软件技术、人才、创业和企业管理的随想文集。 作者以诙谐幽默的笔触将自己在软件行业的亲身感悟娓娓道来，观点新颖独特，内容简洁实用。 从不同侧面满足了软件开发人员、设计人员、管理人员以及从事软件相关工作的人员的学习与工作需要。 本书要点如何从上大学伊始便规划自己的程序员之路？ 成功的软件项目是如何运作的？ 怎样才能找到并留住最优秀的程序员？ 软件公司具备哪些特质才能成功？ 本书是一位软件行业老兵的随想录，为圈内圈外的读者了解软件行业打开了一扇门。 总结了软件行业最本质、最重要的实践、技巧和种种前车之鉴 精编书摘人员管理作者通过回忆在微软工作的时候，自己设计Excel函数语言的时候被比尔盖茨做例行审查，盖茨在作者提交的规格说明上认真做了笔记，审查的时候问了非常细致的问题，包括一个在之前产品出现的日期函数错误问题。作者通过将比尔盖茨与其他几位不懂技术的大公司高管对比，得出结论。 优秀的人才从不在市场上求职 内部激励比外部激励强的多。人们会为那些他们真正想做的事格外努力地工作。 认同法的管理目标是，使得人们认同你希望达到的目标。 认同法的作用是设法创造出内部激励。 写给未来程序员的建议能够在多个抽象层次上同时思考问题的能力，是设计出哟秀软件架构所必须的。 当程序员遇到问题的时候，他们会把问题重新定义，使得这些问题可以用算法解决。这样一来，问题转化成他们可以解决的形式，但是实际上，那些问题是一种琐碎的问题。也就是说，程序员解决的只是问题的某种外在形式，而并没有解决真正的问题。 微软公司中那些真正优秀的程序经理都是具有优秀写作能力的人。 对计算机专业学生的7条建议 毕业前练好写作 毕业前学好C语言 毕业前学好微观经济学 不要因为枯燥就不选修非计算机专业的课程 选修有大量编程实践的课程 别担心所有工作都被印度人抢走 找一份好的暑期实习工作 寻求专业人士的帮助，培养自信心 团体中最有权势和影响力的程序员正是那些表达能力强的程序员，他们无论是做书面表达还是做口头表达，都能够清晰、自如、具有说服力地传达观点。 一个普通程序员与一个优秀程序员的区别，在于他们能否与他人交流思想。 设计的作用一般情况下，用户在面对模棱两可的选择的时候，会倾向于自己习惯的那种。 创造一个有使用价值的软件，就必须时时刻刻都在奋斗，每一次的修补，每一个功能，每一处小小的改进，都是在奋斗，目的就是为了再多创造一点空间，可以再多吸引一个用户加入。没有捷径可走。你需要一点运气，但是这不取决于你是否幸运。你之所以有好运气，那是因为你寸土必争。 给客户的选择越多，他们的选择就越困难，就会感到越不开心。 社会化软件运作的方式在很大程度上决定了围绕它所形成的用户社区的类型。 避开攻击最好的方法之一就是让它看上去好像获得了成功。 管理大型项目在它们的结合点，所有相关的事宜都必须取得一致，否则两者无法一起工作。 标准当然很重要的，但不能迷信标准。 理想主义者在大原则上是百分之百正确的，而实用主义者在现实中也是正确的。 编程建议你的目的是最有效率、最物有所值地实用你的时间。 只有第一线的程序员才能提出完成日期的估计值。 一发现错误就立即修正，将用时算入原始任务的用时之中。 防止管理层向程序员施加压力，要求加快开发速度。 一份日程规划就是一个装满木块的盒子。 有效的日程规划是创造优秀软件的钥匙。它强迫你首先完成最重要的功能，让你做出正确的选择，思考要开发一个怎样的软件。 从长远的观点来看，那些不关心效率、不关心程序是否臃肿、一个劲往软件中加入高级功能的程序员最终将拥有更好的产品。 程序员的最高境界：用心构建代码，发挥洞察力，将它们写的清晰易懂，不容易出错 这里提到了如何应对XSS攻击 让错误的代码显而易见是一种很好的实践，但未必是所有安全问题的最佳解决方案 让一行代码相关的所有信息尽可能地靠拢，缩短它们之间的物理距离。 只要一看到代码，就会试着去找出错误，这样能防止程序出现问题。 为了使代码真正强壮可靠，当你查看代码的时候，需要有一个好的代码书写规范，允许相关信息集中在一个地方。 出现在你眼前的关于代码行为的信息越多，你就能越轻松简单地发现错误。 开办软件公司创办软件公司的真正乐趣就是，创造一些东西，自己参与整个过程，悉心培育，不间断地劳作，不断地投入，看着它成长，看着自己一步步得到报偿。这是世界上最带劲的旅程。 本质上，软件质量的改进会创造出新的价值，而且价值创造的速度要快于成本提升的速度。 经营软件公司良好的办公室——尤其是单独的办公室——能够提高程序员的生产率 组织beta测试的最高秘诀 开放式的beta测试是没有用的 要想找到那些能够向你反馈意见的测试者，最好的方法是诉诸他们言行一致的心理 不要妄想一次完整的beta测试的所有步骤能够在少于8-10周的时间内完成 不要妄想在测试中发布新的软件版本的频率能够快于每两周一次 一次beta测试中计划发布的软件版本不要少于4个 如果在测试过程中为软件添加了一个新功能，那么哪怕这个功能非常微小，整个8个星期的测试也要回到起点，从头来过，而且你还需要再发布3个或4个新版本 即使你有一个申请参加beta测试的步骤，最后也只有五分之一的测试者会向你提交反馈意见 不要混淆技术beta和市场beta 技术支持团队必须能够与开发团队直接沟通。 发布软件软件开发周期的基本规则 确定发布日期，这个日期可以根据客观情况也可以根据主观愿望进行选择 列出软件要实现的功能，然后按照优先顺序排序 每当落后于预定进程时，就把排在最后的功能砍掉 挑选软件发布日期的三种方法 经常发布稍作改进的版本 每12到18个月发布一次 每3年到5年发布一次 如果你的顾客人数较少，那么最好经常性地发布小幅修改的新版本 如果已经有了（或者想有）大量的付费用户，那么最好不要频繁地发布新版本 对于有几百万用户和几百万整合点的软件系统，最好偶尔才发布新版本 从易用性的角度看，降低发布的频率，把多处修改改为一次发布，而不是多次发布。而且在发布的时候，努力使得整个网站的视觉效果发生变化，让网站看上去怪怪的，使得用户凭借直觉就知道网站发生了重大变化。 修订软件当某个地方出错的时候，就问为什么，一遍遍地追问，直到找到根本性的原因为止。然后，针对根本性的原因开始着手解决问题，要从根本上解决这个问题，而不是只解决一些表面的症状。 最好尽一切可能坚持制作面向整个市场销售的上架软件。 不要因为有些事情不得不做，就去做。 如果想把事情做完，无论何时，一定要想清楚什么是眼下最重要、必须马上做好的事。]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[黑客与画家读书笔记]]></title>
    <url>%2F%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%2F%E9%BB%91%E5%AE%A2%E4%B8%8E%E7%94%BB%E5%AE%B6%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[这本书是硅谷创业之父，Y Combinator创始人Paul Graham的文集。之所以叫这个名字，是因为作者认为黑客（并非负面的那个意思）与画家有着极大的相似性，他们都是在创造，而不是完成某个任务。 未来的人类生活不仅是人与人的互动，而是越来越多的与计算机互动，所以必须理解计算机，其关键就是理解计算机背后的人，即设计计算机的人——程序员，而最伟大的程序员就是黑客。 本书要点尝试解释计算机世界里发生了什么事，不仅仅是写给成员看的，也适合所有人。作者最大的目的就是，让普通读者理解我们所处的这个计算机时代。 主要介绍黑客即优秀程序员的爱好和动机，讨论黑客成长、黑客对世界的贡献以及编程语言和黑客工作方法等所有对计算机时代感兴趣的人的一些话题。 适合所有程序员和互联网创业者，也适合一切对计算机行业感兴趣的人 精编书摘为什么书呆子不受欢迎为什么会被歧视和欺负？所有现在还在学校里读书的人可能会又一次觉得，怎么会有人问出这么蠢的问题。怎么可能会有其他结果呢？当然会有其他结果。一般来说，成年人就不会去欺负书呆子。为什么小孩子会这样做呢？ 一部分原因是，青少年在心理上还没有摆脱儿童状态，许多人都会残忍地对待他人。他们折磨书呆子的原因就像拔掉一条蜘蛛腿一样，觉得很好玩。在一个人产生良知之前，折磨就是一种娱乐。 孩子们欺负书呆子的另一个原因是为了让自己感到好受一些。当你踩水的时候，你把水踩下去，你的身体就会被托起来。同样，在任何社会等级制度中，那些对自己没自信的人就会通过虐待他们眼中的下等人来突显自己的身份。我已经意识到，正是因为这个原因，在美国社会中底层白人是对待黑人最残酷的群体。 它就像人生一样，里面无所不包，但又不是事物的真实样子。它只是一个暂时的过程，只要你向前看，你就能超越它，哪怕现在你还是身处其中。 黑客与画家黑客与画家的共同之处，在于他们都是创造者。 创造优美事物的方式往往不是从头做起，而是在现有成果的基础上做一些小小的调整，或者将已有的观点用比较新的方式组合起来。 把整个程序想清楚的时间点，应该是在编写代码的同时，而不是在编写代码之前。 普通黑客和优秀黑客的所有区别中，会不会换位思考可能是最重要的因素。 程序是写出来给人看的，附带能在机器上运行。 不能说的话很多看似叛逆的异端邪说，早就潜伏在我们的思维深处。如果我们暂时关闭自我省查意识，它们就会第一个显现出来。 优秀作品往往来自于其他人忽视的想法，而最被忽视的想法就是那些被禁止的思想观点。 守口如瓶的真正缺点在于，你从此无法享受讨论带来的好处了。讨论一个观点会产生更多的观点，不讨论就什么观点也没有。 所谓流行（传统观念也是一种流行），本质上就是自己看不见自己的样子。 如果自己就是潮水的一部分，怎么能看见潮流的方向呢？你只能永远保持质疑。问自己，什么话是我不能说的？为什么？ 良好的坏习惯在大众眼里，黑客（hacker）就是入侵计算机的人。在程序员眼里，黑客指的是优秀程序员。 只有深入了解当前技术，黑客才能构想下一代技术。 公民自由真的是国家富强的原因，不是结果吗？是的。一个人们拥有言论自由和行动自由的社会，往往最有可能采纳最有优方案，而不是采纳最有权势的人提出的方案。 另一条路如果互联网软件能够击败桌面软件，一定是赢在更方便这一优势上。 对于开发者来说，互联网软件与桌面软件最显著的区别就是，前者不是一个单独的代码块。 设计桌面软件就像设计一幢大楼，而设计互联网软件就像设计一座城市。 早一点发现bug就不容易形成复合式bug。在软件中，复合式bug是最难发现的bug，往往也会导致最大的损失。 互联网软件不仅把开发者与他的代码更紧密地联系在一起了，而且把开发者与他的用户也更加紧密联系在一起了 订阅报纸模式正式互联网软件纯天然的收费模式。 桌面软件迫使用户变成系统管理员，互联网软件则是迫使程序员变成系统管理员。 如何创造财富真正重要的是做出人们需要的东西，而不是加入某个公司。 要致富，需要两样东西：可测量性和可放大性。 成年人在真实世界中的团体，一般来说，都存在某个共同目标。那么领导者通常可以由最善于实现此目标的人承担，而学校里的学生并没有共同目标，但等级关系却不会消失，所以学生们的等级是凭空创造出来的。 关注贫富分化技术在加大收入差距的同时，缩小了大部分的其他差距。 技术使得生产率的差异加速扩大。 防止垃圾邮件的一种方法大多数黑客都是好胜心很强的人。 设计者的品味好设计师简单设计。 好设计永远不是过时的设计。 以永不过时为目标也是一种避开时代风潮的影响的方法。 好设计是解决问题的设计。 好设计是启发性的设计。 好的设计通常是有点趣味性的设计。 好设计是艰苦设计。 好设计是看似容易的设计。 好设计是对称设计。 好设计是模仿大自然的设计。 好设计是一种再设计。 好设计是能够复制的设计。 好设计常常是奇特的设计。 好设计是成批出现的。 好设计常常是大胆的设计。 编程语言解析编程语言的一个重要特点：一个操作需所需的代码越多，就越难避免bug，也越难发现他们。 高级语言还有一个特点，他使得程序更具有可移植性。 用什么语言并不重要，重要的是对问题是否有正确的理解。代码以外的东西才是关键。 一百年后的编程语言编程语言就像生物钟一样，存在一个进化的脉络，许许多多分支最终都会成为进化的死胡同。 提高效率的正确做法是将语言的语义与语言的实现相分离。 应用软件运行速度提升的关键在于有一个好的性能分析器帮助指导程序开发。 拒绝平庸对于应用程序来说，应该选择总体最强大、效率也在可接受范围内的编程语言，否则都不是正确的选择。 书呆子的复仇一般来说，条件越苛刻的项目，强大的编程语言就越能发挥作用。 把软件运行在服务器端就可以没有顾忌地使用最先进的技术。 选择更强大的编程语言会减少所需要的开发人员数量。 衡量语言的编程能力的最简单方法可能就是看代码数量。 格林斯潘第十定律：任何C或Fortran程序复杂到一定程度后，都会包含一个临时开发的、只有一半功能的、不完全符合规格的、到处都是bug的、运行速度很慢的Common Lisp实现。 梦寐以求的编程语言简介性的最重要的方面就是使得语言更加抽象。 简洁性是静态类型语言的力所不及之处 一种真正优秀的编程语言应该既整洁又混乱。整洁的意思是设计得很清楚，内核由数量不多的运算符构成，这些运算符易于理解，每一个都有很完整的独立用途。混乱的意思是它允许黑客以自己的方式使用。 编程时代提供提高代码运行速度的关键是使用好的性能分析器，而不是用其他方法，比如精心选择一种静态类型的编程语言。 人们真正注意到你的时候，不是第一眼看到你站在那里，而是发现过来那么久你竟然还在那里。 将软件内部的接口设计成垂直接口而不是水平接口。 设计与研究最终来说，设计和研究都通向同一个地方，只是前进的路线不同罢了。 低估用户一般来说总是会让设计师出错。 编程语言也是以人为本。 评价一种语言的优劣不能简单地看最后的程序是否表达得漂亮，而要看程序从无到有的那条完成路径是否很漂亮。 为了做出优秀的设计，必须贴近用户，始终寸步不离，永远站在用户的角度调整自己的构想。 原型并不只是模型，不等于将来一定要另起炉灶，完全可以能够在原型的基础上直接做出最后的成品。 先做出原型，再逐步加工做出成品，这种方式有利于鼓舞士气，因为它使得你随时都可以看到工作的成效。]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[人件读书笔记]]></title>
    <url>%2F%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%2F%E4%BA%BA%E4%BB%B6%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[读《人件》这本书，给我最大的感触是人本管理思想，即知识型企业的核心是人，而不是技术的管理理念。 《人件》这本书中并没有涉及具体的编码技术，从几个方面描述对一个团队的管理。 几乎任何一位IT从业人员都能从本书中找到自己或者管理者的影子。 无论是“管理邪恶论”的唯技术论者，还是拥有“小小邪恶心态”的管理者，都能从中找到印证自己观点的“数据”。 对身居要职、困扰于如何从日常琐事中突围，或者是引领（或是被迫引领）创新的人，都会从本书中受到启发。 本书要点人件的想法成于两位大师越洋航班上的一席思考碰撞。 管理与创新的话题一直困扰着高科技行业。前几年的精益创业验证了创新是必须的，因此也在行业里掀起了一股精益创新的潮流 对于一家具有相当历史和规模的公司，究竟该怎样管理才能不阻碍创新，这仍然是一个难题。 两位大师用严谨的科学实验和辛辣尖锐的解剖一步步为我们揭开知识经济下管理中的种种误区。 第一部分：管理人力资源把人当作固定的模块来管理。 引出本书核心观点：人件。进而探索一种迥然不同的思考人及管理人的办法以适应人的分模块化特征。 第二部分：办公环境集中讨论了管理中对办公环境的管理。 可以看出环境对工作的效率影响极大。嘈杂、拥挤、没有隐私的工作环境不仅影响团队效率，还会导致人才流失。想在工作空间上省钱，至少对脑力劳动者是不可能的。对于脑力工作者来说，花费的时间，质量胜于数量。 第三部分：正确的人强调人的重要性。 核心就是找到合适的人，让他们愉快工作不愿离开，让他们自由发挥。一开始找到正确的人至关重要,找对人的同时，留住人也是很重要的。有抱负的公司会有目的性的去打造公司的社区感。 第四部分：高效团队养成建立一个高效团队。 首先什么是高效的团队，一个重要标志就是整体大于部分之和；其次是如何建立这样的团队，作者指出共同目标的重要性，然后转而从反面团队自毁来阐述一个高效团队应该避免的点：防御式管理、官僚主义、物理隔离、牺牲产品质量、伪造截止日期、团伙控制，避免抽象的给出空洞的建议；最后指出在构建这样团队中管理者应该注意的：成功管理的核心是让大家其心协力，然后助推大家到一个连管理者自己都无法让他们停止的点，避免用力过猛。 第五部分：沃土讨论沃土——企业文化的重要性。 应建立一个可以自我愈复的系统，企业应合理控制风险，避免不合理的风险，但是也要承担一定的风险，避免仪式性会议，避免垃圾邮件，建立组织型学习，加强社区的构建以留住人才。 第六部分：快乐地工作提出工作应该是快乐的。 管理者应该自己手下的人从工作中得到快乐，而不是剥夺他们的快乐以一味最大化员工的效率最大化。 精编书摘管理人力资源我们工作中的问题更多属于社会学范畴，而非技术范畴。 大多数管理者坦诚：他们对人的担心更甚于对技术的担心。但他们很少以此种方式去管理。 我们大多数人是从事人类交流的职业。我们的成功源自所有参与者良好的人与人之间的互动，我们失败则归因于这种互动的缺失。 开发的本质完全迥异于生产。 营造一个不容许任何失误的氛围会让大家保持戒心。 对于忙族尊崇生产世界管理风格的管理者来说，员工的独特个性是一种持续的困扰。人性化的管理者却能认识到正是这种独特性使得项目团队产生了化学反应，是团队充满活力与高效的源泉。这是需要培养的。 催化剂很重要，因为项目总是处于不断变化的状态。一个能让项目更稳定的人抵得上两个做事的人。 正所谓磨刀不误砍柴工，我们必须学习如何多花时间在思考上，少花时间在实施上。 现实生活中的管理者如何让人们以牺牲自我生活为代价来承受更大的工作强度和工作时间。 西班牙理论认为世界上的价值总量是定额的，因而财富积累的道路就是学会从大地或者别人的背上攫取。 英国理论则认为，价值是通过智慧和科技创造出来的。 生产效率的定义是收益除以成本。看得见的收益就是通过工作赚取金钱，而成本则是全部的花销，包括替换那些疲惫不堪的工作人员。 压力不会让人工作得更好——只是工作得更快。 我们通常倾向于将我们的自信与生产出的产品质量（并非产品数量）关联。采取任何可能牺牲产品质量的行动都可能挑起员工反对你的情绪。 让买方而不是制造者来设定质量标准，即我们所谓的飞离卓越的航班。 长远来看，以市场为基础制定的质量标准花销更大。 质量，远远不只是最终用户的要求，而是达到高产能的一种方法。 让制造者来设定他们自己满意的质量标准，会带来生产效率的提高，从而抵消为提高质量而产生的额外成本。 帕金森定律：工作会自动膨胀，占满一个人可以用的所有时间。 把团队成员当做帕金森型的员工是不可能奏效的。这只能消磨他们的意志，让他们失去前进的动力。 一个组织的工作如果都忙忙碌碌，就会膨胀以至于占满整个工作日。 软件管理的七个假象 有一个你不知道的新窍门可以让产能飙升 其他管理者正在收货100%、200%乃至更多的增长 技术日新月异，你已经过时啦 改变程序语言会给你带来巨大的提升 因为库存的缘故，你需要马上让产能翻倍 你自动化了其他所有东西。难道不是要你自动化掉你的软件开发人员吗 你的员工在巨大的压力下工作得更好 管理者的作用不是让大家去工作，而是创造环境，让大家可以顺利开展工作 办公环境只要员工还拥挤在嘈杂、低效、干扰不断的环境里，任何除了环境外的改造多事徒劳的 倘若程序员产出呈现10：1的差异是可以理解的，那么软件组织在产出上的10：1 的差异也是存在的 节省成本而造成办公环境达不到标准将会导致工作效率降低，从而抵消掉节省的那点成本。 吉布尔定律：你想要量化的任何东西都能够以某种程度度量，至少聊胜于无 比起采用出勤时间，基于流计算的工作时间计算体系具有如下两大明显优势： 第一，这让大家能够关注流时间的重要性。 第二，这样能够建立起一个有效工作时间的统计 环境参数/E参数：不被打断的小时数 / 出勤时间的小时数 脑力劳动者的工作特点：对于花费的时间，质量胜于数量 员工不太在意环境外表，这一事实经常被误解为他们补关心工作环境的任何属性。 有益工作的环境不是地位的象征，而是一种必需品。 管理要做的最好，就应该保证为大家提供足够的空间、足够的安宁以及足以保护个人隐私的方法，让大家能够创造自己的可工作空间。 当环境中个别的局部需求与总体需求达到完美平衡时，自然或者邮寄的控制就开始显现。在有机的环境里，每一处都是独特的，同时又都是协作的，没有一个组成部分会显得格格不入，而是形成一个统一的整体——能够被这个环境中任何一个人辨识出来的整体。 空间过于封闭或开放，人们都不能有效工作。一个好的工作空间需要找到二者平衡 空间和服务的成本是显而易见的，而它带来的好处（诸如工作效率的提升、以及人员流失率的下降）却没有被仔细衡量，不具有可视性。 正确的人怎么塑造原始的人力资源时管理学的根本。 一开始不适合工作的人，那就永远都不适合。这意味着从一开始找到正确的人至关重要。 这种对整齐划一的要求是部分管理者缺乏安全感的表现。自信的管理者不会关心团队成员是否按时理发或者是否打领带。他们的荣耀系于员工做出的数据。 管理热力学第二定律：组织里的熵总是增加的。 创新依靠领导力，而领导力又需要创新。 一个健康的组织所必需的，是能够为员工经常性地提供独立的自我评价机会。 团队磨合需要时间，而在磨合期内，团队的组成不能轻易改变。如果你使用被动应付的合同工策略，你不可能磨合出一个团队。事实上，你管理的一群员工基本也不可能成为一个团队。 很多人都相信，人才晋升快速的公司是很有行动力的。这很自然，以为年轻员工都有向前发展的冲动。但从企业发展的角度来看，晚提拔是健康的标志。 拥有最低离职率的公司，他们的一个共同点就是广泛的再培训。 重新培训能够帮助组织树立永恒之地的观念，从而形成低离职率和强烈的社区感。 花销是指一笔钱被花掉了。另一方便，投资则是用一种资产去购买另一种资产。价值并没有被使用，只是从一种形式转换成了另一种。当你在支出项中选择投资而不是花销时，你就在对这笔支出进行资本化。 依靠脑力劳动者的公司必须认识到他们在人力资本上的投资是至关重要的。 高效团队养成一个有凝聚力的团队是一组紧密交织在一起的人，他们整体大于个体之和。 团队存在的目标不是达成目标，而是让目标一致。 一些非常重要的因素（钱、地位、升值）在凝聚后变得微不足道，甚至无关紧要了。 有凝聚力的团队通常都有一个很强的自我认知。 有凝聚力的团队对生产出来的产品有强烈的归属感。 一个有凝聚力的工作组可能自傲、自足、让人头疼还有点排外，但对比拼凑起来的可替换部件，它却能帮助管理者实现真正的目标。 大部分形式的团队自毁，其危害来自于贬低工作或者贬低做工作的人。 我们并非是要通过加班来完成工作，而是希望能够在工作根本无法按时完成时通过加班来避免指责。 辅导是成功团队互动的关键因素，它提供了参与者协作和自我提升的机会。 个体的成功时完全建立在集体的成功之上的。 敞开和服的态度恰好是防御式管理的反面。用人不疑，你将他放在这个岗位上，就要信任他，不需要做任何防御。 臭鼬工程指的是项目可以在上层管理不知情的情况下悄悄展开。当底层的员工深信产品的正确性，因而不愿意接受管理层取消项目的决定时，这种项目就会诞生。 对质量的执著追求是催生团队行成的最强催化剂 特别是在团队逐步形成的时候，频繁的闭环是很重要的。团队成员需要建立一种共同成功和共同认可的习惯。这是能够帮助团队提升士气的一种机制。 成功管理的核心是让大家齐心协力，然后助推大家到一个连管理者自己都无法让他们停止的点。 不管精英的特点是什么，它都是形成团队标识的基础，这种标识是具有凝聚力团队的一个本质要素。这里一个重要的限定条件是团队需要在某些方面感到独特，但并非所有方面。 在最棒的团队中，不同的个体是不是会展现出领导力，在自己擅长的专业领域带领大家。没有人是永远的领导者，因为这样的领导者不会是团队中平等的一份子，团队里的互动也会由此瓦解。 团队的结构是一个网络，而非分层结构。 存在一点差异才能极大地帮助我们形成一个有凝聚力的团队。 沃土让系统变为确定性会导致它丧失治愈自身的能力。 事无巨细的文档引入的是问题，而非解决方案。 方法学通过制定法规来强制收敛，这就带来了不可避免的副作用，一边是维法者的强力推进，一边是脑力劳动者强烈的自主意识。 人们在尝试新鲜事物的时候会表现的更好。 风险管理的本质：不是让所有的风险都小时，而是确保风险发生时有相应的应对措施。 会议和演讲都是繁琐的事物，真正的价值体验在那些间隙时间，一段演讲开始或结束等待的公共区、茶歇休息、午餐排队、与其他参会者一起饮茶或聚餐的时间。 会议的目的是达成一致。 双边关系中的一方如果过度表现，另一方就一定会表现不足。 同事之间的自组织和相互协调才是良好团队协作的重要表现。 信但保持怀疑的人才是唯一拥护改变的真正盟友。两个极端，无论是盲目遵从，还是激烈反对，都是真正的敌人。 外来元素的引入会层位改变的催化剂催生改变。这种外来元素可以是一种外部力量，也可以是因为世界的改变所带来的认知。 混乱是改变的必经阶段。 矛盾是，改变只有在容忍失败——至少是一点失败—— 的情况下才有机会成功。 组织型学习的关键问题不在于如何开展学习，而在于在何处开展。 快乐地工作霍桑效应：人们在尝试新颖的东西时，所激发的能量与兴趣可以促进人们生产效率的提高。 在任何一个项目中不要试验超过一种类型的开发技术。 应对家具警察、对付企业熵、挽回团队自毁的趋势，更加关注产品质量、消除帕金森定律、放松正式方法学的限制，提高E参数，敞开和服 社会因素高于技术甚至于金钱。]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[人月神话读书笔记]]></title>
    <url>%2F%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%2F%E4%BA%BA%E6%9C%88%E7%A5%9E%E8%AF%9D%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[《人月神话》在软件领域具有深远影响力，畅销不衰，很少有其他书籍等比得上它。 Brooks博士为人们管理复杂项目提供了最具洞察力的见解，其中既有许多发人深省的观点，又有大量软件工程的实践，适合任何软件开发行业的从业人员阅读，对软件开发工程师、软件项目经理和系统分析师更是必读之作。 虽然0S/360失败了，但是它在开发的过程中解决了很多技术难题。它的开发过程成就了这本《人月神话》。这也让我想明白为什么大家都会觉得在项目实践中我们才可以学到更多。没有项目，你不会去想有什么问题。但是在项目中遇到问题的话，你最好的求助方式是网络和书籍，而且如果在遇到问题的时候能深入研究书籍的话你才会进步的比较快。 书中一再强调文档的重要性。想起我们曾自称“九乡河文档学院”，现在终于明白了为什么本科期间的项目要求写那么多完备的文档出来。 要保证一个项目的进度不被大幅度推迟，制定进度表很重要。 回想起我们本科期间的最大的一次延期，就是参加EL比赛的时候，由于小组内大家都没有提出一个时间节点，导致项目无限延期，最终比赛也就直接退出了。 吃过这个教训后，后来我们再组队完成各项大作业的时候，总是制定了严格的进度表，正常这个进度表会比老师给出的作业的DDL提前一两天，以留出机动时间。有了合格的进度表并遵照执行后，最终我们的各个大作业都取得很好的结果，没有出现临近DDL匆匆忙忙赶工的情形。 现在结合前一年实习的经验，我觉得制定进度表非常重要，而且要求制定者有很强的技术背景，这样才能对碰到的问题和可能花掉的时间做出更准确的估计。 本书要点这本书的内容来自作者在IBM公司担任System/360计算机系列以及其庞大的软件系统OS/360项目经理时的实践经验。这个项目堪称软件开发项目管理的典范。 大型编程项目深受由于人力划分产生的管理问题的困扰，保持产品本身的概念完整性是一个至关重要的需求。本书探索了达成一致性的困难和解决的方法，并探讨了软件工程管理的其他方面。 全书着重回答了对 Tom Watson关于为什么编程难以管理的探索性问题。 大部分内容都是涉及到团队，人和沟通。对于大型软件工程项目，强调人的重要性。在开篇讲开发人员的职业乐趣，后面又通过巴比塔的沟通重要性，在外科手术队伍中的组件和分工。这些都是涉及到团队中人和交互，只有一个有了积极心态和热情的沟通团队，才可能成就一个伟大的团队。从最后的没有银弹，再次肯定开发工作是一种高智力的脑力工作。 精编书摘第一章 焦油坑编程为什么会有趣？作为回报，它的从业者期望得到什么样的快乐？ 首先，这种快乐是一种创建事物的纯粹快乐。 其次，这种快乐来自于开发对他人有用的东西。 第三，快乐来自于整个过程中体现出的一股强大的魅力。 第四，这种快乐是持续学习的快乐，它来自这项工作的非重复特性。 最后，这种快乐还来自于在易于驾驭的介质上工作。 职业的苦恼 首先，苦恼来自追求完美。 其次，苦恼来自由他人来设定目标、供给资源和提供信息。 下一个苦恼——概念性设计是有趣的，但寻找琐碎的bug却只是一项重复性的活动。 最后一个苦恼，有时也是一种无奈——当投入了大量辛苦的劳动，产品在即将完成或者终于完成的时候，却显得陈旧过时。 编程，是一个许多人痛苦挣扎的焦油坑以及一种乐趣和苦恼共存的创造性活动。 第二章 人月神话在众多软件项目中，缺乏合理的进度安排是造成项目滞后的最主要原因，它比其他所有因素加起来的影响还要大。 没有计划，是不行的，任务会一拖再拖，导致最后的延期。这不单单指软件开发，还是有个人日常生活中的事。 所有的编程人员都是乐观主义者，第一错误假设便是“一切都将运行良好”、“这次它肯定会运行”、“我刚刚找到出了最后一个bug”。 编程人员通过非常纯粹的思维活动——概念以及灵活的表现形式来开发程序，期待在实现过程中不会碰到困难。这也就造成了乐观主义的弥漫。 单个任务中的“一切都将运转正常”的假设在进度上具有可实现性。 然而大型的编程工作，或多或少包含了许多任务，某些任务时间还具有前后的次序，一切正常的概率变得非常小，甚至接近于零。 在估计和进度安排中，混淆了工作量和项目进展。人月是危险和带有欺骗性的神话，因为它暗示了人员数量和时间是可以相互替换的。 一方面，部分任务由于次序上的限制不能分解，人手的添加对进度没有帮助。 另一方面，即使是可分解的任务，子任务间需要相互沟通和交流，增加了培训和相互交流的成本。 关于进度安排，我的经验是为1/3计划、1/6编码、1/4构件测试以及1/4系统测试。 Brook法则：向进度落后的项目中增加人手，只会使进度更加落后。 第三章 外科手术队伍需要协作沟通的人员数量影响着开发成本，因为成本的主要组成部分是相互的沟通和交流，以及更正沟通不当所引起的不良结果（系统调试）。 小型、精干队伍是最好的——尽可能的少 小型、精干队伍概念上存在着一定的问题：对于真正意义上的大型系统，它太慢了。 整个系统必须具备概念上的完整性，要有一个系统架构师从上至下地进行所有的设计。要使工作易于管理，必须清晰地划分体系结设计和实现之间的界线，系统架构师必须一丝不苟地专注于体系结构。 第四章 贵族专制、民主政治和系统设计在系统设计中，概念完整性应该是最重要的考虑因素。也就是说，为了反映一系列连贯的设计思路，宁可省略一些不规则的特性和改进，也不提倡独立和无法整合的系统，哪怕它们其实包含着许多很好的设计。 以易用性作为目标，功能与理解上复杂程度的比值才是系统设计的最终测试标准。单是功能本身或者简洁都无法成为一个好的设计评判标准。 外部的体系结构规定实际上是增强，而不是限制实现小组的创造性。 整个创造性的活动包括了三个独立的阶段：体系结构（architecture）、设计实现（implementation）和物理实现（realization）。是在实际情况中，它们往往可以同时开始和并发地进行。 第五章 画蛇添足尽早交流和持续沟通能使结构师有较好的成本意识，以及使开发人员获得对设计的信心，并且不会混淆各自的责任分工。 结构师如何成功地影响实现： 牢记是开发人员承担创造性和发明性的实现责任，所以结构师只能建议，而不能支配 时刻准备着为所指定的说明建议一种实现的方法，同样准备接受其他任何能达到目标的方法 对上述的建议保持低调和不公开 准备放弃坚持所作的修改意见 第六章 贯彻执行规格说明作者应该追求的精确程度：在仔细定义规定什么的同时，定义未规定什么。 项目经理最好的朋友就是他每天要面对的对手——独立的产品测试机构/小组 第七章 为什么巴比伦塔会失败巴比伦塔失败是因为缺乏交流以及交流的结果——组织。 因为左手不知道右手在做什么，所以进度灾难、功能的不合理和系统缺陷纷纷出现。 尤其是当多个团队负责一个项目的时候，这种情况最容易出现。但是太多的交流（比如开会），也是在浪费时间，更加不能不得到正确的策略。 在文件中，记录修订日期记录和标记变更标识条。每日维护的变更小结以“后进先出（LIFO）”的方式保存，在一个固定的地方提供访问。 值得注意的是，工作手册本身没有发生变化。它还是所有项目文档的集合，根据某种经过细致设计的规则组织在一。唯一发生改变的地方是分发机制和查询方法。 巴比伦塔可能是第一个工程上的彻底失败，但它不是最后一个。交流和交流的结果——组织，是成功的关键。交流和组织的技能需要管理者仔细考虑，相关经验的积累和能力的提高同软件技术本身一样重要。 第八章 胸有成竹仅仅通过对编码部分的估计，然后应用任务其他部分的相应系数，是无法得到对整个任务的估计的。 构建独立小型程序的数据不适用于编程系统产品。 工作量是规模的幂函数 对常用的编程语句而言，生产率似乎是固定的。这个固定的生产率包括了编程中需要的注释，并可能存在错误的情况。 使用适当的高级语言，编程的生产效率可以提高5倍。 高级语言确实更容易实现和表达人的思维。 第九章 削足适履没有人可以在自始至终提倡更紧密的软硬件设计集成的同时，又仅仅就规模本身对软件系统提出批评。 仅对核心程序设定规模目标是不够的，必须把所有方面的规模都编入预算。 在指明模块有多大的同时，确切定义模块的功能。 在大型团队中，每个团队成员都倾向于局部优化自己的程序，而不考虑对用户的整体影响。 在整个实现的过程期间，系统结构师必须保持持续的警觉，确保连贯的系统完整性。 在这种监督机制之外，是实现人员自身的态度问题。培养开发人员从系统整体出发、面向用户的态度是软件编程管理人员最重要的职能。 项目经理可以做两件事来帮助他的团队去的良好的空间-时间折衷。 一是确保他们在编程技能上得到培训，而不仅仅是依赖他们自己的才能和先前的经验。 另一种方法是认识到编程需要技术积累，需要开发很多公共单元构件。 精湛的技艺出自创造，精炼、充分和快速的程序也是如此。技艺改进的结果往往是战略上的突破，而不仅仅是技巧上的提高。更普遍的是，战略上突破常来自数字局或表的重新表达——这是程序核心所在。 第十章 提纲挈领每份文档的准备工作是集中考虑，并使各种讨论意见明朗化的主要时刻。不过不这样，项目往往会处于无休止的混乱状态中。文档的跟踪维护是项目监督和预警的机制。文档本身可以作为检查列表、状态控制，也可以作为汇报的数据基础。 为什么要有正式的文档？ 首先，书面记录决策是必要的。只有记录下来，分歧才会明朗，矛盾才会突出。 第二，文档能够作为同其他人的沟通渠道。 最后，项目经理的文档可以作为数据基础和检查列表。 项目经理的基本职责是使每个人都向着相同的方向前进。他的主要工作是沟通，而不是做出决定。 第十一章 未雨绸缪在软件开发的过程中，往往第一个系统存在的问题挺多，它可能太慢、太大，而且难以使用，或者三者兼而有之。要将诶绝所有的问题，除了重新开始意外，没有其他的办法。系统的丢弃和重新设计可以一步完成，也可以一块块地实现。这是所有大型系统开发必须完成的步骤。 为舍弃而计划，无论如何，你一定要这样做。 变化是与生俱来的，不是不合时宜和令人生厌的异常情况。 用户的实际需要和用户感觉会随着程序的构建、测试和使用而变化。 软件产品易于掌握的特性和不可见性，导致它的构建人员面临永恒的需求变更。 抛弃原型概念本身就是对事实的接受——随着学习的过程更改设计。 对于一个广泛使用的程序，其维护总成本通常是开发成本的40%或更多。 程序维护中的一个基本问题是——缺陷修复总会以固定（20%~50%）的几率引入新的bug。整个过程是前进两步，后退一步。 系统软件开发是减少混乱度（减少熵）的过程，所以他本身是处于亚稳态的。 软件维护是提高混乱度（增加熵）的过程，即使是最熟练的软件维护工作们也只是放缓了系统退化到非稳态的进程。 第十二章 干将莫邪项目经理应该制定一套策略，并为通用工具的开发分配资源。与此同时，他还必须意识到对专业工具的需求，对这类工具的开发不能吝啬人力和物力。 使用高级语言的主要原因是生产率和调试速度：存在更少的bug，而且更容易查找。 在某些应用上，批处理系统绝不会被交互式系统所取代。 调试是系统编程中很慢和较困难的部分，而漫长的调试周转时间是调试的祸根。 第十三章 整体部分产品的概念完整性在使它易于使用的同时，也使开发更容易进行，而且bug更不容易产生。 关键的工作是产品定义。 许许多都的失败完全是因为那些产品未精确定义的地方而导致的。 细致的功能定义、仔细的规格说明、规范化的功能描述说明以及这些方法的实施，大大减少了系统中必须查找的bug数量。 在编写任何代码之前，规格说明必须提交给外部测试小组，以详细地检查说明的完整性和明确性。 好的自上而下设计从几个方面避免了bug. 首先，清晰的结构和表达方式更容易对需求和模块功能进行精确地描述。 其次，模块切割和模块独立性避免了系统级的bug。 第三，细节的抑制使结构上的缺陷更加容易识别。 第四，设计在每个精化步骤上都是可以测试的，所以测试可以今早开始，并且每个步骤的重点可以放在合适的级别上。 当遇到一些意想不到的问题时，按部就班的流程并不意味着步骤不能逆转，直到推翻顶层设计，重新开始整个过程。 关键的地方和构建无bug程序的核心，是把系统的结构作为控制结构来考虑，而不是独立的分支语句。 在每次调试会话中，第一次交互取得的工作进展是后续交互的3倍。 软件系统开发过程中出乎意料的困难部分是系统集成测试。系统调试花费的时间会比预料的更长。 直到下一次系统构件的定期发布之前都一直使用快速补丁；而在当前的发布中，把已经通过测试并进行了文档化的修补措施整合到系统平台中。 第十四章 祸起萧墙一天一天的进度落后是难以识别、不容易防范和难以弥补的。 进度落后往往像温水煮青蛙一样让我们难以应付，最重要的就是要防微杜渐。 重大灾害是比较容易处理的，它往往和重大的压力、彻底的重组、新技术的出现有关，整个项目组通常可以应付自如。 但是一天一天的进度落后是难以识别、不容易防范和难以弥补的。 进度表上的每一件事被称为“里程碑”，它们都有一个一个日期。里程碑的选择只有一个原则，那就是里程碑必须是具体的、特定的、可度量的事件，能够进行清洗定义。 里程碑边界明显和没有歧义，比它容易被老板核实更为重要。如果里程碑定义非常明确，程序员不会就里程碑的进展弄虚作假。 慢性进度偏离同样也是士气杀手。一方面，时间拖久了，人心疲倦。另一方面，还要考虑当前系统是否已经过时了。 进取对优秀的软件开发软对而言是非常重要的。进取提供了缓冲和储备，使开发队伍能够处理常规的灾祸，可以预计和防止小的灾祸。 必须关心每一天的之后，它们是大灾祸的基本组成元素。 状态的获取是困难的，一线经理有充分的理由不提供信息共享。 不论协作与否，拥有能了解状态真相的评审机制是必要的，频繁、明确的里程碑是这种评审的基础，完成文档是关键。 第十五章 另外一面对软件编程产品来说，程序向用户所呈现的和提供给机器识别的内容同样重要。 即使是完全开发给自己使用的程序，描述性文字也是必要的。因为记忆衰退的规律会使用户-作者逐渐失去对程序的了解，于是他们不得不重拾自己劳动的各个字节。在编码过程中，恰当的注释是非常重要的。 什么样的文档才是好文档： 目的：主要的功能是什么？开发程序的原因是什么？ 环境：程序运行在什么样的机器、硬件配置和操作系统上 范围：输入的有效范围是什么？允许显示的合法输出范围是什么？ 实现功能和使用的算法：精确地阐述它做了什么。 输入-输出格式：必须是确切和完整的。 操作指令：包括控制台及输出内容中正常和异常的结束行为。 选项：用户的功能选项有哪些？如何在选项之间进行挑选？ 运行时间：在指定的配置下，解决特定规模问题所需要的时间？ 精度和校验：期望结果的精确程度？如何进行校验？ 自文档化的程序：将文档整合到源程序，这对正确维护是直接有力的推动，保证编程用户能方便、及时地得到文档资料。 将文档的负担降到最小的三个方法 第一是借助那些出于语言的要求而必须存在的语句，来附加尽可能多的文档信息。 第二是尽可能地使用空格和一致的格式提高程序的可读性，表现从属和嵌套关系。 第三，以段落注释的形式，向程序中插入必要的记叙性文字。 第十六章 没有银弹所有的软件活动包括根本任务——打造构成抽象软件实体的复杂概念结构，次要任务——使用编程语言表达这些抽象实体，在空间和实践限制内将它们映射成机器语言。 除非次要任务占了所有工作的9/10，否则即使全部次要任务的时间缩减到零，也不会带来生产率数量级上的提高。 没有任何技术或管理上的进展，能独立地许诺在生产率、可靠性或简洁性上取得数量级的提升。 软件开发中困难的部分是规格说明、设计和测试这些概念上的结构，而不是对概念进行表达和对实现逼真程度进行验证。 现在软件系统中无法规避的内在特性：复杂度、一致性、可变性和不可见性。 软件的复杂度是根本属性，不是次要因素。因此，抽掉复杂度的软件实体描述常常也去掉了一些本质属性。 复杂度不仅仅导致技术上的困难，还引发了很多管理上的问题。它使全面理解问题变 得困难，从而妨碍了概念上的完整性；它使所有离散出口难以寻找和控制；它引起了大量学习和理解上的负担，使开发慢慢演变成了一场灾难。 构建软件最可能的彻底解决方案是不开发任何软件 现在有很多快速开发平台，但是真正能够不写代码就完成业务功能的开发平台基本上没有成功的。特别是在业务场景比较复杂情况下，编程自动化基本是不可能的事情。唯一看到有所突破的是关于统一框架和技术平台等方面的建设，在原有的框架基础上我们来构建一个产品开发平台，将跟业务关系不大的权限模型，工作流引擎等集成进去，将常用的可复用组件集成进去，加快开发速度。 不要在追求自动编程平台上下功夫，可以在加强组件复用和技术平台建设上下功夫。 要多从开发模式的改进上来解决没有银弹所提出的各种实际问题，虽然不能够彻底解决，但是可以通过努力来改进。比如增量迭代的开发模型，快速原型法，测试驱动，高级语言和图形化编程等。 开发软件系统的过程中，最困难的部分是确切地决定搭建什么样的系统。概念性工作中，没有其他任何一个部分比确定详细的技术需求更加困难。 软件开发人员为客户所承担的最重要的职能是不断重复地抽取和细化产品的需求。事实上，客户不知道他们自己需要什么。 关键的问题是如何提高软件行业的核心，一如既往的是——人员。 软件开发是一个创造性的过程。 第十七章 再论《没有银弹》《没有银弹》无可争辩地指出，如果开发的次要部分少于整个工作的9/10，即使不占用任何时间（需要出现奇迹），也不会给生产效率带来数量级的提高。因此，必须着手解决开发的根本问题。 重用和交互的构件开发是解决软件根本困难的一种方法。 复杂性是层次化的。例如，复杂性是最严重的内在困难，单并不是所有的复杂性都是不可避免的。 系统复杂性是无数细节的函数，这些细节必须精确而且详细地说明——或者是借助某种通用规则，或是逐一阐述，但绝不仅仅是统计说明。 系统化软件开发方法的发展是为了解决质量问题（特别是避免大型的灾难），而不是出于生产率方面的考虑。 定制软件包的开发，基本上今天的开源社区已经这么做了。大量的软件包被开发出来，确实提高了通用需求的开发效率。Brooks也承认，他低估了软件包客户化的程度和它的重要性。 面向对象技术不会加快首次或第二次的开发，产品族中第五个项目的开发见鬼异乎寻常的迅速。 极度的前期投入和收益的推后是使OO技术应用迟缓的最大原因。 解决软件构建根本困的最佳方法是不进行任何开发。软件包只是实现上述目标的方法之一，另外的方法是程序重用。 重用是一件说起来容易，做起来难的事情。它同时需要良好的设计和文档。即使我们看来非常罕见的优秀设计，但如果没有好的文档，我们也不会看到能重用的构件。 软件重用的另一个问题是学习成本。越复杂的功能，学习成本越高。高级语言比机器语言强大，但是也更加复杂。而需要重用一个模块，则需要学习相应模块的成本。这种成本今天已经在各类专门开发职业体现出来，如后台程序员、web前端或手机客户端，不同类别的程序员差别就在于其对某一重用模块的专门知识的掌握。 第十九章 20年后的《人月神话》一个整洁、优雅的编程产品必须向它的每位用户提供一个条理分明的模型概念，这个模型描述了应用、实现应用的方法以及用来指明操作和各种参数的用户界面使用策略。用户所感受到的产品概念完整性是易用性中最重要的因素。 将体系结构和设计实现、物理实现相分离。 概念完整性是产品质量的核心。拥有一位结构师是迈向概念完整性最重要的一步。 为了得到完整、明确和共享的用户描述，结构师应该猜测，或者假设一系列完整的属性和频率值。 为用户群的属性明确地记载各种猜测。清晰和错误都比模糊不清好得多。 瀑布模型的基本谬误是它假设项目只经历一次过程，而且体系结构出色并易于使用，设计是合理可靠的，随着测试的进行，编码实现时可以修改和调整的。换句话说，瀑布模型假设所有错误发生在编码实现阶段，因此它们的修复可以很顺畅地穿插在单元和系统测试中。 瀑布模型的第二个谬误是它假设整个系统一次性地被构建，在所有的设计、大部分编码、部分单元测试完成之后，才为闭环的系统测试合并各个部分。 在把任何东西编程代码前，可能要往复迭代两个或更多的体系结构-设计-实现循环。 原型，仅仅反映了概念模型准备过程中所做的设计决策的一个程序版本，它并未反映受显示考虑所驱使的设计决策。 信息隐藏——现在常常内建于面向对象的编程中——是唯一提高软件设计水平的途径。 人力（人）和时间（月）之间的平衡远不是线性关系，使用人月作为生产率的衡量标准实际是一个神话。 对人月神话实际研究发现，向进度落后的项目中添加人手会增加项目的成本，但是不一定会使项目更加落后。如果在项目早期添加额外的人比在后期添加额外的人更安全些。 人就是一切。 公司通过将权利下放到具体的团队，事实上使得组织机构变得更加融洽和繁荣。 彻底提高软件健壮性和生产率的唯一途径，是提升一个级别，使用模块或者对象组合来进行程序的开发。 软件系统可能是人类创造中最错综复杂的事物，只能期待人们在力所能及的或者刚刚超越力所能及的范围内进行探索和尝试。这个复杂的行业需要：进行持续的发展；学习使用更大的要素来开发；新工具的最佳使用；经论证的工程管理方法的最佳应用；良好判断的自由发挥以及能够使我们认识到自己不足和容易犯错的谦卑。 作者简介Frederick P.Brooks,Jr.曾获美国计算机领域最具声望的图灵奖桂冠。美国计算机协会（ACM）称赞他“对计算机体系结构、操作系统和软件工程作出了里程碑式的贡献”。 Brooks博士是北卡罗莱纳大学KENAN-FLAGLER商学院的计算机科学教授。他被认为是“IBM 360系统之父”，曾担任360系统的项目经理，以及360系统项目设计阶段的经理。凭借在此项目中的接触贡献，他与Bob Evans和Erich Bloch在1985年荣获了美国国家技术奖。]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建临时邮箱]]></title>
    <url>%2F%E5%A5%87%E6%8A%80%E6%B7%AB%E5%B7%A7%2F%E6%90%AD%E5%BB%BA%E4%B8%B4%E6%97%B6%E9%82%AE%E7%AE%B1%2F</url>
    <content type="text"><![CDATA[今天来介绍一个临时搭建的即用即毁的邮箱系统forsaken-mail。当我们不想使用自己的真实邮箱，或者想批量注册账号的时候，就可以很方便的用上了 系统会自动分配随机邮箱，当有新邮件时页面会自动刷新，页面关闭后邮件即丢失，适合临时使用 本文的邮箱系统搭建在搬瓦工VPS上 源码原作者开源地址。源码是nodejs编写。部署非常简单！ Linux安装部署服务器上需要事先安装git、nodejs等环境 安装步骤 从github上clone项目：git clone https://github.com/denghongcai/forsaken-mail.git 进入项目目录下，安装依赖npm install 启动项目npm start 这时通过ip地址+端口号3000就能访问页面了 开机自启123456789#安装pm2工具npm install -g pm2#启动项目pm2 start bin/www#设置开机启动（可选）pm2 startuppm2 save Nginx域名转发除了通过IP地址访问，还可以自定义邮箱域名 首先需要在域名解析中添加如下记录 添加域名：A记录指向到服务器ip地址 添加域名：MX记录指向到服务器ip地址，优先级设置10 这时使用http://域名:3000/访问就可以看到页面了。但是带着端口访问，强迫症患者总是觉得不舒服，此时就需要配置Nginx转发了。下面是我的配置文件12345678910111213server&#123; listen 80; server_name mail.frees.gq ; location / &#123; proxy_set_header HOST $host; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://localhost:3000/; &#125;&#125; nginx -s reload使之生效。这样就可以使用http://mail.frees.gq来访问了。开始愉快地玩耍吧~ 注意事项 如果提示Error: listen EADDRINUSE 0.0.0.0:25，需要事先关闭服务器上的sentmail服务service sendmail stop # 关闭sendmail服务 chkconfig sendmail off # 关闭sendmail自启动 除了本站搭建的邮箱服务器，此外我还发现了另外一个免费的在线邮箱网站：Guerrilla 邮箱 - 一次性临时邮箱]]></content>
      <categories>
        <category>奇技淫巧</category>
      </categories>
      <tags>
        <tag>VPS</tag>
        <tag>邮箱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[给自己的网站加上HTTPS]]></title>
    <url>%2FHexo%E5%8D%9A%E5%AE%A2%2F%E7%BB%99%E8%87%AA%E5%B7%B1%E7%9A%84%E7%BD%91%E7%AB%99%E5%8A%A0%E4%B8%8AHTTPS%2F</url>
    <content type="text"><![CDATA[这个纯属偶然看到一篇博文里面的，觉得https很好高大上，于是我也想玩玩 Let’s Encrypt是一个免费、自动化、开放的证书签发服务。它由ISRG（Internet Security Research Group，互联网安全研究小组）提供服务，而ISRG是来自于美国加利福尼亚州的一个公益组织。Let&#39;s Encrypt得到了Mozilla、Cisco、Akamai、Electronic Frontier Foundation和Chrome等众多公司和机构的支持，发展十分迅猛。 申请Let&#39;s Encrypt证书不但免费，还非常简单，虽然每次只有90天的有效期，但可以通过脚本定期更新，配好之后一劳永逸。本文记录本站申请过程和遇到的问题。 获取免费证书Certbot是Let&#39;s Encrypt官方推荐的获取证书的客户端，可以帮我们获取免费的Let&#39;s Encrypt证书。Certbot是支持所有Unix内核的操作系统的，我的博客服务器系统是CentOS，这篇教程也是通过在个人博客上启用HTTPS的基础上完成的。 安装Certbot客户端 wget https://dl.eff.org/certbot-auto chmod +x certbot-auto 获取证书 webroot模式 certbot certonly --webroot -w /var/www/example -d example.com -d www.example.com 这个命令会为example.com和www.example.com这两个域名生成一个证书，使用--webroot模式会在/var/www/example中创建.well-known文件夹，这个文件夹里面包含了一些验证文件，certbot会通过访问example.com/.well-known/acme-challenge来验证你的域名是否绑定的这个服务器。这个命令在大多数情况下都可以满足需求 standalone模式 但是，有时候我们没有根目录，例如一些微服务和本博客。这时候使用--webroot就走不通了。certbot还有另外一种模式--standalone，这种模式不需要指定网站根目录，他会自动启用服务器的443端口，来验证域名的归属。我们有其他服务（例如nginx）占用了443端口，就必须先停止这些服务，在证书生成完毕后，再启用。 certbot certonly --standalone -d example.com -d www.example.com 证书生成完毕后，我们可以在/etc/letsencrypt/live/目录下看到对应域名的文件夹，里面存放了指向证书的一些快捷方式。 Nginx配置启用HTTPS客系统使用的是Nginx 服务器来转发请求，这里贴一下我的Nginx配置。12345678910111213141516171819202122232425262728293031server&#123; listen 443; server_name blog.winsky.wang ; ssl on; ssl_certificate /etc/letsencrypt/live/blog.winsky.wang/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/blog.winsky.wang/privkey.pem; location / &#123; proxy_set_header HOST $host; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://localhost:4000/; &#125;&#125;server&#123; listen 80; server_name blog.winsky.wang ; location / &#123; proxy_set_header HOST $host; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://localhost:4000/; &#125; # return 301 https://$server_name$request_uri; #强制跳转到Https&#125; 最新的Nginx配置文件可以查看Hexo自动部署优化——解决TTFB过长的问题 自动更新SSL证书Let&#39;s Encrypt提供的证书只有90天的有效期，我们必须在证书到期之前，重新获取这些证书，certbot给我们提供了一个很方便的命令，那就是certbot renew。通过这个命令，他会自动检查系统内的证书，并且自动更新这些证书。 我们可以运行这个命令测试一下certbot renew --dry-run。不过，运行时候出现了错误123Attempting to renew cert (blog.winsky.wang) from /etc/letsencrypt/renewal/blog.winsky.wang.conf produced an unexpected error: Problem binding to port 80: Could not bind to IPv4 or IPv6.. Skipping.All renewal attempts failed. The following certs could not be renewed: /etc/letsencrypt/live/blog.winsky.wang/fullchain.pem (failure) 这是因为生成证书的时候使用的是--standalone模式，验证域名的时候，需要启用443端口，这个错误的意思就是要启用的端口已经被占用了。这时候必须把nginx先关掉，才可以成功。果然，先运行service nginx stop运行这个命令，就没有报错了，所有的证书都刷新成功 证书是90天才过期，我们只需要在过期之前执行更新操作就可以了。 这件事情就可以直接交给定时任务来完成。 新建了一个文件/home/certbot/certbot_auto_renew.sh123SHELL=/bin/bashPATH=/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin/bin/certbot renew --pre-hook &quot;/sbin/service nginx stop&quot; --post-hook &quot;/sbin/service nginx start&quot; --pre-hook这个参数表示执行更新操作之前要做的事情 --post-hook这个参数表示执行更新操作完成后要做的事情 2019-06-08更新，加上path参数避免定时任务找不到命令 使用chmod u+x /home/certbot/certbot_auto_renew.sh赋予权限 最后我们添加一个定时任务crontab -e然后添加下面这行，每天凌晨4:00执行更新操作10 4 * * * /home/certbot/certbot_auto_renew.sh 填坑虽然上面的过程看上去一帆风顺，但是实际操作过程中还是碰到了很多问题 Python版本我的服务器是CentOS 6.6的，上面默认的Python版本是2.6.6，安装时会报错No supported Python package available to install. Aborting bootstrap!。所以需要升级一下Python版本12345678910111213wget http://python.org/ftp/python/2.7.3/Python-2.7.3.tar.bz2 #下载Python-2.7.3tar -jxvf Python-2.7.3.tar.bz2 #解压cd Python-2.7.3 #更改工作目录./configure #安装make all #安装make install #安装make clean #安装make distclean #安装/usr/local/bin/python2.7 -V #查看版本信息mv /usr/bin/python /usr/bin/python2.6.6 #建立软连接，使系统默认的 python指向 python2.7ln -s /usr/local/bin/python2.7 /usr/bin/python #建立软连接，使系统默认的 python指向 python2.7python -V #检验Python 版本vi /usr/bin/yum #将文件头部的#!/usr/bin/python 改成#!/usr/bin/python2.6.6 端口占用在执行certbot certonly --standalone -d blog.winsky.wang时会提示端口占用，这是因为服务器上Nginx服务开着，占用了端口，所以在安装、更新证书的时候需要先停止Nginx服务 Let’s Encrypt 使用教程，免费的SSL证书，让你的网站拥抱 HTTPS 搬瓦工VPS申请Let’s Encrypt免费SSL证书时报错解决办法 遗留问题2018-02-12更新百度的自动推送代码，虽然提供了https的接口，但是https请求回来的代码仍然是去请求http，导致不能正确baidu_push]]></content>
      <categories>
        <category>Hexo博客</category>
      </categories>
      <tags>
        <tag>HTTPS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[博客自启动]]></title>
    <url>%2FHexo%E5%8D%9A%E5%AE%A2%2F%E5%8D%9A%E5%AE%A2%E8%87%AA%E5%90%AF%E5%8A%A8%2F</url>
    <content type="text"><![CDATA[折腾到现在，整个博客终于快弄好了。搭建教程参照：快速搭建自己的个人博客，主题美化，自动将更新部署到VPS 但是，当服务器重启后，博客不能自启动，每次需要手动登录服务器来启动服务，这也很麻烦啊（摊手） 今天就来介绍一下如何配置博客的自启动 自启动脚本编写自启动脚本auto_start.sh123456789101112131415source /etc/profilesource ~/bash_profilecd /home/blog# webhook服务启动/sbin/runuser -l root -c &quot;pm2 start /home/blog/webhooks.js&quot;# 改为静态部署之后，不需要在自启动脚本中启动# 生成静态内容# hexo generate# 启动博客# hexo s &amp; 执行脚本我使用的服务器版本是CentOS，其他版本的Linux应该也大同小异 编辑/etc/rc.d/rc.local，添加刚才的脚本：/home/blog/auto_start.sh 2&gt;&amp;1 &gt; /dev/null &amp;。注意还要查看一下/etc/rc.d/rc.local文件的执行权限，如果没有需要赋予执行权限]]></content>
      <categories>
        <category>Hexo博客</category>
      </categories>
      <tags>
        <tag>HEXO</tag>
        <tag>博客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自动将更新部署到VPS]]></title>
    <url>%2FHexo%E5%8D%9A%E5%AE%A2%2F%E8%87%AA%E5%8A%A8%E5%B0%86%E6%9B%B4%E6%96%B0%E9%83%A8%E7%BD%B2%E5%88%B0VPS%2F</url>
    <content type="text"><![CDATA[博客雏形搭建好了之后，又浪了几天没有折腾。昨天晚上自己买了个域名，想着是时候好好把博客完善起来了。下面继续开始折腾… 前面我们介绍了如何在搬瓦工VPS上快速搭建自己的个人博客，做了相关的主题美化，同时也进行了简单的SEO优化。 在这个过程中，每次需要更新，我都是直接手动将文件拖到VPS上的。作为一个“懒人”，这个过程还是太烦了。今天我们就来学习一下如何自动将博客部署到VPS上。 2019-01-05 更新 推荐直接在VPS上部署静态页面，访问速度更快 修改参照Hexo自动部署优化————解决TTFB过长的问题 这篇文章重点介绍如何通过git webhooks实现远程vps的自动部署 具体流程：先在本机搭建好hexo环境，push到git仓库，再部署到服务器上 本地安配置hexo环境在本地用hexo搭建一个个人博客很简单，分分钟可以搞定。如果以前没有接触过，可以参考我前面的博文：个人博客Hexo搭建 提交到远程仓库首先需要一个在线的仓库，可以选择github或者Coding.net。这里我选择了常用的github 先在github上创建一个项目HexoBlog，并拷贝仓库ssh地址（使用ssh需要配置ssh公钥和私钥，如果不会配可以google一下或使用http地址）。注意，如果需要通过webhooks实现服务器自动化部署，推荐使用ssh会更方便一些 然后在本地hexo目录初始化本地仓库并提交到github12345git initgit add .git commit -m &quot;first commit&quot;git remote add origin git@github.com:winsky94/HexoBlog.gitgit push -u origin master 注意，如果以前没有配置github的SSH提交，可以参考这篇博文GitHub的SSH提交配置 VPS配置我使用的是搬瓦工VPS。服务器上安装好了nodejs,git,nginx，具体不会的可以谷歌一下 将代码从github上拉取下来同样，这里也需要在服务器上配置github的SSH登录。参考GitHub的SSH提交配置1234mkdir /home/bloggit initgit remote add origin git@github.com:winsky94/HexoBlog.gitgit pull origin master 安装hexo模块123cd /home/blognpm install hexo-cli -gnpm install hexo静态编译1hexo g 这一步会在/home/blog目录下生成一个public目录，这里面就是编译后的静态文件目录，其实这时候直接访问里面的html文件即可看到完整的效果了，只不过还需要一个服务来运行它 配置nginx进入nginx服务配置文件目录/usr/local/nginx/conf/vhost，新建一个配置文件blog.conf，内容为12345678910111213server&#123; listen 80; server_name blog.winsky.wang ; location / &#123; proxy_set_header HOST $host; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://localhost:4000/; &#125;&#125; 重载nginx，使配置生效nginx -s reload。然后就可以通过http://blog.winsky.wang来访问博客了 Git WebHooks 自动化部署是不是觉得每次写完文章还要登录服务器去执行一次git pull很麻烦？最起码对我这个“懒人”来说，这样很耗时啊 幸运的是，git有很多钩子，可以在仓库发生变化的时候触发，类似js中的事件。WebHooks就是在你本地执行git push的时候，远程仓库会检测到仓库的变化，并发送一个请求到我们配置好的WebHooks 实现WebHooks自动化部署的推荐条件： 服务器端配置ssh认证 服务器端配置nodejs服务，接收github发来的请求 服务器webhook配置由于hexo是基于NodeJS的，所以这里用NodeJS来接收github的push事件。安装依赖库github-webhook-handler：1npm install github-webhook-handler 安装完成之后配置webhooks.js123456789101112131415161718192021222324252627282930var http = require(&apos;http&apos;)var createHandler = require(&apos;github-webhook-handler&apos;)var handler = createHandler(&#123; path: &apos;/webhooks_push&apos;, secret: &apos;winsky_nju&apos; &#125;)// 上面的 secret 保持和 GitHub 后台设置的一致function run_cmd(cmd, args, callback) &#123; var spawn = require(&apos;child_process&apos;).spawn; var child = spawn(cmd, args); var resp = &quot;&quot;; child.stdout.on(&apos;data&apos;, function(buffer) &#123; resp += buffer.toString(); &#125;); child.stdout.on(&apos;end&apos;, function() &#123; callback (resp) &#125;);&#125;handler.on(&apos;error&apos;, function (err) &#123; console.error(&apos;Error:&apos;, err.message)&#125;)handler.on(&apos;push&apos;, function (event) &#123; console.log(&apos;Received a push event for %s to %s&apos;, event.payload.repository.name, event.payload.ref); run_cmd(&apos;sh&apos;, [&apos;./deploy.sh&apos;], function(text)&#123; console.log(text) &#125;);&#125;)try &#123; http.createServer(function (req, res) &#123; handler(req, res, function (err) &#123; res.statusCode = 404 res.end(&apos;no such location&apos;) &#125;) &#125;).listen(6666)&#125;catch(err)&#123; console.error(&apos;Error:&apos;, err.message)&#125; 其中secret要和github仓库中webhooks设置的一致，6666是监听端口可以随便改，不要冲突就行，./deploy.sh是接收到push事件时需要执行的shell脚本，与webhooks.js都存放在博客目录下；path: &#39;/webhooks_push&#39;是github通知服务器的地址 因为我们的服务器上使用了Nginx，所以这里我们也需要使用Nginx来转发6666端口。在Nginx配置文件目录下新建一个webhooks.conf，内容如下：12345678910111213server&#123; listen 80; server_name git.winsky.wang ; location / &#123; proxy_set_header HOST $host; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://localhost:6666/; &#125;&#125; 然后配置git.winsky.wang的域名解析 最后git上配置的地址是：http://git.winsky.wang/webhooks_push 配置./deploy.sh1234cd /home/blog/git reset --hardgit pull origin master hexo generate 然后运行node webhooks.js，就可以实现本地更新push到github，服务器会自动更新部署博客。 最后要将进程加入守护，通过pm2来实现1npm install pm2 --global 然后通过pm2启动webhooks.js1pm2 start /home/blog/webhooks.js 自启动参考服务器重启后自动运行hexo服务 快速搭建Hexo博客+webhook自动部署+全站HTTPS 给你的项目增加Webhooks，自动进行部署 使用Github的webhooks进行网站自动化部署]]></content>
      <categories>
        <category>Hexo博客</category>
      </categories>
      <tags>
        <tag>HEXO</tag>
        <tag>博客</tag>
        <tag>部署</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何让谷歌和百度搜索到自己的博客]]></title>
    <url>%2FHexo%E5%8D%9A%E5%AE%A2%2F%E5%A6%82%E4%BD%95%E8%AE%A9%E8%B0%B7%E6%AD%8C%E5%92%8C%E7%99%BE%E5%BA%A6%E6%90%9C%E7%B4%A2%E5%88%B0%E8%87%AA%E5%B7%B1%E7%9A%84%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[前面我们介绍了如何在搬瓦工VPS上快速搭建自己的个人博客，并进行相关的主题美化 现在问题来了，既然博客搭建起来了，总该要被主流搜索引擎收录吧。今天我们就来学习一下如何然自己的博客被谷歌和百度搜索到 验证网站查看是否被收录 方法：打开百度或谷歌搜索，在搜索框里输入site:blog.winsky.wang 如果提示说：找不到和您查询的site:blog.winsky.wang相符的内容或信息，说明未被收录 如果搜索结果中你第一眼就看到了你的博客站点，说明已被收录，不用再继续看下面的内容了 百度搜索提交 登录百度站长平台 在站点管理中添加博客的站点 第一步填写自己博客的域名 第二步选举站点属性 第三步验证域名所属权 可以在域名解析中加入了百度的CNAME的域名 或者下载验证文件，在html文件前面加上如下内容，这样可以保证该文件不会被编译，然后将文件上传到source文件夹下12layout: false--- 进入链接提交页提交链接 这里有很多种方式，实在不行就选取手动提交的方式 我这边选取了sitemap.xml提交的方式 Hexo的Next主题也已经部署了自动推送的代码，我们只需在主题配置文件中找到baidu_push字段 , 设置其为true即可 百度站长平台抓取诊断功能，是直接让百度抓取我们的网站，测试能否正常抓取 这时百度搜索site:blog.winsky.wang就能搜到自己的网站了（好吧实际操作的时候我过了一段时间才能搜到） 百度的自动提交根据百度站长平台的指导，在themes/next/layout/_third-party/seo目录下创建baidu-push.swig文件，并填入相应内容，就可以自动推送文章到百度搜索中。 谷歌搜索提交 点击Google网站站长，然后点击首页的SEARCH CONSOLE 添加网站，验证所有权（直接在推荐的方法下点击验证即可） 进入到抓取-&gt;Google抓取方式 点击抓取或者抓取并呈现 若提示完成或者部分完成，则可以将网址提交至索引，有两种提交方式：仅抓取此网址、抓取此网址及其直接链接 添加站点地图https://blog.winsky.wang/sitemap.xml 这时谷歌搜索site:blog.winsky.wang就能搜到自己的网站了 参考文章 如何让谷歌和百度搜索到自己GitHub上的博客]]></content>
      <categories>
        <category>SEO优化</category>
      </categories>
      <tags>
        <tag>SEO</tag>
        <tag>收录</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo博客Next主题配置]]></title>
    <url>%2FHexo%E5%8D%9A%E5%AE%A2%2FHexo%E5%8D%9A%E5%AE%A2Next%E4%B8%BB%E9%A2%98%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[前面我们介绍了如何在搬瓦工VPS上快速搭建自己的个人博客,不会的小伙伴赶快去学习一下 其中我们也介绍了如何将Hexo默认的主题替换为Next主题，但是Next的原始配置还是太过简洁，看上去不尽人意 因此这次我们就来学习一下如何优化Next的配置，丰富其功能。一起来吧~~ 站点设置展示社交联系方式 编辑站点配置文件 修改socal字段，并注释相应的字段，修改值 设置社交链接居中对齐 修改themes\next\source\css\_common\components\sidebar\sidebar-author-links.styl文件，添加如下样式123.links-of-author-item &#123; text-align: center;&#125; 添加友链 编辑主题配置文件 添加如下内容 12345# title, chinese availablelinks_title: 友情链接# linkslinks: 百度: https://www.baidu.com/ 修改Blog rolls下的links_title为中文 设置友链左对齐 本博客侧栏友情链接使用了与侧栏社交链接相同的css样式，但文本左对齐 实现方法为：修改themes\next\layout\_macro\sidebar.swig，将如下内容123456789&lt;ul class=&quot;links-of-blogroll-list&quot;&gt; &#123;% for name, link in theme.links %&#125; &lt;li class=&quot;links-of-blogroll-item&quot;&gt; &lt;a href=&quot;&#123;&#123; link &#125;&#125;&quot; title=&quot;&#123;&#123; name &#125;&#125;&quot; target=&quot;_blank&quot;&gt; &#123;&#123; name &#125;&#125; &lt;/a&gt; &lt;/li&gt; &#123;% endfor %&#125;&lt;/ul&gt; 修改为123456789&lt;ul class=&quot;links-of-blogroll-list&quot;&gt; &#123;% for name, link in theme.links %&#125; &lt;span class=&quot;links-of-author-item&quot; style=&quot;text-align:left&quot;&gt; &lt;a href=&quot;&#123;&#123; link &#125;&#125;&quot; title=&quot;&#123;&#123; name &#125;&#125;&quot; target=&quot;_blank&quot;&gt; &#123;&#123; name &#125;&#125; &lt;/a&gt; &lt;/span&gt; &#123;% endfor %&#125;&lt;/ul&gt; SEO推广sitemap Sitemap用于通知搜索引擎网站上有哪些可供抓取的网页，以便搜索引擎可以更加智能地抓取网站 执行命令npm install hexo-generator-sitemap --save和npm install hexo-generator-baidu-sitemap --save，安装插件，用于生成sitemap 运行hexo g生成站点文件 在站点配置文件中添加如下字段12345# Sitemap Settingsitemap: path: sitemap.xmlbaidusitemap: path: baidusitemap.xml 添加 robots.txt 文件放在站点的source文件夹下123456789101112User-agent: *Allow: /Allow: /archives/Disallow: /js/Disallow: /css/Disallow: /fonts/Disallow: /lib/Disallow: /fancybox/Sitemap: http://www.cylong.com/sitemap.xmlSitemap: http://www.cylong.com/baidusitemap.xml 添加字数统计 安装hexo-wordcount插件npm install hexo-wordcount --save 在站点配置文件中开启字数统计配置 12# 字数统计word_count: true 然后在/themes/next/layout/_partials/footer.swig文件尾部加上 1234&lt;div class=&quot;theme-info&quot;&gt; &lt;div class=&quot;powered-by&quot;&gt;&lt;/div&gt; &lt;span class=&quot;post-count&quot;&gt;博客全站共&#123;&#123; totalcount(site) &#125;&#125;字&lt;/span&gt;&lt;/div&gt; 主题设定 version: 6.0.3选择 Scheme Scheme是NexT提供的一种特性 借助于Scheme，NexT提供了多种不同的外观 同时，几乎所有的配置都可以在Scheme之间共用 目前NexT支持三种Scheme，他们是： Muse - 默认Scheme，这是 NexT最初的版本，黑白主调，大量留白 Mist - Muse的紧凑版本，整洁有序的单栏外观 Pisces - 双栏Scheme，小家碧玉似的清新 Scheme的切换通过更改主题配置文件，搜索scheme关键字，你会看到有三行Scheme的配置，将你需用启用的scheme前面注释#去除即可123456789# ---------------------------------------------------------------# Scheme Settings# ---------------------------------------------------------------# Schemes#scheme: Muse#scheme: Mistscheme: Pisces#scheme: Gemini 设置语言 编辑站点配置文件，将language设置成你所需要的语言 具体支持的语言可以查看官网说明 建议明确设置你所需要的语言，例如选用简体中文，配置如下1language: zh-CN 设置菜单展示菜单内容 菜单配置包括三个部分 第一是菜单项（名称和链接） 第二是菜单项的显示文本 第三是菜单项对应的图标。NexT使用的是Font Awesome提供的图标，Font Awesome提供了600+的图标，可以满足绝大的多数的场景，同时无须担心在Retina屏幕下图标模糊的问题。 编辑主题配置文件，修改以下内容： 设定菜单内容，对应的字段是menu菜单内容的设置格式是：item name: link。其中item name是一个名称，这个名称并不直接显示在页面上，她将用于匹配图标以及翻译 123456789101112131415161718# ---------------------------------------------------------------# Menu Settings# ---------------------------------------------------------------# When running the site in a subdirectory (e.g. domain.tld/blog), remove the leading slash from link value (/archives -&gt; archives).# Usage: `Key: /link/ || icon`# Key is the name of menu item. If translate for this menu will find in languages - this translate will be loaded; if not - Key name will be used. Key is case-senstive.# Value before `||` delimeter is the target link.# Value after `||` delimeter is the name of FontAwesome icon. If icon (with or without delimeter) is not specified, question icon will be loaded.menu: home: / || home archives: /archives/ || archive tags: /tags/ || tags categories: /categories/ || th about: /about/ || user #schedule: /schedule/ || calendar #sitemap: /sitemap.xml || sitemap #commonweal: /404/ || heartbeat 注意：若你的站点运行在子目录中，请将链接前缀的/去掉 NexT默认的菜单项有（斜体的项表示需要手动创建这个页面） 键值 | 设定值 | 显示文本（简体中文）—|— | —home | home: / | 主页archives | archives: /archives | 归档页categories | categories: /categories | 分类页tags | tags: /tags | 标签页about | about: /about | 关于页面commonweal | commonweal: /404.html | 公益 404 设置菜单项的显示文本。在第一步中设置的菜单的名称并不直接用于界面上的展示。Hexo 在生成的时候将使用 这个名称查找对应的语言翻译，并提取显示文本。这些翻译文本放置NexT主题目录下的languages/{language}.yml（{language} 为你所使用的语言）以简体中文为例，若你需要添加一个菜单项，比如something。那么就需要修改简体中文对应的翻译文languages/zh-Hans.yml 添加标签页 在站点source文件夹下，建立tags目录 在tags目录中创建index.md，内容如下：123456---title: 标签date: 2018-02-04 21:33:54type: &quot;tags&quot;comments: false--- 添加分类页 在站点source文件夹下，建立categories目录 在categories目录中创建index.md，内容如下：123456---title: 分类date: 2018-02-04 21:33:54type: &quot;categories&quot;comments: false--- 添加关于页 在站点source文件夹下，建立about目录 在about目录中创建index.md，具体内容参加github源码 设置侧栏 默认情况下，侧栏仅在文章页面（拥有目录列表）时才显示，并放置于右侧位置。 可以通过修改主题配置文件中的sidebar字段来控制侧栏的行为。 侧栏的设置包括两个部分，其一是侧栏的位置， 其二是侧栏显示的时机。 设置侧栏的位置，修改sidebar.position的值，支持的选项有 left - 靠左放置 right - 靠右放置12sidebar: position: left 设置侧栏显示的时机，修改sidebar.display的值，支持的选项有 post - 默认行为，在文章页面（拥有目录列表）时显示 always - 在所有页面中都显示 hide - 在所有页面中都隐藏（可以手动展开） remove - 完全移除12sidebar: display: post 设置头像 编辑站点配置文件， 添加字段avatar，值设置成头像的链接地址 其中，头像的链接地址可以是： 完整的互联网 URI 站点内的地址 将头像放置主题目录下的source/uploads/（新建uploads目录若不存在），配置为：avatar:/uploads/avatar.png 或者 放置在 source/images/目录下，配置为：avatar: /images/avatar.png12# Avataravatar: /images/avatar.jpg 设置作者昵称 编辑站点配置文件 设置author为你的昵称 设置站点描述 编辑站点配置文件 设置description为你的站点描述。站点描述可以是你喜欢的一句签名:) 设置首页预览和阅读全文 编辑主题配置文件 设置auto_excerpt的配置123auto_excerpt: enable: true length: 300 关闭打开文章自动跳转到more 编辑主题配置文件 修改scroll_to_more值为false fontawesome图标显示问题 2018-02-12更新*今天早上打开博客发现导航的图标不能正常显示了。然而本地启动确实正常了。经过一番检查，发现是服务器上无法加载fontawesome的css。没办法，只能自己指定fontawesome的css的地址了 编辑主题配置文件，指定fontAwesome的cdn地址1fontawesome: https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css 集成第三方服务百度统计 登录百度统计, 定位到站点的代码获取页面 复制 hm.js? 后面那串统计脚本 id，如 编辑主题配置文件，修改字段 baidu_analytics字段，值设置成你的百度统计脚本id 谷歌统计 需科学上网 登录谷歌统计， 定位到管理页面 创建新的媒体资源，获取跟踪id 编辑主题配置文件，添加字段 google_analytics字段，值设置成你的谷歌统计跟踪id 集成Disqus评论 注册登陆Disqus 点击Admin进入管理页面 选择第二个I want to install Disqus on my site 按照表单填写信息，记住Website Name这条属性，配置文件中需要用到 接下来按照指引填写信息（基本都是默认就行） 安装过程中会出现下面页面，这里面会有disqus在不同博客系统上或者其他系统上对应的代码。因为hexo自带支持disqus，所以不需要这里面的代码，这个页面的内容会在其他除hexo之外的博客系统中用到，如果是hexo搭建博客disqus，可以跳过 接下来配置主题下面的config.yml文件123456# Disqusdisqus: enable: true shortname: winsky #就是前面填写的Website Name属性 count: true lazyload: false 不蒜子 编辑主题配置文件中的busuanzi_count的配置项123456789101112131415busuanzi_count: # count values only if the other configs are false enable: true # custom uv span for the whole site site_uv: true site_uv_header: &lt;i class=&quot;fa fa-user&quot;&gt;&lt;/i&gt; site_uv_footer: 人 # custom pv span for the whole site site_pv: true site_pv_header: &lt;i class=&quot;fa fa-eye&quot;&gt;&lt;/i&gt; site_pv_footer: 次 # custom pv span for one page only page_pv: true page_pv_header: &lt;i class=&quot;fa fa-file-o&quot;&gt;&lt;/i&gt; page_pv_footer: 不蒜子 本地搜索2018-05-08更新，添加本地搜索功能在hexo的根目录下执行命令：npm install hexo-generator-searchdb --save 在根目录下的_config.yml文件中添加配置：12345search: path: search.xml field: post format: html limit: 10000 在根目录下的/theme/next/_config.yml文件中搜索local_search，将enable改为true：12local_search: enable: true 运行效果 参考文章 next中文手册 Hexo+nexT主题搭建个人博客 为Hexo NexT主题添加字数统计功能 Github 搭建 hexo （四）——更换主题，disqus，RSS]]></content>
      <categories>
        <category>Hexo博客</category>
      </categories>
      <tags>
        <tag>HEXO</tag>
        <tag>博客</tag>
        <tag>Next</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[个人博客Hexo搭建]]></title>
    <url>%2FHexo%E5%8D%9A%E5%AE%A2%2F%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2Hexo%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[前言最近沉迷各种折腾，无心学习 T^T VPS基本折腾完了，现在又想开始折腾自己搭建一个博客来耍耍 这篇笔记的博客也是搭建在搬瓦工VPS上的 安装过程 安装nodejs 安装git 安装hexo npm install -g hexo 新建一个文件 mkdir blog 生成模板 cd blog hexo init 安装依赖 cd blog npm install 启动服务 hexo s，当控制台提示如下信息时表示启动成功 123[root@bwh themes]# hexo sINFO Start processingINFO Hexo is running at http://localhost:4000/. Press Ctrl+C to stop. 通过localhost:4000来访问博客 切换主题 可以在Themes-Hexo页面查看各种各样的主题 将喜欢的主题下载下来，放到博客目录下的themes文件夹 之后修改博客的_config.yml文件 1234# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: next # 修改成你的主题文件夹名 Hexo 默认主题是 landscape。我使用的是next主题，清晰简约，非常符合我的喜好。Next主题的配置可以参考 Next主题下载 Next主题中文文档 博客的具体配置，后面专门开一篇博文来记录一下 2018-02-04 更新Hexo博客Next主题配置 参考文章 Hexo Hexo搭建博客教程 Next主题中文文档]]></content>
      <categories>
        <category>Hexo博客</category>
      </categories>
      <tags>
        <tag>HEXO</tag>
        <tag>博客</tag>
      </tags>
  </entry>
</search>
